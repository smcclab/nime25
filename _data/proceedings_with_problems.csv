id,track,format,type,session_code,session_name,duration,presence,title,authors,abstract,paper_url,video_url,slides_url,image_url,image_credit,problems,location
8,paper,poster,short,posters-1,,5,remote,8,Sabina Hyoju Ahn; Ryan Millett; Seyeon Park,"Neural Tides is a neural network-based granular synthesizer that examines plastiglomerates—hybrid formations of plastic and organic material in marine environments. The system maps sound grains from oceanic field recordings to a navigable latent space using autoencoders and clustering techniques, controlled via hand gestures and touch. This interface physically connects performers with sonic representations of anthropogenic material transformations in coastal environments.",nime2025_8.pdf,,,8.jpg,,registration,
39,paper,oral,long,,,15,in person,"Synthetic Ornithology: Machine learning, simulations and hyper-real soundscapes",Frederick Rodrigues,"This paper presents Synthetic Ornithology, an interactive sound-based installation that uses machine learning to simulate sonic representations of localised Australian ecological futures, extending work in soundscape composition to engage in a speculative domain. Central to Synthetic Ornithology is a bespoke ML model, Environmental Audio Generation for Localised Ecologies (EAGLE), capable of generating high-quality, birdsong-focused soundscapes, up to 23 seconds in length. This paper outlines the development of the installation and how its design aims to influence audience perception of the sonic content of the work, extending established practices in NIME and sonic arts to a parafictional approach, and hyperreal aesthetics. Additionally, the paper examines the design and capabilities of the EAGLE model, and reflecting on how generative tools are positioned within a creative context, re-imagines the technical processes of training and configuring ML models as sites of artistic authorship in an expanded creative audio practice.",nime2025_39.pdf,,,39.jpg,,registration,
199,music,Installation,,,,N/A*,In person,Fountains of Data,Scott Smallwood; Marilene Oliver; Daniel Evans,"FlowState is a virtual reality (VR) installation inspired by one of several dozen ancient Celtic baths in the Brittany region of France, built by Bretons in the 3-5th centuries. These baths were believed to be places of healing, often of very specific ailments, and were part of a larger culture of water as a healing agent. In our piece, the visitor initially encounters a field of particles, a point cloud outlining the bath structure in an empty black space (see fig. 1). As the visitor explores the area, moving arms in swim-like strokes causes interaction with the soundscape, including the addition of water sounds, and eventually leading to the particles resolving into a more realistic conception of the space, via lidar scans, 360-degree video, and sound recorded on the actual site of the fountain.",nime2025_199.pdf,,,199.jpg,,cr,
7,music,Live Performance,,,,6,In person,Live Microtonal Performance,Amanda Cole,"I will compose and perform a new microtonal work for Lumatone keyboard, iPad ensemble and tuned metallic bars (Tubophone) made by a Canberra based percussionist. I will play the Lumatone MIDI keyboard part using my own Lumatone (isomorphic microtonal keyboard with coloured hexagonal keys). The Lumatone will use an original mapping of a 48 note to the octave 7-limit just intonation scale. The iPad ensemble will be 'Touchscreen Ensemble', which is an ANU students ensemble coordinated by Charles Martin.  This ensemble piece will explore harmonies, melodies and textures in just intonation tuning played on new interfaces for musical expression.",nime2025_7.pdf,,,7.jpg,,"registration, cr",
130,music,Live Performance,,,,15,In person,"The Chromaplane, an Analog Electromagnetic Synthesizer",Nicoletta Favari; Christopher Salvito,"The Chromaplane is an instrument that lives in that hidden electromagnetic world, but with an intentional musicality. It is played using two electromagnetic pickup coils, which are used to listen to a cloud of electromagnetic fields produced by the instrument. The fields are laid out in an isomorphic pattern and identified by small dots on the instrument’s surface. Making use of the Chromaplane  and other electronic devices, this 10 minute long musical performance highlights the instrument’s capabilities in a performance context.",nime2025_130.pdf,,,130.jpg,,registration,26th black box
338,workshop,,,workshops-1,,,,MoNoDeC: The Mobile Node Controller Platform,Nick Hwang; Anthony Marasco,"MoNoDeC is a multichannel audio system that uses audience mobile phones and IoT-hardware-driven speakers as point sources for configurable and dynamic immersive audio speakers and audience performance interfaces. Audience participants register their current location within a customizable audience space diagram on their mobile phones. Their phones then become a point source speaker within the immersive experience. A person designated as the performance/installation ‘controller’ sends audio, control, and interface data to participants throughout the experience. In addition to diffusing audio to mobile devices, MoNoDeC can include embedded computer instruments known as ‘autonomous hubs’: IoT-based speakers that receive playback/diffusion data. These autonomous hubs are meant to be used in larger sound diffusion performance settings as well as installation/fixed point source scenarios in conjunction with audience-provided devices. 

This workshop will lead participants on setting up the MoNoDeC system, designing registration and location-choosing presets for audience members to engage with, and methods for panning audio around the networked mobile device speaker array during a live composition. Participants will also learn the basics of Collab-Hub, our framework for sharing control data between networked telematic performers that serves as the backbone of MoNoDeC. After learning the core features of both systems, participants will form groups and design short immersive compositions using the framework. participants will set up the MoNoDec system, design registration and location-choosing presets for audience members to engage with, and explore methods for panning audio around the networked mobile device speaker array during a live composition. At the end of the session, participants will form groups and design short immersive compositions using the system. ",nime2025_338.pdf,,,338.jpg,,registration,Hanna Neumann 3.41
