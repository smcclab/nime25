id,track,format,type,session_code,session_name,duration,presence,title,authors,abstract,paper_url,video_url,slides_url,image_url,image_credit,problems,location
8,paper,poster,short,posters-1,,5,remote,Eco-Sonic Interfaces for Embodied AI Sound Exploration,Sabina Hyoju Ahn; Ryan Millett; Seyeon Park,"Neural Tides is a neural network-based granular synthesizer that examines plastiglomerates—hybrid formations of plastic and organic material in marine environments. The system maps sound grains from oceanic field recordings to a navigable latent space using autoencoders and clustering techniques, controlled via hand gestures and touch. This interface physically connects performers with sonic representations of anthropogenic material transformations in coastal environments.",nime2025_8.pdf,,,8.jpg,,registration,
39,paper,oral,long,,,15,in person,"Synthetic Ornithology: Machine learning, simulations and hyper-real soundscapes",Frederick Rodrigues,"This paper presents Synthetic Ornithology, an interactive sound-based installation that uses machine learning to simulate sonic representations of localised Australian ecological futures, extending work in soundscape composition to engage in a speculative domain. Central to Synthetic Ornithology is a bespoke ML model, Environmental Audio Generation for Localised Ecologies (EAGLE), capable of generating high-quality, birdsong-focused soundscapes, up to 23 seconds in length. This paper outlines the development of the installation and how its design aims to influence audience perception of the sonic content of the work, extending established practices in NIME and sonic arts to a parafictional approach, and hyperreal aesthetics. Additionally, the paper examines the design and capabilities of the EAGLE model, and reflecting on how generative tools are positioned within a creative context, re-imagines the technical processes of training and configuring ML models as sites of artistic authorship in an expanded creative audio practice.",nime2025_39.pdf,,,39.jpg,,registration,
7,music,Live Performance,,,,6,In person,Live Microtonal Performance,Amanda Cole,"I will compose and perform a new microtonal work for Lumatone keyboard, iPad ensemble and tuned metallic bars (Tubophone) made by a Canberra based percussionist. I will play the Lumatone MIDI keyboard part using my own Lumatone (isomorphic microtonal keyboard with coloured hexagonal keys). The Lumatone will use an original mapping of a 48 note to the octave 7-limit just intonation scale. The iPad ensemble will be 'Touchscreen Ensemble', which is an ANU students ensemble coordinated by Charles Martin.  This ensemble piece will explore harmonies, melodies and textures in just intonation tuning played on new interfaces for musical expression.",nime2025_7.pdf,,,7.jpg,,"registration, cr",
51,music,Remote Performance,,,,8,Remote,CONJURE-II,Huichun Yang,"CONJURE-II explores the dynamic intra-action between body, sound, and materiality through the use of a balloon as an apparatus. The tension and pressure between the performer's body and the balloon push the boundaries of ""skin,"" suggesting that these boundaries are not fixed but rather determined by the vibrations that resonate within and between all entities. The balloon becomes a site where vibrations mediate the boundaries of self and world, creating an ephemeral interplay of effort, vulnerability, and transformation. The system uses sound captured from the balloon surface, the performer's breathing, and modular synthesis, spatialized through ambisonics to create an evolving sonic environment.",nime2025_51.pdf,,,51.jpg,,"registration, cr",
130,music,Live Performance,,,,15,In person,"The Chromaplane, an Analog Electromagnetic Synthesizer",Nicoletta Favari; Christopher Salvito,"The Chromaplane is an instrument that lives in that hidden electromagnetic world, but with an intentional musicality. It is played using two electromagnetic pickup coils, which are used to listen to a cloud of electromagnetic fields produced by the instrument. The fields are laid out in an isomorphic pattern and identified by small dots on the instrument’s surface. Making use of the Chromaplane  and other electronic devices, this 10 minute long musical performance highlights the instrument’s capabilities in a performance context.",nime2025_130.pdf,,,130.jpg,,registration,
141,music,Installation,,,,N/A*,Remote,"break me, genAI",Mayank Sanganeria; Kurt Werner,"This paper presents ""break me, generative ai,"" an audiovisual artwork exploring the intersection of human perception and artificial intelligence in music visualization. The work addresses limitations in current Music Information Retrieval (MIR) approaches by implementing a novel multilayered system that combines hand-drawn MIDI parameter curves, procedurally generated visuals inspired by synesthetic artists, and various AI transformation processes including StyleGAN, Stable Diffusion, and AnimateDiff. By preserving artistic intent throughout the generation process while leveraging AI capabilities, the system creates visuals that better reflect the complex, context-dependent nature of musical perception. The resulting artwork serves as both a technical demonstration and an artistic exploration of human-AI collaboration, challenging viewers to consider questions of authorship, perception, and the preservation of artistic intent in AI-processed creative works. This approach offers new possibilities for the creation of audiovisual art that bridge the gap between purely automated systems and traditional human-driven approaches.",nime2025_141.pdf,,,141.jpg,,cr,
158,music,Installation,,,,N/A*,In person,Bending Nature: An Architectural Sound Installation Exploring the Resonance of Architectural Texture and Natural Sound,Han Xu; Zehao Wang,"Bending Nature is an architectural sound installation situated in a confined, irregular space. By repurposing part of the raw speakers as microphones and surface transducers placed directly on the walls activate the architectural texture, the work merges natural sounds with feedback loops and creates a resonant dialogue between the building and its environment. Through a visually minimalist presentation, viewers are immersed in a dynamic, ever-changing soundscape that challenges spatial perception and offers a novel sensory experience of sound and structure.",nime2025_158.pdf,,,158.jpg,,registration,
199,music,Installation,,,,N/A*,In person,Fountains of Data,Scott Smallwood; Marilene Oliver; Daniel Evans,"FlowState is a virtual reality (VR) installation inspired by one of several dozen ancient Celtic baths in the Brittany region of France, built by Bretons in the 3-5th centuries. These baths were believed to be places of healing, often of very specific ailments, and were part of a larger culture of water as a healing agent. In our piece, the visitor initially encounters a field of particles, a point cloud outlining the bath structure in an empty black space (see fig. 1). As the visitor explores the area, moving arms in swim-like strokes causes interaction with the soundscape, including the addition of water sounds, and eventually leading to the particles resolving into a more realistic conception of the space, via lidar scans, 360-degree video, and sound recorded on the actual site of the fountain.",nime2025_199.pdf,,,199.jpg,,cr,
293,music,Live Performance,,,,15,In person,"Improvising with ""GravField"": A Participatory Performance Exploring How Digital Objects Mediate Intercorporeal Movements within Collocated Mixed Reality",Botao Hu; Yuemin Huang; Mingze Chai; Xiaobo Hu; Yilan Tao; Rem RunGu Lin," As mixed reality technologies evolve, they blur the boundaries between the digital and physical worlds, prompting a reevaluation of how we engage with digital objects and mediated environments. This paper investigates the emerging concept of ""digital physics"" in MR, where digital objects, as informational entities, shape both cognitive and somatic experiences. Building on the ""somatic turn"" in human-computer interaction, we explore how the human body, as a site of interaction, adapts to new forms of embodied collaboration in MR spaces. Through the lens of contact improvisation, we introduce GravField, a live performance-based MR system that uses Audio-Visual Virtual Mediators (AVVMs) to facilitate interdependent behaviors among participants. These AVVMs incorporate metaphors such as springs, ropes, and magnetic fields, shaping bodily movement and social dynamics through dynamic, sensory feedback. Drawing from post-phenomenological theories, this research examines the relationships between embodiment, interpretation, alterity, and background in the context of MR. Our findings contribute to the understanding of how digital objects mediate embodied interaction and collective behavior, offering practical insights for designing entangled and embodied experiences in MR environments.",nime2025_293.pdf,,,293.jpg,,"registration, cr",
334,workshop,,,workshops-2,,,,"Global Entanglements, Music and the Commodification of Cultural Labour in the Age of AI",Tom Willma; Oliver Bown,"Motivated by global discussions of socio-technical change and recent Australian legislative lobbying, this short-form, 3-hour workshop targets the commodification of cultural labour and current entanglements between social, political, corporate and technological actors in the music industry. The objective of this workshop is to hold a critically minded discussion between guest panellists from the Australian music sector and NIME colleagues, grappling with these sociotechnical imperatives both generally, across the industry, and particularly, within the context of live performance. Key takeaways of this workshop include:

• An empirical overview that survey’s sociotechnical change and AI use across global music industries
• A critical discussion that scopes key political-economic concerns in the music sector
• Proposed frames for NIME research community to engage with industry and develop key strategies for ethical AI adoption
• Directions for future discourses, policy considerations and further dialogues of sociotechnical transformation in the music industry

Discussions between panellists and attendees will be guided by three critical questions:
1) What political-economic changes have been taking place in global music cultures, including but not limited to those involving AI and live virtualisation technologies?
2) How does the NIME research community interface with music cultures beyond academia in an age of AI, particularly in the development of live music technologies?
3) What blind sports are being neglected by the current trajectory of political and industrial discourses? How is our work being affected, what voices are being heard above others and what could/should be done?

Please note that the content of this workshop will be recorded and may be developed and referenced in future research and publications by the organisers, as may a record of workshop minutes be published.",nime2025_334.pdf,,,334.jpg,,registration,Hanna Neumann 1.37
338,workshop,,,workshops-1,,,,MoNoDeC: The Mobile Node Controller Platform,Nick Hwang; Anthony Marasco,"MoNoDeC is a multichannel audio system that uses audience mobile phones and IoT-hardware-driven speakers as point sources for configurable and dynamic immersive audio speakers and audience performance interfaces. Audience participants register their current location within a customizable audience space diagram on their mobile phones. Their phones then become a point source speaker within the immersive experience. A person designated as the performance/installation ‘controller’ sends audio, control, and interface data to participants throughout the experience. In addition to diffusing audio to mobile devices, MoNoDeC can include embedded computer instruments known as ‘autonomous hubs’: IoT-based speakers that receive playback/diffusion data. These autonomous hubs are meant to be used in larger sound diffusion performance settings as well as installation/fixed point source scenarios in conjunction with audience-provided devices. 

This workshop will lead participants on setting up the MoNoDeC system, designing registration and location-choosing presets for audience members to engage with, and methods for panning audio around the networked mobile device speaker array during a live composition. Participants will also learn the basics of Collab-Hub, our framework for sharing control data between networked telematic performers that serves as the backbone of MoNoDeC. After learning the core features of both systems, participants will form groups and design short immersive compositions using the framework. participants will set up the MoNoDec system, design registration and location-choosing presets for audience members to engage with, and explore methods for panning audio around the networked mobile device speaker array during a live composition. At the end of the session, participants will form groups and design short immersive compositions using the system. ",nime2025_338.pdf,,,338.jpg,,registration,Hanna Neumann 3.41
