id,title,abstract,speaker,contact-email,authors-institutions,author-names,authors,author-emails,primary-subject,secondary-subject,status,format,presence,notes,session_code,session_name,paper_url,video_url,slides_url,image_url,keywords
10,Looping slowly: Diffraction through the lens of nostalgia,"This paper concerns magnetic tape and the nostalgia of media, finding new relevance in old technology, remaking and adapting practices to fit within a modern workflow. Pushing against the driving force of economic structures, which emphasises a continuous cycle of replacement, musicians and instrument designers are drawing on a shared history to create new pieces of art and machines. This can be read as reflecting NIME's Code of Practice and, more generally, the unfolding climate crisis.  For some, NIME may convey a focus on new musical instruments, but here, we focus on the notion of new through the diffracted lens of the old. Defined in recent NIME conferences by zooming in on the 'O' in NIME through the importance of reusing and repurposing old musical instruments and, in our case, old practices and processes. This paper considers magnetic tape and the machines that process it as the material and instrument.  Following a survey, we present a diffracted reading through an intra-related process of how musicians, producers, and others who work with audio integrate tape into their practice. Drawing on post-humanist theories, we explore how slowness, community, and the old can inform NIME as a methodology. It provides insight for NIME to continue moving forward while focusing, through its Code of Practice, on sustainability, connection with our past, our history, years of artistic practice, and workflows that are not simply optimised for efficiency or the new.",Benedict Gaster,benedict.gaster@uwe.ac.uk,Benedict Gaster (Univeristy of West England)*; Nathan Renney (University of West of England); Jasmine Butt (University of West of England),"Gaster, Benedict*; Renney, Nathan; Butt, Jasmine",Benedict Gaster; Nathan Renney; Jasmine Butt,benedict.gaster@uwe.ac.uk*; Nathan.Renney@uwe.ac.uk; jasmine2.butt@live.uwe.ac.uk,"Entangled NIME: Intertwined, multilayer contexts in NIME research","Discussions about the artistic, cultural, and social impact of NIME technology ; Music-related human-computer interaction",Accept as Long Paper (up to 6000 words),oral,in person,,,,nime2025_10.pdf,,,10.jpg,"Tape, nostalgia, art, agency, ethnography, design"
11,Simulated EEG-Driven Audio Information Mapping Using Inner Hair-Cell Model and Spiking Neural Network,"This study presents a framework for mapping audio information into simulated neural signals and dynamic control maps. The system is based on a biologically-inspired architecture that traces the sound pathway from the cochlea to the auditory cortex. The system transforms acoustic features into neural representations by integrating Meddis's Inner Hair-Cell (IHC) model with spiking neural networks (SNN). The mapping process occurs in three phases: the IHC model converts sound waves into neural impulses, simulating hair cell mechano-electrical transduction. These impulses are then encoded into spatio-temporal patterns through an Izhikevich-based neural network, where spike-timing-dependent plasticity (STDP) mechanisms enable the emergence of activation structures reflecting the acoustic information's complexity. Finally, these patterns are mapped into both EEG-like signals and continuous control maps for real-time interactive performance control. This approach bridges neural dynamics and signal processing, offering a new paradigm for sound information representation. The generated control maps provide a natural interface between acoustic and parametric domains, enabling applications from generative sound design to adaptive performance control, where neuromorphological sound translation explores new forms of audio-driven interaction.",Pasquale Mainolfi,mnlpql@gmail.com,"Pasquale Mainolfi (Conservatorio G. Martucci, Salerno IT)*","Mainolfi, Pasquale*",Pasquale Mainolfi,mnlpql@gmail.com*,Musical mapping strategies,"Machine learning in musical performance; New music performance paradigms; Software frameworks, interface protocols, and data formats, for supporting musical interaction",Accept as Long Paper (up to 6000 words),oral,remote,,,,nime2025_11.pdf,,,11.jpg,"Audio Information Mapping, Inner Hair-Cell Model, Spiking Neural Network, EEG, Neural Control Signals"
21,"Entangling with Light and Shadow: layers of interaction with the Pattern Organ, a camera-based wavetable synthesiser","This paper explores a method of visual-to-audio synthesis that seeks to respond closely to the material behaviours captured in its camera input. Just as an acoustic musical instrument's resonances are defined by its form and material, a one-to-one luminance-to-amplitude mapping allows us to explore sound texture through patterns of light and shadow created by different material assemblages.   In detailing the design of the Pattern Organ, a camera-based wavetable synthesiser, we demonstrate how frugal mappings require an external input to provide complexity, extending an instrument's sound engine beyond its enclosure. The harmonic content of the wavetable can be informed and altered by many overlapping and interfering material sources and interactions, including casting shadows with forms and torches, interacting with deformable patterned materials, controlling manual lenses and exploring system feedback.  Sharing findings from exploratory workshops and material experiments, we describe how interactions emerge and are informed by both the physical configuration of the instrument and the material behaviours exposed to the camera. These observations have led us to reflect on the instrument as a continually shifting apparatus representing an entanglement of both human and non-human elements that runs on patterns of difference.",Jasmine Butt,jasmine2.butt@live.uwe.ac.uk,Jasmine Butt (UWE)*; Benedict Gaster (UWE (University of the West of England)); Nathan Renney (UWE (University of the West of England)); Maisie Palmer (UWE (University of the West of England)),"Butt, Jasmine*; Gaster, Benedict; Renney, Nathan; Palmer, Maisie",Jasmine Butt; Benedict Gaster; Nathan Renney; Maisie Palmer,jasmine2.butt@live.uwe.ac.uk*; Benedict.Gaster@uwe.ac.uk; Nathan.Renney@uwe.ac.uk; Maisie3.Palmer@live.uwe.ac.uk,"Entangled NIME: Intertwined, multilayer contexts in NIME research","Evaluation and user studies of new interfaces for musical expression ; Music-related human-computer interaction ; Novel controllers, interfaces or instruments for musical expression",Accept as Long Paper (up to 6000 words),oral,remote,,,,nime2025_21.pdf,,,21.jpg,"Visual-to-Audio Synthesis, Optical Sound, Entanglement, Material Drift, DMI Design"
37,Chimera: Prototyping a New DMI for Congenital One-Handed Musicianship Through an Autoethnographic Lens,"Chimera is a new Digital Musical Instrument (DMI) prototype, developed through an autoethnographic lens shaped by congenital one-handedness as well as extensive experience as both a disabled player of standard instruments and a designer of DMIs for other players. Leveraging Eurorack synthesizer modules as a prototyping medium enables the replication of conventional (two-handed) instruments in sound or form, or in associated performance techniques, is avoided, and an iterative prototyping process enables exploration of the distinctive possibilities of one-handed musicianship. Reflection on a hundred day period of iteration and refinement highlights numerous (material) design issues, but also the interconnectedness of physical impairments and the difficulties of designing for an ever-and-unpredictably changing body. Directions for future work are also outlined. Finally, by introducing this instrument prototype and its development, and discussing the complex and intertwined layers that constitute what Koutsomichalis terms its “story of a sort”, this paper contributes a currently severely underrepresented perspective to the evolving dialogue around accessible and inclusive musical instrument design, as well as disability and musicianship more widely.",Mat Dalgleish,mat.dalgleish@staffs.ac.uk,Mat Dalgleish (Staffordshire University)*,"Dalgleish, Mat*",Mat Dalgleish,mat.dalgleish@staffs.ac.uk*,Accessibility and interfaces for musical expression for people with special needs,"Discussions about the artistic, cultural, and social impact of NIME technology ; Entangled NIME: Intertwined, multilayer contexts in NIME research",Accept as Long Paper (up to 6000 words),oral,remote,,,,nime2025_37.pdf,,,37.jpg,"DMI prototyping, physical disability, autoethnography, Eurorack, entangled layers"
39,"Synthetic Ornithology: Machine learning, simulations and hyper-real soundscapes","This paper presents Synthetic Ornithology, an interactive sound-based installation that uses machine learning to simulate sonic representations of localised Australian ecological futures, extending work in soundscape composition to engage in a speculative domain. Central to Synthetic Ornithology is a bespoke ML model, Environmental Audio Generation for Localised Ecologies (EAGLE), capable of generating high-quality, birdsong-focused soundscapes, up to 23 seconds in length. This paper outlines the development of the installation and how its design aims to influence audience perception of the sonic content of the work, extending established practices in NIME and sonic arts to a parafictional approach, and hyperreal aesthetics. Additionally, the paper examines the design and capabilities of the EAGLE model, and reflecting on how generative tools are positioned within a creative context, re-imagines the technical processes of training and configuring ML models as sites of artistic authorship in an expanded creative audio practice.",Frederick Rodrigues,s222405968@deakin.edu.au,Frederick Rodrigues (Deakin University)*,"Rodrigues, Frederick*",Frederick Rodrigues,s222405968@deakin.edu.au*,Machine learning and artificial intelligence in NIMEs,Interactive sound art and sound installations ; Sonic interaction design,Accept as Long Paper (up to 6000 words),oral,in person,,,,nime2025_39.pdf,,,39.jpg,"soundscapes, machine learning, climate change, generative audio"
50,Drum Modal Feedback: Concept Design of an Augmented Percussion Instrument,"We here outline the concept design of an augmented percussion instrument, conceived for and used as part of a variety of distinct performances and compositions. Throughout the curation of this project, each creative act has enabled us to contextualise, examine and reflect upon the design of this augmented instrument. In accordance with Stolterman and Wiberg’s concept driven design methodology, we do not present a singular instrument design, but instead an overarching design concept alongside its developmental and evaluative narrative. This augmentation centres upon the use of a drum trigger and a tactile transducer, which when coupled together can be used to feedback or resonate a drum. The resultant soundworld develops upon the idiomatic sonority of a drum, and allows for the duration and timbre of a drum strike to be continuously manipulated and shaped throughout a performance. In exploring the soundworld which results from this approach, we have experimented with numerous configurations of these pieces of hardware, and have also employed various pieces of software to parametrise the sonic subtleties that this approach engenders. Most prominently, we have developed a bespoke piece of software which analyses the modes of a drum prior to performance, and uses this modal analysis to shape the overall feedback and resonance. Throughout this design process, we have consistently been met with new creative criteria that challenge our approach and ideas, in response to the particularities of the musicians we are working alongside, as well as the performative and aesthetic environments we are working within.",Lewis Wolstanholme,l.wolstanholme@qmul.ac.uk,Lewis Wolstanholme (Queen Mary University of London)*; Jordie Shier (Queen Mary University of London); Rodrigo Constanzo (Royal Northern College of Music); Andrew McPherson (Imperial College London),"Wolstanholme, Lewis*; Shier, Jordie; Constanzo, Rodrigo; McPherson, Andrew",Lewis Wolstanholme; Jordie Shier; Rodrigo Constanzo; Andrew McPherson,l.wolstanholme@qmul.ac.uk*; j.m.shier@qmul.ac.uk; rodrigo.constanzo@rncm.ac.uk; andrew.mcpherson@imperial.ac.uk,"Augmented, embedded and hyper instruments",Practice-based research approaches/methodologies/criticism,Accept as Long Paper (up to 6000 words),oral,in person,,,,nime2025_50.pdf,,,50.jpg,"augmented percussion instruments, concept design, practice based reasearch"
58,Collaborative Expression in VR,"While the technical affordances of virtual reality (VR) have provided new ways for artists to aestheticize immersion, spectator agency, embodiment and multi-sensory engagement, they have also opened new possibilities for composers interested in exploring how interactive musical scores might become a means through which collaboration itself becomes the locus of aesthetic expression. In this paper, the author will provide an overview of an ongoing project which explores new ways of thinking about musical collaboration in VR through the 3D visualization of interactive, graphic scores adapted from works by composers Earle Brown, Christian Wolff, and Toru Takemitsu.",David Kim-Boyle,david.kim-boyle@sydney.edu.au,David Kim-Boyle (The University of Sydney)*,"Kim-Boyle, David*",David Kim-Boyle,david.kim-boyle@sydney.edu.au*,"Extended reality environments: augmented, virtual, mixed reality",Technologies or systems for collaborative music-making,Accept as Long Paper (up to 6000 words),oral,in person,,,,nime2025_58.pdf,,,58.jpg,"Virtual Reality, Multiplayer, Networking, Collaboration"
59,SVOrk: [Anonymous] Virtual Reality Orchestra,"This paper chronicles the creation of [Anonymous] Virtual Reality Orchestra (SVOrk), a first-of-its-kind computer music ensemble where both performers and audience engage in a shared, fully-immersive virtual reality (VR) chamber-esque concert experience. Motivated to explore group-based live performance within VR, SVOrk has designed and crafted fantastical 3D-modeled environments, musical interfaces specifically for the medium of VR, and a network infrastructure to support real-time shared participation that provides audience and performance a sense of sonic and spatial intimacy. Inherent within this initiative is a reimagining of conventional concert experiences, introducing virtual lobbies, customizable avatars, and immersive audience interactions. These experimental features explore new forms of social presence, audience identities, and expressive communication to help address an overarching question, “what does it mean to participate in a VR musical performance?” SVOrk’s premiere concert took place in June 2024 with five performers and approximately 60 audience members (across five presentations), featuring a program of six musical works. In this paper, we describe the motivations behind SVOrk, its research and development process—including designs for networking, avatar, and audience interaction—and takeaways from the premiere concert. We also present audience feedback and reflect on our experiences in curating group VR performances.",Kunwoo Kim,kunwoo@ccrma.stanford.edu,"Kunwoo Kim (Center for Computer Research in Music and Acoustics, Stanford University)*; Andrew Zhu (Center for Computer Research in Music and Acoustics, Stanford University); Eito Murakami (Center for Computer Research in Music and Acoustics, Stanford University); Marise van Zyl (Center for Computer Research in Music and Acoustics, Stanford University); Yikai Li (Center for Computer Research in Music and Acoustics, Stanford University); Max Jardetzky (Center for Computer Research in Music and Acoustics, Stanford University); Ge Wang (Center for Computer Research in Music and Acoustics, Stanford University)","Kim, Kunwoo*; Zhu, Andrew; Murakami, Eito; van Zyl, Marise; Li, Yikai; Jardetzky, Max; Wang, Ge",Kunwoo Kim; Andrew Zhu; Eito Murakami; Marise van Zyl; Yikai Li; Max Jardetzky; Ge Wang,kunwoo@ccrma.stanford.edu*; azaday@ccrma.stanford.edu; eitom@ccrma.stanford.edu; marise@ccrma.stanford.edu; yikaili@ccrma.stanford.edu; mjardetz@ccrma.stanford.edu; ge@ccrma.stanford.edu,"Extended reality environments: augmented, virtual, mixed reality",New music performance paradigms,Accept as Long Paper (up to 6000 words),oral,in person,,,,nime2025_59.pdf,,,59.jpg,"Virtual Reality, VR Concert, VR Orchestra, VR Instrument Design"
68,Between Garment and Prosthesis:  The Design of an E-Textile Musical Interface,"This paper presents an E-textile musical interface designed to control sound through body movement in performance. Unlike traditional wearable interfaces that function as garments, this work reimagines the textile interface as a prosthetic, augmenting the performer’s body. The paper discusses the design of the interface and its sonification method, highlighting how the flexibility of E-textiles can transform bodily movement and influence interaction. This work explores the potential of E-textile interfaces to challenge conventional notions of wearability and embodiment in performance.",Qiaosheng Lyu,qslyu2-c@my.cityu.edu.hk,"Qiaosheng Lyu (City, University of Hong Kong)*; Ryo Ikeshiro ( City, University of Hong Kong)","Lyu, Qiaosheng*; Ikeshiro, Ryo",Qiaosheng Lyu; Ryo Ikeshiro,qslyu2-c@my.cityu.edu.hk*; ryo.ikeshiro@cityu.edu.hk,"Novel controllers, interfaces or instruments for musical expression","Entangled NIME: Intertwined, multilayer contexts in NIME research; Explorations of relationships between motion, gesture and music; Gesture to sound mapping",Accept as Short Paper (up to 2000 words),oral,remote,,,,nime2025_68.pdf,,,68.jpg,"E-textile, Body Instrument, Wearable Interface, Musical Prosthesis"
74,"Hacking Sound, Hacking History: Patricia Cadavid and the Electronic_Khipu_","To more fully understand researcher and artist Patricia Cadavid Hinojosa’s instrument the Electronic\_Khipu\_, we must define the project as an instance of hacking. Cadavid deconstructs colonial understandings of the Andean device known as the khipu, pulling apart the academic view of khipus as artifacts to be deciphered, the strict delineation between administrative and ritualistic uses of the khipu, and the separation of the oral tradition from the object. Through deliberate design choices and musical expression in performance, Cadavid emphasizes the inextricability of coding, art, and ritual by creating a tactile device that re-tells history and challenges the false oppositional binary between Indigeneity and technology. Understanding this project of digital lutherie as an act of creation through hacking – specifically, as the deconstruction and reconfigurement of artistic and historical components – allows us to fully appreciate its power.",Margaret Needham,mneedham@smith.edu,Margaret Needham (Smith College)*,"Needham, Margaret*",Margaret Needham,mneedham@smith.edu*,"Discussions about the artistic, cultural, and social impact of NIME technology","Entangled NIME: Intertwined, multilayer contexts in NIME research; User studies and evaluations of NIMEs",Accept as Medium Paper (up to 4000 words),oral,Does not know yet,,,,nime2025_74.pdf,,,74.jpg,"Hacking,NIME,Khipu,Tangible Interface"
75,(De)Constructing Timbre at NIME: Reflecting on Technology and Aesthetic Entanglements in Instrument Design,"Timbre, pitch, and timing are often relevant in digital musical instrument (DMI) design. Compared with the latter two, timbre is neither easy to define nor discretise when negotiating audio representations and gesture-sound mappings. We conduct a corpus assisted discourse analysis of ``timbre'' in all NIME proceedings to date (2001--2024). Combining this with a detailed review of 18 timbre-focused papers at NIME, we examine how definitions of timbre and timbre interaction methods are constructed through, for instance, Wessel's numerical timbre control space, synthesis tools and programming languages, machine learning and AI approaches, and other trends in digital lutherie practices. While acknowledging the practical utility of technical constructions of timbre in NIME (and other digital music research communities), we contribute discussion on the entanglement of technology and aesthetics in instrument design, which constitutes what ``timbre'' becomes in NIME research and reflect on how timbre description becomes prescription: how DMIs and musical practices have been reconstituted around particular timbral values operationalised in NIME. In response, we propose ways that the NIME community can embrace more critical approaches and awareness to how our methods and tools shape and co-create our notions of timbre, as well as other musical concepts, connecting more openly with diverse types of sonic phenomena.",Charalampos Saitis,c.saitis@qmul.ac.uk,Charalampos Saitis (Queen Mary University of London)*; Courtney N. Reed (Loughborough University London); Ashley Laurent Noel-Hirst (Queen Mary University of London); Giacomo Lepri (University of Genoa); Andrew McPherson (Imperial College London),"Saitis, Charalampos*; Reed, Courtney N.; Noel-Hirst, Ashley Laurent; Lepri, Giacomo; McPherson, Andrew",Charalampos Saitis; Courtney N. Reed; Ashley Laurent Noel-Hirst; Giacomo Lepri; Andrew McPherson,c.saitis@qmul.ac.uk*; c.n.reed@lboro.ac.uk; a.l.noel-hirst@qmul.ac.uk; giacomo.lepri@edu.unige.it; andrew.mcpherson@imperial.ac.uk,"Entangled NIME: Intertwined, multilayer contexts in NIME research","Discussions about the artistic, cultural, and social impact of NIME technology ; Historical, theoretical or philosophical discussions about designing or performing with new interfaces ; Music-related human-computer interaction",Accept as Long Paper (up to 6000 words),oral,remote,,,,nime2025_75.pdf,,,75.jpg,"Timbre, Entanglement, Metaphor, Reification, Constructivism"
78,Towards Neurodiverse Sensemaking: Pluralizing Agency in Wearable Music and Participatory Workshopping,"We, a team of teachers and researchers, share examples of collectively playable instruments that challenge normative assumptions about intention and agency in digital musical instruments. These instruments enliven neurodiverse sensemaking in participatory design and STEAM learning. Through a multiyear research-practice partnership (RPP), we collaborated with teaching fellows to co-design a curriculum for neurodiverse middle school students that activates computational thinking (CT). This collaboration led to a web-based, quasi-modular interface connected to wearable music sensors. We situate our work within the growing literature on participatory design of collaborative accessible digital musical instruments (CADMIs). We describe how our co-design methods address the complex demands of ecosystemic thinking, sensitive to the varied entanglements that complicate traditional human-computer interaction (HCI) design and evaluation methods. Our pedagogical and methodological approach diverges from deficit-focused strategies that aim to develop neurotypical communication skills in neurodivergent individuals. Instead, we promote cross-neurotype collaboration without presuming a single mode of ""correct"" communication. Furthermore, we surface the potential of CADMIs by linking this notion to a pluralization of agency that extends beyond one-to-one body-sensor relationships. We develop accessible instruments within neurodiversity and autism contexts, avoiding reification of mindbody relations and recognizing them as dynamic, field-like, and embedded in facilitative relations for these communities.",Seth Thorn,seth.thorn@asu.edu,Seth Thorn (Arizona State University)*; Anani Vasquez (Neurodiversity Education Research Center); Corey Reutlinger (Arizona State University); Margarita Pivovarova (Arizona State University); Mirka Koro (Arizona State University),"Thorn, Seth*; Vasquez, Anani; Reutlinger, Corey; Pivovarova, Margarita; Koro, Mirka",Seth Thorn; Anani Vasquez; Corey Reutlinger; Margarita Pivovarova; Mirka Koro,seth.thorn@asu.edu*; avasquez@neurodiversitycenter.org; creutlin@asu.edu; Margarita.Pivovarova@asu.edu; Mirka.Koro@asu.edu,Accessibility and interfaces for musical expression for people with special needs,"Entangled NIME: Intertwined, multilayer contexts in NIME research; Pedagogical perspectives and/or student projects in NIME-related courses",Accept as Long Paper (up to 6000 words),oral,remote,,,,nime2025_78.pdf,,,78.jpg,"Neurodiversity, Accessible Digital Musical Instrument, Computational Thinking, Wearable Music, Entanglement"
88,The Slide-A-Phone: a Tactile Accessible Musical Instrument,"AMIs, Inclusive design, autobiographical design This paper details the design and development of the Slide-A-Phone, an Accessible Musical Instrument (AMI). The first author's spinal cord injury in 2004 hindered their ability to play traditional instruments, which motivated the development of the Slide-A-Phone. The Slide-A-Phone utilises tactile interfaces coupled with analogue and digital sensors to replicate the playability and expressive control of a saxophone, the instrument the first author used to play before the incident. The design process incorporated phenomenological perspectives and a blend of design methodologies, with the specific goal of fostering a robust musician-instrument relationship. We report insights into how personal experiences shape design and functionality and the importance of accessible instruments in enabling creative practice and performance for individuals with limited functionality. We also describe the design and technical implementation of the Slide-A-Phone evaluate the instrument's effectiveness and reflect on its potential to enhance musical engagement, social connections, cultural participation, and professional development.",Andrew McMillan,drewmcm@blackbridge.co.nz,Andrew McMillan (University of Auckland)*; Fabio Morreale (The University of Auckland),"McMillan, Andrew*; Morreale, Fabio",Andrew McMillan; Fabio Morreale,drewmcm@blackbridge.co.nz*; f.morreale@auckland.ac.nz,Accessibility and interfaces for musical expression for people with special needs,Evaluation and user studies of new interfaces for musical expression ; Practice-based research approaches/methodologies/criticism ; User studies and evaluations of NIMEs,Accept as Medium Paper (up to 4000 words),oral,remote,,,,nime2025_88.pdf,,,88.jpg,"AMIs, Inclusive design, autobiographical design, Evaluation"
90,Longevity of Deep Generative Models in NIME: Reflections on Reactivation,"In this paper, we first evaluate whether Deep Generative Models (DGM) presented at NIME are still available, afterward we present the recreation process of seven DGM models from previous NIME conferences to the end of investigation of the issues related to NIME DGM longevity and documentation. We examine the availability and completeness of resources needed to recreate DGM models, and discuss the challenges encountered during recreation. We highlight key challenges and propose guidelines to improve documentation and future reuse of DGM models in NIME.",Isaac Clarke,ijclarke590@connect.hkust-gz.edu.cn,"Isaac Clarke (The Hong Kong University of Science and Technology (Guangzhou))*; Francesco Ardan  Dal Rí (Department of Information Engineering and Computer Science, University of Trento); Raul Masu (The Hong Kong University of Science and Technology (Guangzhou))","Clarke, Isaac*; Dal Rí, Francesco Ardan ; Masu, Raul",Isaac Clarke; Francesco Ardan  Dal Rí; Raul Masu,ijclarke590@connect.hkust-gz.edu.cn*; francesco.dalri-2@unitn.it; raul@raulmasu.org,Machine learning and artificial intelligence in NIMEs,"Practice-based research approaches/methodologies/criticism ; Software frameworks, interface protocols, and data formats, for supporting musical interaction",Accept as Medium Paper (up to 4000 words),oral,Does not know yet,,,,nime2025_90.pdf,,,90.jpg,"longevity, reuse, recreation, machine learning, documentation, sustainability"
96,Embodying Sustainability: Paving Opportunities for NIME Research,"While sustainability has gained attention in NIME research, primarily focusing on instrument longevity and durability, the role musical interfaces in promoting environmental awareness remains unexplored. This paper investigates how musical interfaces can foster sustainability through designing embodied experiences. We present a literature review examining the integration of sustainability and embodiment in sonic interaction, synthesize practical points on how sound, materials, data, and interactions can support embodying sustainability, and explore these concepts through a design case study. Our findings suggest that embodied musical experiences offer unique opportunities to cultivate environmental consciousness, contributing to a deeper understanding of sustainable musical interfaces relying on artistic expressions.",Xinran Chen,xchen805@connect.hkust-gz.edu.cn,"Xinran Chen (The Hong Kong University of Science and Technology (Guangzhou))*; Raul Masu (The Hong Kong University of Science and Technology (Guangzhou)); Mela Bettega (Open Lab, School of Computing, Newcastle University); Iurii Kuzmin (The Hong Kong University of Science and Technology)","Chen, Xinran*; Masu, Raul; Bettega, Mela; Kuzmin, Iurii",Xinran Chen; Raul Masu; Mela Bettega; Iurii Kuzmin,xchen805@connect.hkust-gz.edu.cn*; raul@raulmasu.org; mela.bettega@newcastle.ac.uk; ikuzmin@connect.ust.hk,"Entangled NIME: Intertwined, multilayer contexts in NIME research","Discussions about the artistic, cultural, and social impact of NIME technology ; Music-related human-computer interaction ; Sonic interaction design",Accept as Long Paper (up to 6000 words),oral,in person,,,,nime2025_96.pdf,,,96.jpg,"Embodiment, Sustainability, Environmental awareness, Aesthetics, Sustainability through design (StD)"
107,EMMA: Enhancing Real-Time Musical Expression through Electromyographic Control,"This paper presents the Electromyographic Music Avatar (EMMA), a digital musical instrument (DMI) designed to enhance real-time sound-based composition through gestural control. Developed as part of a doctoral research project, EMMA combines electromyography (EMG) and motion sensors to capture nuanced finger, hand, and arm movements, treating each finger as an independent instrument. This approach bridges embodied performance with computational sound generation, enabling expressive and intuitive interaction. The system features a glove-based design with EMG sensors for each finger and motion detection for the wrist and arm, allowing seamless control of musical parameters. By addressing key challenges in DMI design, such as action-sound immediacy and performer-instrument dynamics, EMMA contributes to developing expressive and adaptable tools for contemporary music-making.",João  Coimbra,jpcoimbra@ua.pt,João  Coimbra (Aveiro University/INET-md)*; Luís Aly (Escola Superior de Media Artes e Design/IPP); Henrique  Portovedo (Aveiro University/INET-md); Tiago  Bolaños (Instituto Superior Técnico/IT); Sara Carvalho (Aveiro University/INET-md),"Coimbra, João *; Aly, Luís; Portovedo, Henrique ; Bolaños, Tiago ; Carvalho, Sara",João  Coimbra; Luís Aly; Henrique  Portovedo; Tiago  Bolaños; Sara Carvalho,jpcoimbra@ua.pt*; luisaly@esmad.ipp.pt; henriqueportovedo@ua.pt; tiago.bolanos@tecnico.ulisboa.pt; scarvalho@ua.pt,"Novel controllers, interfaces or instruments for musical expression","Gesture to sound mapping; Music-related human-computer interaction ; Sensor and actuator technologies, including haptics and force feedback devices",Accept as Short Paper (up to 2000 words),oral,remote,,,,nime2025_107.pdf,,,107.jpg,"Gestural interface, Electromyography, Digital Musical Instrument, Human-Computer Interaction, Performance"
115,"Evolving the Living Looper: Artistic Research, Online Learning, and Tentacle Pendula","This is an update on a software NIME project called the Living Looper, including a description of the evolving interface, enhancements to the learning algorithm, and musician perspectives. A new graphical interface facilitates use of the instrument by non-programmers and visualizes each loop to aid performers in tracking which loop is making which sound. We describe a new living loop algorithm based on incremental partial least squares regression. We also report on an artistic project using the Looper and lessons learned, resulting in an increased importance of training data and a developing sense of relationality.",Victor Shepardson,victorshepardson@hi.is,Victor Shepardson (Intelligent Instruments Lab)*; Thor Magnusson (Intelligent Instruments Lab); Halla Steinunn Stefánsdóttir (Intelligent Instruments Lab),"Shepardson, Victor*; Magnusson, Thor; Stefánsdóttir, Halla Steinunn",Victor Shepardson; Thor Magnusson; Halla Steinunn Stefánsdóttir,victorshepardson@hi.is*; thormagnusson@hi.is; hallasteinunn@hi.is,"Novel controllers, interfaces or instruments for musical expression","Augmented, embedded and hyper instruments ; Machine learning and artificial intelligence in NIMEs ; Machine learning in musical performance",Accept as Medium Paper (up to 4000 words),oral,in person,,,,nime2025_115.pdf,,,115.jpg,"software,machine learning,neural synthesis,looper"
129,Aquapella- Gestural Interactions with Liquid Turbulence as Musical Expression,"The Aquapella is a hand-held gestural instrument for exploring the unique relationship of liquid turbulence and musical expression. The device consists of eight conductive water-level sensors in a custom 3D printed container. As a musician moves the device, it generates a chaotic flow of water within the container and translates the motion into real-time midi signals for audio-visual interpretation. In our initial performances and tests with the Aquapella, we have focused on turning the flowing characteristics of the device into ambient and glitch soundscapes that move between noise and harmonics. We present the primary findings in developing the Aquapella including related works, description of the project development, and ideas for future iterations.",John Lettang,john.lettang@colorado.edu,John Lettang (University of Colorado Boulder)*; August Black (University of Colorado Boulder),"Lettang, John*; Black, August",John Lettang; August Black,john.lettang@colorado.edu*; augustblack@gmail.com,"Novel controllers, interfaces or instruments for musical expression","Explorations of relationships between motion, gesture and music; Gesture to sound mapping; Sensor and actuator technologies, including haptics and force feedback devices",Accept as Long Paper (up to 6000 words),oral,in person,,,,nime2025_129.pdf,,,129.jpg,"Water Instrument, Gestural Instrument, Tangible Interface"
131,Touching Wires: tactility and a quilted musical interface for human-AI musical co-creation,"Interactions with computers have traditionally been mediated by rigid materials, but as technology evolves, there is increasing potential to rethink these relationships. This paper explores how soft, textile-based interfaces can reshape human-AI interaction, particularly in musical co-creation. We introduce a textile-based human-AI system used both for musical performance and public interaction. This system enables embodied, tactile engagement, offering users a more intuitive, playful, and participatory experience with a digital synthesiser. An AI agent provides machine input to the system to participate in the human-AI musical co-creation. Our findings reveal that users engage more actively with such systems, finding joy in exploring new forms of interaction. Trust in the system grows as users familiarise themselves with its context, and they view AI outputs as valuable sources of creative inspiration. This work demonstrates that soft materials provide unique opportunities for enhancing user engagement and creativity in AI systems. By integrating tactile interfaces, we contribute to the broader conversation on designing more accessible, engaging, and emotionally resonant human-AI collaborations.",Sandy Ma,sandy.ma@anu.edu.au,Sandy Ma (ANU)*; Charles Martin (Australian National University),"Ma, Sandy*; Martin, Charles",Sandy Ma; Charles Martin,sandy.ma@anu.edu.au*; charles.martin@anu.edu.au,"Novel controllers, interfaces or instruments for musical expression",User studies and evaluations of NIMEs,Accept as Long Paper (up to 6000 words),oral,in person,,,,nime2025_131.pdf,,,131.jpg,"human-AI interaction, e-textiles, tangible and embodied interaction"
134,NIME: A Mis-User's Manual,"Ever since the initiating workshop at the 2001 ACM CHI'01 Conference, annual New Interfaces for Musical Expression conferences have seen a proliferation of work featuring different forms of music, research values, philosophical, ethical and political standpoints. The 2025 ‘Entangled’ theme celebrates this diversity of creative, technical, and social ‘intelligencings’ (Thrift [68, p153-154]). It is precisely the non- or pluri-paradigmatic character of NIME that is its strength.    Drawing on Maria Lugones [41], we characterise NIME less as an entangled weave—where threads maintain their separate yet assembled and interconnected character—than as a ‘curdling’ where relationships are more complex, varied, mutually interrupting and shaping, indeterminate and unknown without careful dialogue. We do not consider it appropriate to offer unifying frameworks or mappings with often hidden authoritarian implications. Rather, following Rancière [5], we prefer a radically democratic dissensus and, following Lugones, a spirit of ‘festive resistance’ where we poke at the limits of our inherited metaphors to undermine attempts to provide a fixed orderliness, (re)framing topics to kickstart exchange on new fertile grounds for collaboration. Multiple kinds and collisions of agency, and the lively openness of what some might deem ‘failure’ are prioritised over the often inhibiting closure and certainty of ‘success’ [e.g. 10, 11, 12, 40].    Our topics include: multiple ways of making as a means of maximising exposure to possible failure; shifting from interfaces to interfacing to create arenas for action rather than tools for purposes; foregrounding risk, inefficiency and forgetting; formulating improvisation as knowing-when and composing-the-now; performance practice, settings and contingencies; alternative resourcings/reframings for research; a wild spirit of tactical oppositionalism, dynamic uncompromise, and existential pluralism, to embrace the independence of divergent voices.",Sally Jane Norman,sallyjane.norman@vuw.ac.nz,"Sally Jane Norman (New Zealand School of Music, Victoria University of Wellington)*; Paul Stapleton (SARC, Queen's University Belfast); John Bowers (Independent Artist Researcher)","Norman, Sally Jane*; Stapleton, Paul; Bowers, John",Sally Jane Norman; Paul Stapleton; John Bowers,sallyjane.norman@vuw.ac.nz*; p.stapleton@qub.ac.uk; john.m.bowers@gmail.com,"Discussions about the artistic, cultural, and social impact of NIME technology","Entangled NIME: Intertwined, multilayer contexts in NIME research; Historical, theoretical or philosophical discussions about designing or performing with new interfaces ; Practice-based research approaches/methodologies/criticism",Accept as Long Paper (up to 6000 words),oral,in person,,,,nime2025_134.pdf,,,134.jpg,"curdling, performance, festive resistance, dissensus, existential pluralism"
137,The Imperfect Copy: Role Playing Reenactments of Historical Electronic Sound Instruments,"Reenactment forms a unique method of exploring the social, political, historical, conceptual, contextual and other aspects of electronic sound instruments from the past, without necessarily reproducing the instrument’s physical, functional or sonic characteristics. Rather, the reenactment presents a novel instrument, realized through contemporary means, reflecting on contemporary concerns and within a contemporary context. We find reenactment complementary to conservation, maintenance, reconstruction and emulation in working with archival and museum objects. Our paper presents an analytic framework developed for use in workshop scenarios. This series of questions helps determine and understand which aspects of an instrument might be reenacted. To illustrate the process in action, we describe an example workshop wherein participants use methods of media archaeology, design fiction and role playing to imagine and reenact new features, affordances, contexts and applications of electronic instruments from a museum exhibition.",Derek Holzer,idholzer@kth.se,Derek Holzer (KTH Royal Institute of Technology in Stockholm)*; Henrik Frisk (KMH Royal College of Music Stockholm); André Holzapfel (KTH Royal Institute of Technology in Stockholm),"Holzer, Derek*; Frisk, Henrik; Holzapfel, André",Derek Holzer; Henrik Frisk; André Holzapfel,idholzer@kth.se*; henrik.frisk@kmh.se; holzap@kth.se,"Historical, theoretical or philosophical discussions about designing or performing with new interfaces","Discussions about the artistic, cultural, and social impact of NIME technology ; Entangled NIME: Intertwined, multilayer contexts in NIME research; Pedagogical perspectives and/or student projects in NIME-related courses",Accept as Long Paper (up to 6000 words),oral,Does not know yet,,,,nime2025_137.pdf,,,137.jpg,"Media archaeology, workshop, reenactment, design fiction, role playing"
138,A Visual-First Approach to Gestural Audio-Visual Composition,"In this paper we present our visual-first approach to creating gestural mappings for a new audio-visual percussion work titled Cymbalism. Unlike most audio-visual works, Cymbalism was inspired by the creation of a series of interactive visual scenes that respond to the performer's real-time movements. In leading with the visual interaction, we discuss how this approach fostered a union between the physical, audio and visual elements of the work, creating a piece where the visualisation is not simply a feedback mechanism but fundamental in inspiring compositional concepts and new ways of interacting with sound. Through practice-based research, we use the insights gained through creative development and performance outcomes to guide the continued evolution of an established wearable gestural DMI.",Sam Trolland,sam.trolland@monash.edu,Sam Trolland (Monash)*; Alon Ilsar (Monash); Jon McCormack (Monash),"Trolland, Sam*; Ilsar, Alon; McCormack, Jon",Sam Trolland; Alon Ilsar; Jon McCormack,sam.trolland@monash.edu*; alon.ilsar@monash.edu; Jon.McCormack@monash.edu,"Explorations of relationships between motion, gesture and music","Extended reality environments: augmented, virtual, mixed reality; Gesture to sound mapping; Musical mapping strategies",Accept as Long Paper (up to 6000 words),oral,in person,,,,nime2025_138.pdf,,,138.jpg,"Gesture, Audio-Visual, Instrument, Percussion, Performance, Visual"
147,Audiation Development System for Gugak's Fluid Musical Parameters Utilizing Audio Feedback Stimuli,"Gugak, the traditional music of Korea, is defined by its distinctive musical characteristics, including flexible tuning, non-metric rhythms, and intricate ornamentation. These unique features, while artistically rich, pose significant challenges for novice learners, particularly when approached through conventional, textbook-based methods. To bridge this gap, we introduce the Audiation Development System for Gugak, an interactive platform that leverages algorithmic analysis to support both learning and teaching. Central to this system is the development of signal processing functions for rhythmic guidance, pitch detection, and additive harmonic synthesis—algorithms specifically designed to capture the expressive nuances of Gugak. These functions generate real-time auditory and visual feedback, providing a responsive learning environment aligned with the updated Korean music curriculum. The system not only enables student-centred exploration of Gugak's fluid structures but also supports educators through dynamic, visualized feedback tools. Beyond its technical foundation, this research sets the stage for future development of user interfaces and investigates the educational efficacy of computer-driven learning compared to traditional methods. By integrating music technology and pedagogy, this work contributes to both the accessibility and sustainability of Korea’s musical heritage.",Michaella Moon,michaella.moon@vuw.ac.nz,Michaella Moon (Victoria University of Wellington)*; Dale Carnegie (Victoria University of Wellington); Jim Murphy ( Victoria University of Wellington),"Moon, Michaella*; Carnegie, Dale; Murphy, Jim",Michaella Moon; Dale Carnegie; Jim Murphy,michaella.moon@vuw.ac.nz*; dale.carnegie@vuw.ac.nz; jim.murphy@vuw.ac.nz,"Entangled NIME: Intertwined, multilayer contexts in NIME research","Discussions about the artistic, cultural, and social impact of NIME technology ; Historical, theoretical or philosophical discussions about designing or performing with new interfaces ; Music-related human-computer interaction",Accept as Long Paper (up to 6000 words),oral,in person,,,,nime2025_147.pdf,,,147.jpg,"Korean-traditional music genre, Gugak educational controllers, Audio stimuli, Feedback system"
153,Ongoing Production of a “Growing Instrument” Using Mycelium-Based Materials,"This study explores “growing instruments” made from fungal mycelium, highlighting their natural unpredictability and role as musical instruments. Mycelium’s growth and interactions with the environment create unique features not found in traditional instruments. Positioned within “non-human-centric approaches” in design and art, the research emphasizes the mutual creation and interconnectedness of diverse actors. By shaping mycelium into tubes and adding recorder heads, playable flute-like instruments were created. However, their condition and playability were highly influenced by conditions such as temperature and humidity. The study emphasizes embracing uncertainty and suggests that these imperfections can offer new insights into musical instrument design.",Taisei GOTO,goto.taisei.703@s.kyushu-u.ac.jp,Taisei GOTO (Kyushu University)*; Kazuhiro JO (Kyushu University),"GOTO, Taisei*; JO, Kazuhiro",Taisei GOTO; Kazuhiro JO,goto.taisei.703@s.kyushu-u.ac.jp*; jo@jp.org,Practice-based research approaches/methodologies/criticism,"Historical, theoretical or philosophical discussions about designing or performing with new interfaces",Accept as Short Paper (up to 2000 words),oral,in person,,,,nime2025_153.pdf,,,153.jpg,"Mycelium, Growing Instruments, Non-Human-Centered Design, Uncertainty, Musical Instrument Design"
160,Adaptation and Perceived Creative Autonomy in Gesture-Controlled Interactive Music,"With the variety and rapid pace of developments in Artificial Intelligence (AI), musicians can face difficulty when working with AI-based interfaces for musical expression as understanding and adaptation to AI behaviors takes time. In this paper, we explore the use of AI in an interactive music system designed to adapt to users as they learn to perform with it. We present GestAlt, an AI-based interactive music system that collaborates with a performer by analyzing their gestures and motion to generate audio changes. It uses computer vision, online machine learning, and reinforcement learning to adapt to a user's hand motion patterns and allow a user to communicate their musical goals to the system. It communicates its decision-making to the user through visualizations and its musical output. We conducted a study in which five musicians performed using this software over multiple sessions. Participants discussed how their preferences for the system’s behavior were influenced by their experiences as musicians, how adaptive reinforcement learning affected their expectations for the system’s autonomy, and how their perceptions of the system as a creatively autonomous, collaborative partner evolved as they learned how to perform with the system.",Jason Smith,jsmith775@gatech.edu,Jason Smith (Georgia Institute of Technology)*; Jason Freeman (Georgia Institute of Technology),"Smith, Jason*; Freeman, Jason",Jason Smith; Jason Freeman,jsmith775@gatech.edu*; jason.freeman@gatech.edu,Music-related human-computer interaction,Gesture to sound mapping; Machine learning and artificial intelligence in NIMEs ; Machine learning in musical performance,Accept as Long Paper (up to 6000 words),oral,in person,,,,nime2025_160.pdf,,,160.jpg,"Interactive Music, Artificial Intelligence, Machine Learning, Human-Computer Interaction"
161,Threading the Sound: The Carpet Tufting Gun as an Electroacoustic Performance Interface,"This paper explores the carpet tufting gun as a novel electroacoustic performance interface. Leveraging its distinctive acoustic properties and electromechanical kinetics, the tufting gun presents a range of physical affordances that can be creatively repurposed for musical expression. While prior intersections between textile production processes and musical practices exist, the tufting gun remains largely underexplored as a tool for structured musical composition. This work reimagines the gun’s mechanical gestures and performative affordances, transforming its utilitarian motions into expressive sonic gestures. By positioning the tufting gun as both an acoustic source and an interactive performance interface, this project works at the intersection of fibre craft and experimental sound art, where both historico-cultural context of textile making, and the ergonomics of the gun, present musical affordances.",Joseph Burgess,unregisteredmasterbuilder@gmail.com,Joseph Burgess (University of the Sunshine Coast)*; Toby Gifford (University of the Sunshine Coast),"Burgess, Joseph*; Gifford, Toby",Joseph Burgess; Toby Gifford,unregisteredmasterbuilder@gmail.com*; tgifford@usc.edu.au,"Novel controllers, interfaces or instruments for musical expression",Musical mapping strategies ; New music performance paradigms; Sonic interaction design,Accept as Short Paper (up to 2000 words),oral,in person,,,,nime2025_161.pdf,,,161.jpg,"Carpet, Tufting, Electroacoustic, Interfaces"
187,Entangled Sense: Designing Sensory NIME for Autism,"This paper explores how Entangled NIME may inform the design of musical interfaces tailored for children with Autism Spectrum Disorder (ASD), focusing on their sensory processing challenges. Given the prevalence of sensory over-responsivity (SOR) and under-responsivity (SUR) in ASD, traditional sensory interventions often fail to accommodate the highly individualized and fluctuating sensory needs of autistic individuals. The authors highlight the potential for multisensory NIME to address the diverse range of sensory needs, promoting emotional regulation and sensory balance through new creative musical opportunities and activities. This paper presents research in the form of a narrative review and comparative case study of recent NIME and sensory intervention research, exploring emerging approaches, rhythm-based interventions, generative algorithms, play-centered designs and other possibilities for enhancing sensory engagement and emotional regulation. Drawing on insights from 30 recent NIME papers, this research explores the boundaries of current approaches and seeks to establish an understanding of multisensory NIME for ASD. The research underscores the profound variability in sensory profiles for ASD, necessitating a shift from clinician-directed interventions to creative, inclusive, multisensory solutions. Finally, a set of sensory NIME design principles are offered, emphasizing the importance of sensory perception, sensory equilibrium and the promotion of emotional regulation for ASD.",Stuart Favilla,sfavilla@swin.edu.au,Aditya Arora (Swinburne University of Technology); Stuart Favilla (Swinburne University of Technology)*; Erica Tandori (Monash University); James Marshall (Swinburne University of Technology),"Arora, Aditya; Favilla, Stuart*; Tandori, Erica; Marshall, James",Aditya Arora; Stuart Favilla; Erica Tandori; James Marshall,104206190@student.swin.edu.au; sfavilla@swin.edu.au*; erica.tandori@monash.edu; jgmarshall@swin.edu.au,Accessibility and interfaces for musical expression for people with special needs,"Entangled NIME: Intertwined, multilayer contexts in NIME research",Accept as Medium Paper (up to 4000 words),oral,in person,,,,nime2025_187.pdf,,,187.jpg,"music interfaces, autism, multisensory"
195,The Memory Cloud: Personal media libraries as affordance and constraint,"The Memory Cloud is a musical instrument that uses a player’s own library of personal recordings as sonic material. This paper presents the design of the instrument, situating it within sustainability HCI studies and constraints-based design, before describing the instrument being used by two musicians in a professional context. Over 2000 sounds from the musician's personal cloud library, dating back over 10 years, were placed in the instrument as the only sonic material available for exploring. I argue that a radically small scale and personal approach could be one strategy for addressing the issues of longevity in NIME, and I suggest that using personal media libraries presents a potential affordance and constraint for musical instrument design.",Yann Seznec,yannse@kth.se,Yann Seznec (KTH Royal Institute of Technology)*,"Seznec, Yann*",Yann Seznec,yannse@kth.se*,"Novel controllers, interfaces or instruments for musical expression","Discussions about the artistic, cultural, and social impact of NIME technology ; Entangled NIME: Intertwined, multilayer contexts in NIME research",Accept as Medium Paper (up to 4000 words),oral,remote,,,,nime2025_195.pdf,,,195.jpg,"instrument, sustainability, sound, music, experimental"
198,Designing Percussive Timbre Remappings: Negotiating Audio Representations and Evolving Parameter Spaces,"Timbre remapping is an approach to audio-to-synthesizer mapping that aims to transfer timbral expressions from a source instrument onto synthesizer controls. This is a complicated process, partially owing to the difficulties in defining timbre and determining how to represent it, but also due to challenges in the process of navigating the complex relationship between synthesizer parameter spaces and their sonic output. In this work, we focus on real-time timbre remapping with percussion instruments, combining technical development with practice-based methods to navigate these challenges. As a technical contribution, we introduce a genetic algorithm for generating datasets of synthesizer presets that vary according to target timbres, enabling application to black-box synthesizers, including VSTs and hardware modular synthesizers. Additionally, we propose a neural network-based approach that predicts features from short onset windows, supporting low-latency performance and high-level feature-based control. Our technical development is grounded in musical practice, demonstrating how iterative and collaborative processes—drawing on episodes of musical practice and reflection—can yield insights into open-ended challenges in DMI design. Through experiments on various audio feature representations, we uncover meaningful insights into timbre remapping by coupling data-driven design with practice-based reflection. This work is accompanied by an annotated portfolio, presenting a series of musical performances and experiments with reflections.",Jordan Shier,j.m.shier@qmul.ac.uk,Jordan Shier (Queen Mary University of London)*; Rodrigo Constanzo (Royal Nothern College of Music); Charalampos Saitis (Queen Mary University of London); Andrew Robertson (Ableton AG); Andrew McPherson (Imperial College London),"Shier, Jordan*; Constanzo, Rodrigo; Saitis, Charalampos; Robertson, Andrew; McPherson, Andrew",Jordan Shier; Rodrigo Constanzo; Charalampos Saitis; Andrew Robertson; Andrew McPherson,j.m.shier@qmul.ac.uk*; rodrigo.constanzo@rncm.ac.uk; c.saitis@qmul.ac.uk; andrew.robertson@ableton.com; andrew.mcpherson@imperial.ac.uk,Gesture to sound mapping,"Augmented, embedded and hyper instruments ; Machine learning and artificial intelligence in NIMEs",Accept as Long Paper (up to 6000 words),oral,in person,,,,nime2025_198.pdf,,,198.jpg,"synthesizer parameter mapping, timbre remapping, machine learning, practice-led research,"
213,O一: An Epistemic DMI for Cross-Cultural Reflection on Time and Music,"This paper introduces a Digital Musical Instrument (DMI) that inscribed linear and circular conceptions of time, inspired by Western teleological and Eastern, particularly Chinese, time philosophies. The DMI employs two boards equipped with ESP32 chips for Wi-Fi control and WS2812 LEDs, providing a visual and sonic representation of these temporal frameworks. It features interactive boxes, each fitted with a Light Dependent Resistor (LDR) sensor, utilizing ESP32 Mini boards to facilitate empty protocol activation. The project collaborates with composers from diverse cultural backgrounds to explore co-composition and a reflective session with three composers. Through its design and co-composing practice, the DMI underscores the role of DMIs as epistemic tools in promoting cultural understanding and critically highlighting the socio-cultural role of technology. Through co-composing and reflection, the significance of rediscovering time in contemporary globalization and philosophy was explored. This process challenges the view of justifying time as merely a measurement parameter, striving to reveal the importance of understanding musical time across different cultural contexts. It discovered time's role as an impediment in art, influencing music practice. This practice wishes to expand the constitutive role of musical time, demonstrating its diversity and prompting a reflective layer of the perception of performative and musical time in NIME.",Hanyu Qu,hqu817@connect.hkust-gz.edu.cn,"Hanyu Qu (Hong Kong University of Science and Technology(Guangzhou))*; Francesco Dal Rí  (Department of Information Engineering and Computer Science University of Trento, IT); Hao Zou (University of Missouri-Kansas City); Hanqing Zhou (Southern University of Science and Technology); Raul Masu (Hong Kong University of Science and Technology (Guangzhou))","Qu, Hanyu*; Dal Rí , Francesco; Zou, Hao; Zhou, Hanqing; Masu, Raul",Hanyu Qu; Francesco Dal Rí; Hao Zou; Hanqing Zhou; Raul Masu,hqu817@connect.hkust-gz.edu.cn*; francesco.dalri-2@unitn.it; zouhaocomposer@gmail.com; 12331483@mail.sustech.edu.cn; raul@raulmasu.org,"Discussions about the artistic, cultural, and social impact of NIME technology","Entangled NIME: Intertwined, multilayer contexts in NIME research; Technologies or systems for collaborative music-making",Accept as Long Paper (up to 6000 words),oral,in person,,,,nime2025_213.pdf,,,213.jpg,"Digital Musical Instrument (DMI), Epistemic DMI, co-composition, time conception"
222,Gesture-Driven DDSP Synthesis for Digitizing the Chinese Erhu,"This paper presents a gesture-controlled digital Erhu system that merges traditional Chinese instrumental techniques with contemporary machine learning and interactive technologies. By leveraging the Erhu’s expressive techniques, we develop a dual-hand spatial interaction framework using real-time gesture tracking. Hand movement data is mapped to sound synthesis parameters to control pitch, timbre, and dynamics, while a differentiable digital signal processing (DDSP) model, trained on a custom Erhu dataset, transforms basic waveforms into authentic timbre which remians sincere to  the instrument’s nuanced articulations. The system bridges traditional musical aesthetics with digital interactivity, emulating Erhu bowing dynamics and expressive techniques through embodied interaction. The study contributes a novel framework for digitizing Erhu performance practices, explores methods to align culturally informed gestures with DDSP-based synthesis, and offers insights into preserving traditional instruments within digital music interfaces.",Wenqi WU,wwu252@connect.hkust-gz.edu.cn,"Wenqi WU (Computational Media and Art, The Hong Kong University of Science and Technology (Guangzhou))*; Hanyu QU (Computational Media and Art, The Hong Kong University of Science and Technology (Guangzhou))","WU, Wenqi*; QU, Hanyu",Wenqi WU; Hanyu QU,wwu252@connect.hkust-gz.edu.cn*; hqu817@connect.hkust-gz.edu.cn,"Explorations of relationships between motion, gesture and music","Discussions about the artistic, cultural, and social impact of NIME technology ; Gesture to sound mapping; Machine learning in musical performance",Accept as Medium Paper (up to 4000 words),oral,in person,,,,nime2025_222.pdf,,,222.jpg,"DDSP,Erhu,Gesture,Chinese Instrument,Interactive Music Performance"
229,From Performance to Installation: How Interactive Reinforcement Learning Reframes the Roles of Performers and Audiences,"This paper explores how interactive reinforcement learning (IRL) reconfigures the roles of performers and audiences in audiovisual performance and immersive installation. We adapt the Co-Explorer (a software tool originally developed for musical co-creation) to audiovisual immersive contexts and examine its creative potential using a reflexive research-creation approach. Our study reveals how IRL splits the role of the performer into three distinct positions: (1) the designer, who defines the parametric space; (2) the guide, who reinforces the agent’s behavior; and (3) the machine performer, whose actions are shaped by interactive training. As IRL introduces agency into the creative process, it transforms traditional notions of authorship and control, enabling unexpected emergent outcomes. By showcasing an interactive installation/performance, we further explore how audiences contribute to collective creation through reinforcement-based interaction. Our findings underscore the challenges of balancing the temporality of IRL with the demands of public-facing works and of adapting RL-based systems to different exhibition contexts. Our work contributes to the discourse on co-creative systems, emphasizing the evolving roles of artists, artificial agents, and audiences in hybrid creative ecosystems.",Vincent Cusson,cusson.v@gmail.com,Vincent Cusson (UQAM)*; Danny Perreault (UQAM); Sofian Audry (UQAM); Victor Drouin (UQAM); David Drouin (UQAM),"Cusson, Vincent*; Perreault, Danny; Audry, Sofian; Drouin, Victor; Drouin, David",Vincent Cusson; Danny Perreault; Sofian Audry; Victor Drouin; David Drouin,cusson.v@gmail.com*; perreault.dan@gmail.com; audry.sofian@uqam.ca; victordrouint@gmail.com; drouin.david@courrier.uqam.ca,Machine learning and artificial intelligence in NIMEs,Machine learning in musical performance; Music-related human-computer interaction ; Practice-based research approaches/methodologies/criticism,Accept as Medium Paper (up to 4000 words),oral,remote,,,,nime2025_229.pdf,,,229.jpg,"art Installation, interactive machine learning, reinforcement learning, audiovisual performance"
230,Enabling Embodied Music-Making for Non-Musicians,"Many advancements in music technology are aimed at musicians and artists. In this paper we present a Research through Design exploration of the potential for using tangible and embodied interactions to enable active music experiences - musicking - for non-musicians. We present the Tubularium prototype, which aims to facilitate music-making to non-musicians by not requiring any initial skill while still eliciting agency and overall, providing a meaningful experience. We present the design of the prototype and the features implemented and reflect on insights from a public event in which the prototype was trialed.",Anders Løvlie,asun@itu.dk,Lucía Montesínos (IT University of Copenhagen); Halfdan Hauch Jensen (IT University of Copenhagen); Anders Løvlie (IT University of Copenhagen)*,"Montesínos, Lucía; Jensen, Halfdan Hauch; Løvlie, Anders*",Lucía Montesínos; Halfdan Hauch Jensen; Anders Løvlie,lumo@itu.dk; halj@itu.dk; asun@itu.dk*,"Novel controllers, interfaces or instruments for musical expression",Machine learning and artificial intelligence in NIMEs ; Technologies or systems for collaborative music-making,Accept as Medium Paper (up to 4000 words),oral,in person,,,,nime2025_230.pdf,,,230.jpg,"Embodied Interaction, Tangible Interaction, Musicking, Non-Musicians, Research through Design"
232,AR Matchmaking: The Compatibility of Musical Instruments with an AR Interface,"Augmented Reality (AR) interfaces present new possibilities for musical expression by extending the interaction capabilities of acoustic and electronic instruments. The AR Matchmaking experiment explores the compatibility of various musical instruments with the ARCube, an AR interface for 3D spatial control. The user study includes 12 skilled musicians across a diverse selection of instruments. Participants engaged in structured tasks, free exploration and a creative performance task, to evaluate the playability and integration of their instruments with the ARCube. User feedback was gathered between tasks and after the experiment. Thematic analysis of open-ended survey responses identified key usability challenges, including real-time control limitations for two-handed instruments and gesture misinterpretation. Despite these issues, participants explored spatial sound manipulation, hybrid performance techniques, and interactive improvisation. The results show distinct interaction patterns based on instrument type and revealed challenges and opportunities for AR-enhanced musical performance. This experiment contributes to the understanding of AR interfaces in musical performances which informs future developments in hybrid interactive music systems.",Hyunkyung Shin,luhee2458@gmail.com,Hyunkyung Shin (Georgia Institute of Technology)*; Henrik von Coler (Georgia Institute of Technology),"Shin, Hyunkyung*; von Coler, Henrik",Hyunkyung Shin; Henrik von Coler,luhee2458@gmail.com*; hvc@gatech.edu,Evaluation and user studies of new interfaces for musical expression,"Augmented, embedded and hyper instruments ; Extended reality environments: augmented, virtual, mixed reality; Music-related human-computer interaction",Accept as Long Paper (up to 6000 words),oral,remote,,,,nime2025_232.pdf,,,232.jpg,"Augmented Reality Interface, Spatial Sound, User Experience Evaluation"
253,The Sound Tree Project: Developing Personal and Collective Expression with Accessible Digital Musical Instruments,"The Sound Tree Project investigates how accessible digital musical instruments (ADMIs) can champion both personal and collective musical expression. Through a sustained six-month ethnographic engagement with five performers and two support artists, we explored how to create personalised instruments for a public per- formance outcome. The technical framework combined multiple wireless motion sensor devices placed inside different objects and the development of a real-time movement-to-sound pro- cessing hub within a live coding environment. The performance was centred on an accessible sound sculpture, the Sound Tree, where digital instruments coexisted with traditional sound mak- ing objects. Drawing from our shared process of experimentation, improvisation, and personalised instrument creation, we present some key ‘magic moments’ that were woven into the final per- formance. The emergence of these moments demonstrate the value of real-time system adaptation in encouraging individual expression, the importance of sustained engagement in develop- ing personalised instruments and having effective strategies for balancing personal and collective music-making. These insights have implications in developing accessible music technology and broader approaches to designing technologies that support diverse forms of creative collaboration.",Steph OHara,stephen.ohara@monash.edu,"Steph OHara (SensiLab, Monash University)*; Alon Ilsar (SensiLab, Monash University)","OHara, Steph*; Ilsar, Alon",Steph OHara; Alon Ilsar,stephen.ohara@monash.edu*; alon.ilsar@monash.edu,Accessibility and interfaces for musical expression for people with special needs,"Explorations of relationships between motion, gesture and music; Gesture to sound mapping; Musical mapping strategies",Accept as Medium Paper (up to 4000 words),oral,in person,,,,nime2025_253.pdf,,,253.jpg,"Interaction, Accessible, Co-design"
266,Physical Music Albums in the Digital Era: Exploring Experiential Value Through the Integration of AR,"This study explores physical music albums in the digital age, as well as the creation of new music experiences through the integration of Augmented Reality (AR) into physical albums. An online survey was conducted to examine the differences in experiences between digital and physical albums, and this informed the development of a physical music album incorporating AR. We provided 8 K-POP fans, who engage with physical albums more frequently than fans of other genres, the opportunity to test existing AR-integrated albums and a new prototype featuring AR packaging animations, multiplayer virtual concerts, and interactive photo features. The results underscored the importance of understanding and respecting fan culture when using AR.  The results suggest that, compared to digital albums, physical albums derive significant experiential value from traditional supplementary materials such as booklets and lyric cards. However, AR has the potential as a complementary new material that provides users with novel experiences.  This work leads to a reconsideration of Walter Benjamin’s concept of “aura,” which critiques the reproducibility of art.",Ena Fumihira,ena.svt@gmail.com,Ena Fumihira (University of Technology Sydney)*; Andrew Johnston (University of Technology Sydney),"Fumihira, Ena*; Johnston, Andrew",Ena Fumihira; Andrew Johnston,ena.svt@gmail.com*; andrew.johnston@uts.edu.au,"Extended reality environments: augmented, virtual, mixed reality","Discussions about the artistic, cultural, and social impact of NIME technology ; Evaluation and user studies of new interfaces for musical expression ; Music-related human-computer interaction",Accept as Medium Paper (up to 4000 words),oral,in person,,,,nime2025_266.pdf,,,266.jpg,"Augmented Reality (AR), Physical Albums, Music Experience, Fan Culture, Walter Benjamin"
277,Exploring the impact of spatial awareness on large-scale AR DMIs,"Large-scale Digital Musical Instruments (DMIs) offer immersive performance experiences and rich forms of expression, but often pose physical challenges and limit accessibility.  Traditional large-scale DMIs' size limits performers' ability to interact with the instrument, causing discomfort when engaging with distant components, highlighting the need for more flexible and user-friendly large-scale DMI designs. We present an Augmented Reality (AR) DMI that removes physical constraints by allowing performers to customise the instrument’s size and layout according to their performance environment. We aim to show how AR-based configuration supports immersive performance, promotes expressive gestures, and improves spatial awareness without sacrificing large-scale instrument capabilities. Our user study revealed increased physical engagement and spatial immersion, a strong sense of ownership, and a positive user experience. These findings indicate that our AR DMI is creatively empowering, reasonably addressing the constraints of large-scale instruments. Our research emphasises the potential of AR to enable flexible and customisable DMI design where interfacems can be adapted to suit the neeeds of individual performers.",Qiance Zhou,u7520051@anu.edu.au,Qiance Zhou (Australian National University)*; Charles Martin (Australian National University),"Zhou, Qiance*; Martin, Charles",Qiance Zhou; Charles Martin,u7520051@anu.edu.au*; charles.martin@anu.edu.au,"Extended reality environments: augmented, virtual, mixed reality",Music-related human-computer interaction ; User studies and evaluations of NIMEs,Accept as Medium Paper (up to 4000 words),oral,in person,,,,nime2025_277.pdf,,,277.jpg,"augmented reality, scale, user study"
279,Hyperwilding: Sonic Perplexity as Urban Acupuncture to Promote Environmental Kinship,"This paper invites discussion of how sound art installations, specifically those situated in urban environments, can serve as respite from urban stressors as well as advocate for increased awareness and engagement of acoustic ecology. The author invokes the theoretical framework of Karen Barad to juxtapose the Urban Acupuncture movement with the Solarpunk ethos, arguing that sound installations may be crafted as agential cuts to the entangled relationship of humans and their built environments. This paper surveys sound artists that have specifically engaged the urban space—an environment that one could argue is more “natural” to humans than the remote picturesque landscapes commonly associated with the concept. Finally, the author describes some of his past sonic interventions and expounds on his current project, “Standing Wave,” commissioned by the city government and non-profits to address Extreme Urban Heat. He discusses how this installation, coupled with targeted community engagement through “Environmental Listening” workshops, urges us to rethink the temporality of intervention, recognizing that long-term strategies, while not immediate solutions, are crucial for future cooling and remediating the effects of climate change.",Shomit Barua,shomitbarua@gmail.com,Shomit Barua (Moire Intermedia)*,"Barua, Shomit*",Shomit Barua,shomitbarua@gmail.com*,"Discussions about the artistic, cultural, and social impact of NIME technology","Entangled NIME: Intertwined, multilayer contexts in NIME research; Interactive sound art and sound installations ; Practice-based research approaches/methodologies/criticism",Accept as Medium Paper (up to 4000 words),oral,remote,,,,nime2025_279.pdf,,,279.jpg,"Ecoacoustics, Acoustic Ecology, Sustainability, Public Art, Circuitbending, Solar Power, Analog Synthesis"
295,Negotiating Entanglements in the Composition and Curation of an Ultrasonic Art Installation,"This paper explores the entangled activities of composing, curating and experiencing the sound installation 'Sonographies' at a contemporary art gallery. The installation extends our work with an ultrasonic technology that sonifies and magnifies the physical entanglement of a listener with a spatial sound field to produce rich movement-sound interaction without the use of sensors. Taking a research through practice approach, we examine the process of creating 'Sonographies' while deliberately allowing the nonhuman influences of site and technology to inform creative ideation and decision-making. We propose that an attunement to entanglement foregrounds the co-production of aesthetic qualities by the entire musical assemblage and fosters a sensitivity to fragile and changeable qualities of NIMEs, contingent on specific technical, material and social situations.",Nicole Robson,n.s.robson@qmul.ac.uk,Nicole Robson (Queen Mary University of London)*; Andrew  McPherson (Imperial College London ); Nick  Bryan-Kinns (University of the Arts London ),"Robson, Nicole*; McPherson, Andrew ; Bryan-Kinns, Nick",Nicole Robson; Andrew  McPherson; Nick  Bryan-Kinns,n.s.robson@qmul.ac.uk*; andrew.mcpherson@imperial.ac.uk; n.bryankinns@arts.ac.uk,"Entangled NIME: Intertwined, multilayer contexts in NIME research",Interactive sound art and sound installations ; Practice-based research approaches/methodologies/criticism,Accept as Long Paper (up to 6000 words),oral,Does not know yet,,,,nime2025_295.pdf,,,295.jpg,"entanglement, sound installation, ultrasound, practice"
323,Drawing Space with Rain: The Umbrella as a Flow Interface,"This study explores new possibilities for the transformation of perceived space by utilizing an umbrella as a dynamic spatial auditory interface. While spatial audio technologies have been widely applied across various fields, opportunities to consciously perceive the boundary between personal and external space in daily life remain limited. However, due to its structure and usage, the umbrella possesses a unique ability to render such boundaries perceptible.  Focusing on the ""flow"" of raindrops across the umbrella’s surface, the system detects the continuously shifting movement of rain in real time, rather than simply capturing the impact sound of falling droplets. By providing auditory feedback through spatial sound, users are encouraged to perceive the dynamic boundaries of space. Through this approach, rain traces the contours of space as it interacts with the umbrella, examining the possibilities for the perceptual expansion and transformation of personal space.  For this purpose, the umbrella is conceptualized as an interactive auditory interface, capturing raindrop movements in real time and applying spatial audio processing. While conventional multichannel speaker-based spatial audio systems aim to reproduce fixed sound fields, this approach seeks to virtually transform the acoustic environment itself. The umbrella is proposed not merely as a protective barrier but as a medium for perceiving dynamic environmental changes.  By leveraging the movement of rain, the study presents new possibilities for spatial acoustics that integrate natural phenomena.",Kana Yamaguchi,kana.yamaguchi@digitalnature.slis.tsukuba.ac.jp,Kana Yamaguchi (University of Tsukuba)*; Yuga Tsukuda (University of Tsukuba); Yoichi Ochiai (University of Tsukuba),"Yamaguchi, Kana*; Tsukuda, Yuga; Ochiai, Yoichi",Kana Yamaguchi; Yuga Tsukuda; Yoichi Ochiai,kana.yamaguchi@digitalnature.slis.tsukuba.ac.jp*; tsukuda@digitalnature.slis.tsukuba.ac.jp; wizard@slis.tsukuba.ac.jp,"Discussions about the artistic, cultural, and social impact of NIME technology",Interactive sound art and sound installations,Accept as Long Paper (up to 6000 words),oral,in person,,,,nime2025_323.pdf,,,323.jpg,"Umbrella, Rain, Sensing flow, Spatial Audio, Augmented Spatial Perception"
8,Anonymized Project Name: A Neural Network-Based Granular Synthesizer for Oceanic Soundscapes,"This paper presents a neural network-based granular synthesizer designed to explore the intersection of sound synthesis, machine learning, and ecological storytelling. The project interprets the environmental transformations caused by the entanglement of synthetic and natural materials, particularly focusing on plastiglomerates—hybrid formations of plastic debris fused with organic elements in marine environments. By organizing granular segments of oceanic field recordings within a latent space, the instrument enables real-time interaction with hybridized sound textures, allowing performers to navigate an evolving sonic landscape. Utilizing autoencoders for dimensionality reduction, spectral analysis, and machine learning-based clustering, the system constructs an abstract space that reflects the hybridization of synthetic and organic materials through sound. The physical instrument integrates gesture-controlled interaction and touch-based interfaces as a means of fostering an intuitive engagement with the sonic footprint of the Anthropocene. Anonymized Project Name contributes to contemporary eco-acoustic and AI-driven sound art by offering a speculative and interactive approach to sonic materiality, inviting reflection on the impact of synthetic waste on natural ecosystems.",Sabina Hyoju Ahn,sabina_ahn@ucsb.edu,Sabina Hyoju Ahn (University of California Santa Barbara)*; Ryan Millett ( University of California Santa Barbara); Seyeon Park (Artist),"Ahn, Sabina Hyoju*; Millett, Ryan; Park, Seyeon",Sabina Hyoju Ahn; Ryan Millett; Seyeon Park,sabina_ahn@ucsb.edu*; rmillett@ucsb.edu; seyparc@gmail.com,"Entangled NIME: Intertwined, multilayer contexts in NIME research","Explorations of relationships between motion, gesture and music; Sonic interaction design",Accept as Short Paper (up to 2000 words),poster,remote,,posters-1,,nime2025_8.pdf,,,8.jpg,"Granular Synthesis, Soundscpaes, Plastiglomerates, Neural audio synthesis, Dimensionality reduction"
13,Instant Design: Five Strategies for the use of Generative AI in NIME Ideation Workshops,"This paper presents five strategies for facilitating workshops that incorporate AI text-to-image (TTI) generators in the conceptual design of new musical instruments. Developed through a series of iterative workshops, this approach examines the integration of generative AI (GenAI) within creative processes, with a particular focus on idea generation and the interplay between AI-driven tools and traditional craft-based activities in workshop contexts.  The primary study was conducted at the 'XXX' conference and the paper shares insights from the workshop, including the combination of physical prototyping and GenAI concept design through image creation. The paper emphasises the practical implications of incorporating AI tools into group design fiction workshops and offers five suggestions for facilitators and practitioners. It considers the tensions and opportunities that arise in the collaboration between AI and human creativity, underscoring the importance of iterative feedback and the benefits of clearly defined design briefs within speculative design practices.",Hugh Aynsley,hugh2.aynsley@live.uwe.ac.uk,Hugh Aynsley (UWE)*; Pete Bennett (University of Bristol); Dave Meckin (Royal College of Art ); Sven Hollowell (University of Bristol); Thomas J. Mitchell (University of the West of England (UWE)),"Aynsley, Hugh*; Bennett, Pete; Meckin, Dave; Hollowell, Sven; Mitchell, Thomas J.",Hugh Aynsley; Pete Bennett; Dave Meckin; Sven Hollowell; Thomas J. Mitchell,hugh2.aynsley@live.uwe.ac.uk*; pete.bennett@bristol.ac.uk; dave.meckin@rca.ac.uk; sven.hollowell@bristol.ac.uk; tom.mitchell@uwe.ac.uk,Machine learning and artificial intelligence in NIMEs,"Accessibility and interfaces for musical expression for people with special needs ; Novel controllers, interfaces or instruments for musical expression",Accept as Medium Paper (up to 4000 words),poster,remote,,posters-1,,nime2025_13.pdf,,,13.jpg,"Speculative Design, NIME Workshops, Ideation, Novel Interfaces"
19,Finding Kinesthetic Empathy: A Study to Discover Metrics to Measure Kinesthetic Empathy During Interactive Music Performance,"Kinesthetic empathy is a term used in performance and kinesthetic interaction, defined as the ability of participants to “read, decode and react to each other’s input”. In prior studies, performers of interactive music self-reported sensing the presence of other musicians. The purpose of the present study was to identify kinesthetic empathy between two individuals in a live electronic performance reported as perceived interactivity. Participants viewed eight videos, both real duets, and spliced solos appearing as real duets, rating each video. The questions guiding this study were: (a) is there a difference in perceived interactivity between the live and spliced duets, (b) is there a relationship between performance rating and perceived interactivity. Results showed a significant difference in the perceived interactivity of the video conditions. Further, the results showed a significant relationship between performance rating and perceived interactivity of the performers.",Ryan Ingebritsen,waldtiere@gmail.com,Ryan Ingebritsen (Ryan Ingebritsen)*; Daniel Evans (West Texas A&M University); Christopher Knowlton (Rush Medical University),"Ingebritsen, Ryan*; Evans, Daniel; Knowlton, Christopher",Ryan Ingebritsen; Daniel Evans; Christopher Knowlton,waldtiere@gmail.com*; devans@wtamu.edu; chrisknowlton@gmail.com,Music-related human-computer interaction,"Entangled NIME: Intertwined, multilayer contexts in NIME research",Accept as Long Paper (up to 6000 words),poster,remote,,posters-1,,nime2025_19.pdf,,,19.jpg,"kinesthetic interaction, kinesthetic empathy, interactive electronic music performance, kinesthetic interaction design"
62,You’re An Instrument!: Creating active music-making experiences through worldbuilding and storytelling,"This paper outlines the development and key discoveries relating to active audience participation within You’re an Instrument!, an immersive childrens’ theatre show that turns a planted audience member into a musical instrument. We outline the use of wireless gestural instruments in the show, exploring their novel use as hidden props and theatrical devices that help invite audience members into a fictional world. Through the creation of this fictional world, the audience members became more actively involved in music making with these devices in the last third of the show. This paper is a call for instrument designers to consider using worldbuilding and storytelling techniques to more actively engage audience members in discovering the workings of new instruments.",Ciaran Frame,ciaran.frame@monash.edu,"Ciaran Frame (SensiLab, Monash University)*; Erick Mitsak (SensiLab, Monash University); Alon Ilsar (SensiLab, Monash University)","Frame, Ciaran*; Mitsak, Erick; Ilsar, Alon",Ciaran Frame; Erick Mitsak; Alon Ilsar,ciaran.frame@monash.edu*; erick.mitsak@monash.edu; alon.ilsar@monash.edu,Accessibility and interfaces for musical expression for people with special needs,"Gesture to sound mapping; Musical mapping strategies ; Novel controllers, interfaces or instruments for musical expression",Accept as Long Paper (up to 6000 words),poster,in person,,posters-1,,nime2025_62.pdf,,,62.jpg,"worldbuilding, instrument deign, accessibility"
25,Appropriating Technology for Interactive Media in Theatre: Design Strategies and Aesthetic Insights,"This paper investigates interactive media design as a narrative agent in theatrical performance through a practice-based design approach. We introduce the role of the Interaction Director into the production team and examine challenges in appropriating technology for theatrical interaction design. A two-layer workflow is proposed, integrating macro-scale conceptual design with micro-scale cue-to-cue interaction mapping. We address the mutual dependency among creative disciplines, highlighting the collaborative processes necessary to resolve conflicts during the design and rehearsal stages. Furthermore, we adopt a scenographic perspective to analyse how interactive media contributes to dramaturgical storytelling by crafting visual and auditory metaphors. We contextualize interaction design with reader-response theory and the horizon of expectations, demonstrating how interactive media fosters collaborative creativity and expands the narrative potential of theatrical storytelling.",Ruoxi Jia,ruoxi.jia@outlook.com,"Ruoxi Jia (Goldsmiths, University of London)*; Xuebiao Liu (Goldsmiths, University of London)","Jia, Ruoxi*; Liu, Xuebiao",Ruoxi Jia; Xuebiao Liu,ruoxi.jia@outlook.com*; liuxuebia@gmail.com,Practice-based research approaches/methodologies/criticism,Music-related human-computer interaction ; Technologies or systems for collaborative music-making,Accept as Long Paper (up to 6000 words),poster,remote,,posters-1,,nime2025_25.pdf,,,25.jpg,"Interaction, theatre, storytelling"
66,Harmonix Series: Accessible Digital Musical Instruments for Mindfulness and Creativity,"This paper introduces the Harmonix series, a collection of Accessible Digital Musical Instruments (ADMIs) designed to enhance mood stability and mindfulness through intuitive and interactive music-making. Recognizing the barriers posed by traditional digital musical instruments—including steep learning curves, high costs, and uninspiring outputs—Harmonix prioritizes affordability, portability, and user-friendly interfaces to cater to individuals with no prior musical training.   The study evaluates two instruments, ZenithChimes and Equilibrio. ZenithChimes employs touch-sensitive keys mapped to meditative tones in the Aeolian mode, promoting creativity and relaxation. Equilibrio, a 3D-printed ""stone stack,"" uses tilt gestures to modulate soundscapes, symbolizing balance and harmony. Both instruments integrate calming auditory outputs and minimalist design aesthetics to create an engaging and meditative experience.   A workshop-based study with 10 participants, spanning diverse backgrounds, demonstrated the instruments' accessibility and therapeutic potential. Results showed that participants found the instruments easy to use, aesthetically appealing, and suitable for mindfulness practices, with 70% identifying their integration into meditation or yoga sessions as beneficial. However, feedback highlighted the need for more customization options, particularly in Equilibrio's soundscapes.   By bridging art, technology, and mindfulness, Harmonix fosters creative exploration and emotional regulation, with implications for therapeutic, educational, and artistic applications. Future work will explore sustainability, inclusivity, and multi-sensory feedback to enhance the instruments’ design and impact. This study underscores the potential of ADMIs to transcend conventional music-making, offering innovative tools for well-being and self-expression.",Wing Hei Cheryl Hui,wingheicherylhui@gmail.com,"Wing Hei Cheryl Hui (Individual Researcher)*; Patrick Hartono (Goldsmiths, University of London)","Hui, Wing Hei Cheryl*; Hartono, Patrick",Wing Hei Cheryl Hui; Patrick Hartono,wingheicherylhui@gmail.com*; Patrick.Hartono@gold.ac.uk,Accessibility and interfaces for musical expression for people with special needs,"Interactive sound art and sound installations ; Novel controllers, interfaces or instruments for musical expression",Accept as Medium Paper (up to 4000 words),poster,in person,,posters-1,,nime2025_66.pdf,,,66.jpg,"Accessible Digital Musical Instruments (ADMIs), Music and Mindfulness, HCI, Tactile interfaces for music expression"
165,Maximum Silence to Noise: Sound synthesis for responsive gestural control,"Modulation synthesis has been a foundational technique in the development of electronic musical instruments since their inception. This paper presents a novel approach to ring modulation synthesis, termed Maximum Silence to Noise (MSN), along with an associated method of gestural control facilitated by a pressure-sensitive multi-touch controller. The primary objective of this research is to develop an instrument capable of producing a broad and diverse range of audio spectra that can be expressively articulated through responsive touch-based interaction. Integrating the synthesis process with gestural parameter mapping is crucial for the performative capabilities of New Interfaces for Musical Expression (NIMEs). The technical development of the MSN instrument was subject to an iterative design process with mixed method evaluation. The usability and practical application of the MSN instrument was refined through performance experiences, which illustrate the effectiveness of the synthesis-gesture mappings in providing dynamic and expressive control over the diverse generated audio spectra.",Andrew Brown,andrew.r.brown@griffith.edu.au,Andrew Brown (Griffith University)*,"Brown, Andrew*",Andrew Brown,andrew.r.brown@griffith.edu.au*,"Augmented, embedded and hyper instruments","Explorations of relationships between motion, gesture and music; Novel controllers, interfaces or instruments for musical expression",Accept as Medium Paper (up to 4000 words),poster,in person,,posters-2,,nime2025_165.pdf,,,165.jpg,"Synthesis, gesture, interaction, expression"
44,Out-of-Control Feedback Systems and Collaborative Influence with the Instrumentalist Mixer Feedback Transmutation System,"This paper explores a novel live interactive electroacoustic system Instrumentalist Mixer Feedback Transmutation (IMFT) (formally called the ANONYMIZED) [1]. IMFT occurs when an instrumentalist is patched into a mixing board with feedback loops. The instrumentalist interacts and influences the mixer’s feedback together with another performer playing the mixer. In this system, a chaotic, out-of-control relationship can be formed where the output of the mixer and the gestures from the mixer performer can be in battle with the input from the instrumentalist and vice-versa. After a brief historical contextualization of mixer feedback (a.k.a no-input mixer), the IMFT system and the complex relationships that form between human and machine are introduced. No-input mixer performance practices are discussed, followed by exploration of a single feedback loop to illustrate some of the mixer’s possible sound worlds and the nature of the instrument. Performance experiences from two recent compositions by the first author, generative open graphic score #1 (2023) and noise ritual (2023), are described in order to explore different performance interactions created by different instrumentalists working with the IMFT system. This practice-based research provides a useful case study examining the entangled relationship between performers and interfaces in feedback-based music systems and how innovative approaches to an established electronic practice can create new perspectives and collaborative opportunities.",Nolan Hildebrand,nolanahildebrand@gmail.com,Nolan Hildebrand (University of Toronto)*; Timothy Roth (University of Toronto),"Hildebrand, Nolan*; Roth, Timothy",Nolan Hildebrand; Timothy Roth,nolanahildebrand@gmail.com*; tim.roth@mail.utoronto.ca,New music performance paradigms,"Augmented, embedded and hyper instruments ; Entangled NIME: Intertwined, multilayer contexts in NIME research",Accept as Medium Paper (up to 4000 words),poster,remote,,posters-1,,nime2025_44.pdf,,,44.jpg,"Feedback, No-input Mixing Board, Augmented instruments, Distortion"
215,Embedded Comparo: Small DSP Systems Side-by-Side,"This paper presents a comparative analysis of four embedded platforms designed for real-time audio processing: Bela, Daisy, OWL, and Raspberry Pi. These platforms have become integral tools in the field of embedded musical instrument design, offering a variety of workflows, programming environments, and deployment methods. Although each system carries its own distinct strengths and constraints, the current workflow to embed DSP code across multiple devices lacks standardized approaches. To address this challenge, we develop a methodology that focuses on deploying Pure Data patches across all four platforms. Our study is structured around four test patches. Our findings highlight the trade-offs in latency, processing power, and memory constraints across the selected platforms. As a result, we propose a streamlined workflow to deploy Pd patches across all boards using Plugdata, the Heavy Compiler, and their respective web IDEs. As an ongoing contribution to the NIME community, we document our methodologies, workflows, and best practices in an open source online repository, which serves as a continuously evolving resource for future research in the hands of musicians, researchers, and developers working with embedded musical systems.",Francesco Di Maggio,f.di.maggio@tue.nl,Francesco Di Maggio (Eindhoven University of Technology)*; Bart Hengeveld (Eindhoven University of Technology); Atau Tanaka (Goldsmiths),"Di Maggio, Francesco*; Hengeveld, Bart; Tanaka, Atau",Francesco Di Maggio; Bart Hengeveld; Atau Tanaka,f.di.maggio@tue.nl*; b.j.hengeveld@tue.nl; a.tanaka@gold.ac.uk,"Augmented, embedded and hyper instruments",Evaluation and user studies of commercially available “off the shelf” interfaces,Accept as Long Paper (up to 6000 words),poster,in person,Have been updated by Francesco about in-person attandace. ,posters-3,,nime2025_215.pdf,,,215.jpg,"Embedded Platforms, DSP, Plugdata, Bela, Daisy, OWL, Raspberry Pi"
55,Repurposing a Rhythm Accompaniment System for Pipe Organ Performance,"This paper presents an overview of a human-machine collaborative musical performance by AnonPerformer utilizing multiple MIDI-enabled pipe organs at AnonVenue, as part of the AnonConcerts concert series. Our earlier collaboration with AnonPerformer focused on live performances using drum generation systems, where generative models captured rhythmic transient structures while ignoring harmonic information. For the organ performance, we required a system capable of generating harmonic sequences in real-time, conditioned on AnonPerformer's performance. Instead of developing a comprehensive state-of-the-art model, we integrated a more traditional generative method to convert our pitch-agnostic rhythmic patterns into harmonic sequences. This paper details the development process, the creative and technical considerations behind the final performance, and a reflection on the efficacy and adaptability of the chosen methodology.",Nicholas Evans,nicholas.evans@upf.edu,Nicholas Evans (Universitat Pompeu Fabra)*; Behzad Haki (Universitat Pompeu Fabra); Sergi Jorda (Universitat Pompeu Fabra),"Evans, Nicholas*; Haki, Behzad; Jorda, Sergi",Nicholas Evans; Behzad Haki; Sergi Jorda,nicholas.evans@upf.edu*; behzad.haki@upf.edu; sergi.jorda@upf.edu,Machine learning in musical performance,Musical mapping strategies ; Performance rendering and generative algorithms,Accept as Short Paper (up to 2000 words),poster,remote,,posters-1,,nime2025_55.pdf,,,55.jpg,"Machine Learning, Generative, Performance"
57,Turntable-Based Electronic Music and Embodied Audience Interaction,"Rings… Through Rings transforms archival maps of Hong Kong’s military fortifications into playable surfaces for turntable-based electronic music. Laser-etched discs encode cartographic data, producing sonic textures manipulated through turntables and enhanced by audio techniques like cross-synthesis, concatenative synthesis, and spatialization. Grounded in theories of transcoding, productive agency, and participatory culture, the project reimagines the turntable as a cultural interface, bridging analog heritage with computational sound.   This hybrid system blends pre-composed musical structures with real-time audience interaction, allowing participants to alter playback, swap discs, and influence spatial audio. By merging cartography, sound, and participatory design, the work offers a collaborative, multisensory approach to intangible heritage. Future developments include expanded spatial configurations, real-time disc fabrication, and AI integration to deepen engagement and cultural reinterpretation.",Tak Cheung Hui,tchui@bu.edu,Tak Cheung Hui (Hong Kong Metropolitan University)*; Xiaoqiao  Li (Hong Kong Metropolitan University); Yu Chia Kuo (McGill University),"Hui, Tak Cheung*; Li, Xiaoqiao ; Kuo, Yu Chia",Tak Cheung Hui; Xiaoqiao  Li; Yu Chia Kuo,tchui@bu.edu*; xili@hkmu.edu.hk; yu.kuo@mail.mcgill.ca,Interactive sound art and sound installations,Practice-based research approaches/methodologies/criticism,Accept as Short Paper (up to 2000 words),poster,remote,,posters-1,,nime2025_57.pdf,,,57.jpg,"Interactive Installation, Digital Fabrication, Interdisciplinary performance, Historical Reinterpretation"
179,Augmentation of a Historical Harpsichord Keyboard Replica for Haptic-Enabled Interaction in Museum Exhibitions,"This paper describes the design and creation of an electronically augmented replica of a 16th-century harpsichord keyboard with a typical Renaissance Italian layout to create a digital musical instrument.  The keyboard was commissioned for exhibition in a musical instrument museum to enhance the visitor experience by providing an interface to digitised versions of instruments within the collection.  The replica is designed to balance the competing demands of historical authenticity, public accessibility, and preservation.  It replicates the original instrument's tactile feedback and mechanical resistance by employing historically informed construction techniques. Optical sensors integrated within the mechanism capture the jacks' motion data, enabling MIDI message generation.  This work situates itself within broader discussions on the role of technology in museums.  A keyboard interface of this type offers an opportunity to enhance visitor interaction while safeguarding delicate artefacts.  The paper examines the keyboard's design principles, technical implementation, and implications, emphasising its contribution to public engagement and the long-term preservation of musical heritage.",Matthew Hamilton,matthew.hamilton2@unibo.it,Matthew Hamilton (Università di Bologna)*; Michele Ducceschi (Università di Bologna); Catalina Vicens (Museo di San Colombano); Roberto Livi (Harpsichord Maker); Andrew McPherson (Imperial College London),"Hamilton, Matthew*; Ducceschi, Michele; Vicens, Catalina; Livi, Roberto; McPherson, Andrew",Matthew Hamilton; Michele Ducceschi; Catalina Vicens; Roberto Livi; Andrew McPherson,matthew.hamilton2@unibo.it*; michele.ducceschi@unibo.it; catalina.vicens@genusbononiae.it; robertvs.livi@gmail.com; andrew.mcpherson@imperial.ac.uk,"Discussions about the artistic, cultural, and social impact of NIME technology","Historical, theoretical or philosophical discussions about designing or performing with new interfaces ; Sensor and actuator technologies, including haptics and force feedback devices",Accept as Medium Paper (up to 4000 words),poster,in person,,posters-2,,nime2025_179.pdf,,,179.jpg,"interactive exhibit, augmented instrument, optical sensing, harpsichord, tactile interface"
123,Making the Immaterial Material: A Diffractive Approach Toward a Politics of Material Culture Within NIME,"Traditional Human-Computer Interaction has often been critiqued for its ostensibly opaque position on ethical, ontological, and epistemological concerns, particularly in relation to completed design artifacts. More recently, similar criticisms have been directed at the NIME (New Interfaces for Musical Expression) community for its relative silence on contemporary political issues. However, it is possible that an implicit ethics of material culture is already embedded within NIME discourse — one that could be critically examined and potentially mobilized as a foundation for a more explicitly political ethics. Inspired by feminist discourse, namely Karen Barad's theory of agential realism, and contextualized through Bruno Latour's remarks regarding the ethics of design, this paper explores the possibilities of entanglement in DMI design. We begin with a discussion of diffraction and entanglement followed by a brief overview of values-oriented and ""world-building"" theoretical models and methodologies of design research. We continue with our generative ""DMI-as-apparatus"" approach to diffractive methodology and conclude with a case study BRAIDS_, a digital music instrument based upon the black American cultural practice of hair braiding, that examines critical design decisions that are otherwise deemed invisible by traditional methods of scientific inquiry.",Brittney Allen,brittneyjuliet@gmail.com,Brittney Allen (Imperial College London)*; Andrew Mcpherson (Imperial College London); Alexandria Smith (Georgia institute of Technology); Jason Freeman (Georgia Institute of Technology),"Allen, Brittney*; Mcpherson, Andrew; Smith, Alexandria; Freeman, Jason",Brittney Allen; Andrew Mcpherson; Alexandria Smith; Jason Freeman,brittneyjuliet@gmail.com*; andrew.mcpherson@imperial.ac.uk; alexandria.smith@gatech.edu; jason.freeman@gatech.edu,"Entangled NIME: Intertwined, multilayer contexts in NIME research","Discussions about the artistic, cultural, and social impact of NIME technology ; Historical, theoretical or philosophical discussions about designing or performing with new interfaces",Accept as Medium Paper (up to 4000 words),poster,in person,,posters-1,,nime2025_123.pdf,,,123.jpg,"Entanglement, Diffraction, Assemblification"
228,Acoustic-digital hybrid synthesizer,"This paper explores the design and evaluation of an acoustic-digital hybrid instrument that aims to address key criticisms of Digital Musical Instruments (DMIs), particularly the separation of control and sound generation. By integrating an interactable physical string with coupled Finite Difference Schemes (FDS) for physical modeling synthesis, the instrument creates a tactile and responsive playing experience. The instrument was evaluated through a mixed-methods approach, combining qualitative think-aloud protocols with the Musician’s Perception of the Experiential Quality of Musical Instruments Questionnaire (MPX-Q). Results indicate that the instrument fosters curiosity and creativity but highlights challenges in achieving traditional acoustic playability, such as latency and perceptual dissonance. These findings emphasize the potential and limitations of acoustic-digital hybrids in reuniting control and sound, offering valuable insights for future developments in musical interface design.",Levin Schnabel,lschna23@student.aau.dk,Levin Schnabel (Aalborg University)*; Dan Overholt (Aalborg University),"Schnabel, Levin*; Overholt, Dan",Levin Schnabel; Dan Overholt,lschna23@student.aau.dk*; dano@create.aau.dk,Evaluation and user studies of new interfaces for musical expression,"Novel controllers, interfaces or instruments for musical expression ; Sensor and actuator technologies, including haptics and force feedback devices",Accept as Long Paper (up to 6000 words),poster,in person,,posters-3,,nime2025_228.pdf,,,228.jpg,"Acoustic-digital hybrid instruments, Physical modeling synthesis, Controller-generator paradigm"
67,Decoupling Physical and Virtual Spaces in Co-Located Collaborative Mixed Reality Instruments with gRAinyCloud,"Collaborative co-located Mixed Reality musical instruments combine some of the expressive opportunities of 3D interaction and communication and cooperation of physical multi-user instruments. However in existing instruments, the fixed coupling between the virtual and physical environments constrains the affordances brought by Mixed Reality, such as per-musician free navigation in or multi-scale control of virtual structures. We designed gRAinyCloud, as a way to reintegrate these lost affordances to a co-located instrument. It allows for the expressive exploration of a set of sounds represented by a virtual structure of shapes placed in the physical space and shared between musicians. Above all, gRAinyCloud enables each musician to freely manipulate their own viewpoint, changing its scale, position and rotation, effectively decoupling the physical and virtual spaces, and to switch between self, other's and absolute viewpoint while playing. We describe the implementation of this decoupling of spaces and analyse its uses and implications for collective musical expression, by relying on a first-person approach.",Pierrick Uro,pierrick.uro@gmail.com,Pierrick Uro (McGill University / Université de Lille)*; Florent Berthaut (Université de Lille); Thomas Pietrzak (Université de Lille); Marcelo Wanderley (McGill University),"Uro, Pierrick*; Berthaut, Florent; Pietrzak, Thomas; Wanderley, Marcelo",Pierrick Uro; Florent Berthaut; Thomas Pietrzak; Marcelo Wanderley,pierrick.uro@gmail.com*; florent.berthaut@univ-lille.fr; thomas.pietrzak@univ-lille.fr; marcelo.wanderley@mcgill.ca,"Extended reality environments: augmented, virtual, mixed reality","Novel controllers, interfaces or instruments for musical expression ; Technologies or systems for collaborative music-making",Accept as Medium Paper (up to 4000 words),poster,remote,,posters-2,,nime2025_67.pdf,,,67.jpg,"Mixed reality, musical ensembles, decoupling, collaboration, co-located, multi-scale"
70,Creating a White Noise Instrument for Collaborative Improvisation,"This paper introduces shiki, a virtual instrument developed for performing Renga for White Noise, an interdisciplinary project that transmediates Japanese renga poetry principles into a frame- work for collaborative improvisation with human and AI agents. We discuss two areas: (1) our transmediation of renga’s struc- turing principles into shiki’s design (2) the technical aspects be- hind the AI agent’s performance with shiki. Through its inter- disciplinary and intercultural entanglements with renga, trans- mediation as a method can illuminate new perspectives on the design and performance of NIMEs.",Austin Oting Har,austin.har@uvi.edu,Austin Oting Har (University of the Virgin Islands)*; Kurt Mikolajczyk (University of New South Wales),"Har, Austin Oting*; Mikolajczyk, Kurt",Austin Oting Har; Kurt Mikolajczyk,austin.har@uvi.edu*; k.mikolajczyk@unsw.edu.au,"Entangled NIME: Intertwined, multilayer contexts in NIME research","Novel controllers, interfaces or instruments for musical expression ; Performance rendering and generative algorithms ; Technologies or systems for collaborative music-making",Accept as Short Paper (up to 2000 words),poster,Does not know yet,,posters-1,,nime2025_70.pdf,,,70.jpg,"Instrument design, Japanese renga poetry, white noise, collaborative improvisation, evolutionary algorithms"
20,XR Musical Keyboard: An Extended Reality Keyboard with an Arbitrary Number of Keys and Pitches,"We introduce the Extended Reality (XR) Musical Keyboard, a system allowing users to overlay a virtual keyboard onto a tabletop surface, such as a standard PC keyboard. This virtual keyboard is highly customizable: users can freely program the number of keys and their respective pitches. Modern software instruments offer advanced capabilities, including microtonal scales (pitches outside the standard 12-tone equal temperament). However, playing these instruments often remains challenging due to the lack of corresponding physical hardware. Our proposed solution addresses this gap by projecting a programmable virtual keyboard onto a tangible object within the XR space. This approach combines the software's flexibility with the tactile feedback of a physical surface, enhancing playability. Users can simplify the keyboard layout (e.g., fewer keys than a piano) or expand it beyond conventional limits to explore new expressive possibilities, particularly for microtonal music. We conducted a small pilot study (N=4) involving participants mostly inexperienced with keyboards to gather preliminary feedback on the interface's ease of use for performance.",Tatsunori Hirai,thirai@komazawa-u.ac.jp,Tatsunori Hirai (Komazawa University)*; Jack Topliss (University of Cantebury); Thammathip Piumsomboon (University of Cantebury),"Hirai, Tatsunori*; Topliss, Jack; Piumsomboon, Thammathip",Tatsunori Hirai; Jack Topliss; Thammathip Piumsomboon,thirai@komazawa-u.ac.jp*; Jack.a.m.topliss@gmail.com; tham.piumsomboon@canterbury.ac.nz,"Extended reality environments: augmented, virtual, mixed reality",Music-related human-computer interaction,Accept as Medium Paper (up to 4000 words),poster,in person,,posters-1,,nime2025_20.pdf,,,20.jpg,"XRMI, Microtones, Microtonal keyboard"
73,VentHackz: Exploring the Musicality of Ventilation Systems,"Ventilation systems can be seen as gigantic examples of interfaces for musical expression, with the potential of merging sound, space, and human interaction. This paper explores conceptual similarities between ventilation systems and wind instruments and explores approaches to ""hacking"" ventilation systems with components that produce and modify sound. These systems enable the creation of unique sonic and visual experiences by manipulating airflow and making mechanical adjustments. Users can treat ventilation systems as musical interfaces by altering shape, material, and texture or augmenting vents. We call for heightened attention to the sound-making properties of ventilation systems and call for action (#VentHackz) to playfully improve the soundscapes of our indoor environments.",Maham Riaz,maham.riaz@imv.uio.no,Maham Riaz (University of Oslo)*; Ioannis Theodoridis (Norwegian Academy of Music); Çağrı Erdem (University of Oslo); Alexander Refsum Jensenius (University of Oslo),"Riaz, Maham*; Theodoridis, Ioannis; Erdem, Çağrı; Jensenius, Alexander Refsum",Maham Riaz; Ioannis Theodoridis; Çağrı Erdem; Alexander Refsum Jensenius,maham.riaz@imv.uio.no*; ioannis.theodoridis@nmh.no; cagrie@ifi.uio.no; a.r.jensenius@imv.uio.no,"Historical, theoretical or philosophical discussions about designing or performing with new interfaces","Discussions about the artistic, cultural, and social impact of NIME technology ; Entangled NIME: Intertwined, multilayer contexts in NIME research; Interactive sound art and sound installations",Accept as Medium Paper (up to 4000 words),poster,remote,,posters-2,,nime2025_73.pdf,,,73.jpg,"Ventilation Systems, HVAC, Soundscape,  Room Acoustics, Urban Intervention"
276,Luna: An AR Musical Instrument on the Meta Quest 2,"Head-mounted augmented reality (AR) computers present the opportunity to develop new musical interfaces that would be impossible to build physically or with conventional computing devices. Unfortunately, typical computer music tools have not been easy to apply within AR development tool chains. Integrating standard computer music tools in AR development would allow more rapid prototyping of new instrument ideas and transfer of knowledge from experienced computer musicians. The goal of this paper is to demonstrate that AR digital musical instruments can be developed using libpd, the library version of the standard computer music environment Pure Data. We present a case study of an AR instrument developed for the Meta Quest 2 integrating libpd in the AR development tool-chain for the interactive audio components. The iterative development process was tracked through autoethnographic reflections and analysed with thematic analysis. We found that Pure Data was an effective way to develop audio interactions on the Quest 2 and that the hand tracking on this platform was capable of complex gestural interactions. This work could enable a broader community of computer musicians to explore AR NIME development, taking advantage of the unique affordances of this medium.",Charles Martin,charles.martin@anu.edu.au,Samuel Dietz (Australian National University); Charles Martin (Australian National University)*,"Dietz, Samuel; Martin, Charles*",Samuel Dietz; Charles Martin,samuel.dietz@anu.edu.au; charles.martin@anu.edu.au*,"Extended reality environments: augmented, virtual, mixed reality","Music-related human-computer interaction ; Software frameworks, interface protocols, and data formats, for supporting musical interaction",Accept as Medium Paper (up to 4000 words),poster,in person,,posters-3,,nime2025_276.pdf,,,276.jpg,"augmented reality, free hand interaction, first-person"
139,Exploring Musical Creation Through Brain-Body Digital Musical Instruments,"Brain-Body Digital Musical Instruments (BBDMI) merge physiological signals with real-time sound processing, enabling performers to use Electromyographic (EMG) data for musical expression. This study explores the creative and technical potential of BBDMI, focusing on signal acquisition, calibration, and mapping for use in composition and performance. Demonstrations with flute, piano showcase its ability to enhance expressivity through gestural control. While key advancements include improved signal stability and refined mapping, challenges such as connectivity issues and notation limitations remain. This research highlights BBDMI’s promise as a transformative tool in contemporary music.",Meiling Wu,meiling.wu@mail.mcgill.ca,Yue Wang (CIRMMT Student member); Meiling Wu (CIRMMT Student member)*,"Wang, Yue; Wu, Meiling*",Yue Wang; Meiling Wu,yue.wang.2@umontreal.ca; meiling.wu@mail.mcgill.ca*,Gesture to sound mapping,Evaluation and user studies of new interfaces for musical expression,Accept as Medium Paper (up to 4000 words),poster,in person,,posters-2,,nime2025_139.pdf,,,139.jpg,"BBDMI, Gesture control, Composition"
254,Designing A Tangible Rhythmic Interface for Digital Drum Talk,"We propose a tangible user interface and communication protocol for computer mediated rhythm-based interaction for educational applications (music, language, multimodality). Thinking beyond the paradigm of keyboard-and-screen-based interfaces, this project is based on previous work on the Drumball (Tchetgen, 2024). By converting rhythmic input into various digital outputs, it creates an entangled ecosystem where music, language, and multimodal communication intersect (link). Such a digital talking drum could offer educators and parents a novel method for introducing children to literacy, language and communication in the early years. We present design iterations of 1) a tangible rhythmic interface for digital drum talk inspired by the style of play of the Djembe, 2) a protocol for sending piezo sensor outputs over a custom PCB shield, which can be recognized across multiple platforms and web-based environments without additional customization; and 3) an activity manager digital platform for rhythm-based communication using the Alphariddims multimodal symbol system. We argue that such a culturally-grounded approach to music technology design provides a viable avenue for the preservation and revitalization of the vibrant, yet intangible, cultural heritage and traditions of the drum language communication systems.",Pierre-Valery Tchetgen,p.tchetgen@northeastern.edu,Pierre-Valery Tchetgen (Northeastern University)*,"Tchetgen, Pierre-Valery*",Pierre-Valery Tchetgen,p.tchetgen@northeastern.edu*,"Historical, theoretical or philosophical discussions about designing or performing with new interfaces",Gesture to sound mapping,Accept as Long Paper (up to 6000 words),poster,in person,,posters-3,,nime2025_254.pdf,,,254.jpg,"drummology,  talking drum controller,  rhythmic interaction, drum language communication, embodied learning"
125,Creating a Real-Time Responsive Handbalancing Interface with HAND★CS,"This paper introduces HAND★CS, a new interface for interdisciplinary expression for music, movement, and light. Our interface augments a pedagogical interface for hand-balancing, Haptics-Assisted iNversions Device (HAND), and transforms it into one for artistic expression. It draws upon Licklider's concept of man-computer symbiosis, specifically the commensalism form of symbiosis. HAND★CS strives to embody a performance apparatus and system with symbiotic connectivity between performer and interface. This paper discusses the inspiration and background for such a system pulling from the fields of human-computer interaction (HCI), music technology and new interfaces for musical expression (NIMEs), and circus arts. In addition, it defines the design and implementation, evaluation of the first version of HAND★CS, and future work.",Linnea Kirby,linnea.kirby@mail.mcgill.ca,"Linnea Kirby (McGill University)*; Christiana Rose (CirqueIT, LLC); Jeremy Cooperstock (McGill University); Marcelo Wanderley (McGill University)","Kirby, Linnea*; Rose, Christiana; Cooperstock, Jeremy; Wanderley, Marcelo",Linnea Kirby; Christiana Rose; Jeremy Cooperstock; Marcelo Wanderley,linnea.kirby@mail.mcgill.ca*; christiana.lauren.rose@gmail.com; jer@cim.mcgill.ca; marcelo.wanderley@mcgill.ca,"Novel controllers, interfaces or instruments for musical expression","Entangled NIME: Intertwined, multilayer contexts in NIME research; Explorations of relationships between motion, gesture and music; Music-related human-computer interaction",Accept as Medium Paper (up to 4000 words),poster,remote,,posters-2,,nime2025_125.pdf,,,125.jpg,"Circus, Handbalancing, Sonification, Performance, Symbiosis"
172,A Synthetic Cicada Soundscape Controlled by Breath,"This paper describes an interactive installation featuring a generative soundscape with breath control, that aims to capture the feeling of being in a forest full of cicadas. Inspired by a period of deep listening to cicada stridulations – in which I found the spatio-temporal pulsation of the sound mass reminiscent of breathing – this installation uses breath control to give a sense of breathing with a forest. The sound mass consists of multiple generative sources, each loosely modelled on an individual cicada stridulating. Each ‘cicada’ comprises a temporal hierarchy of pulse trains modulating a carrier frequency, with a simple sonic spatialization algorithm applied to give the sense of immersion in the sound mass. The algorithm is implemented in the Extempore audiovisual programming language, and utilizes an architecture in which each sonic parameter is inherently stochastic, much as the sound production mechanisms of actual Cicadas exhibit natural variation.",Toby Gifford,toby.gifford@gmail.com,Toby Gifford (University of the Sunshine Coast)*,"Gifford, Toby*",Toby Gifford,toby.gifford@gmail.com*,Interactive sound art and sound installations,Sonic interaction design,Accept as Short Paper (up to 2000 words),poster,in person,,posters-2,,nime2025_172.pdf,,,172.jpg,"generative, soundscape, ecoacoustics"
300,GraviTone: A Tangible Musical Interface using Gravity Well for Sound and Music Creation,"We propose a new musical interface, “GraviTone” that produces sounds and musical compositions via interaction with spherical objects in a gravity-well benchtop setup. The interface consists of several spherical objects in different colours orbiting around the centrally placed static object on the spacetime fabric.   The spherical objects are launched at a certain angle from the setup's periphery, and the objects' motion is tracked by an overhead camera and mapped to different parameters to generate sound. In GraviTone, users can experience sounds generated from objects’ parameters and have control over different configurations. We use the setup to send Open Sound Control (OSC) signals and Musical Instrument Digital Interface (MIDI) signals to map different sounds and musical scales (Indian classical ragas and Western classical scale)  We mapped the moving objects’ parameters to control synth parameters, VSTs, and DAWs. Users can also route generated MIDI data into DAWs using preset configurations or customizing their own frameworks for sound generation. We also incorporate real-time projected visuals onto the fabric for further immersion and interactivity. This integrated interface combines various domains like musical mathematics, sonification techniques, sound synthesis, and sound design for live music creation and real-time audiovisual composition.",Rajashekhar  V S,raja23@iitk.ac.in,Kratika Jain (Indian Institute of Technology - Kanpur); Allwin  Williams (Indian Institute of Technology - Kanpur); Akhilesh Kumar Bhagat (Indian Institute of Technology - Kanpur); Sukanth K (Indian Institute of Technology - Kanpur); Arunav Rajesh (Indian Institute of Technology - Kanpur); Prashant  Pal (Indian Institute of Technology - Kanpur); Krishanan Chandran (TU Dresden); Rajashekhar  V S  (Indian Institute of Technology - Kanpur)*; Anandu Ramesh (Indian Institute of Technology - Kanpur); Gowdham Prabhakar  (Indian Institute of Technology - Kanpur),"Jain, Kratika; Williams, Allwin ; Kumar Bhagat, Akhilesh; K, Sukanth; Rajesh, Arunav; Pal, Prashant ; Chandran, Krishanan; V S , Rajashekhar *; Ramesh, Anandu; Prabhakar , Gowdham",Kratika Jain; Allwin  Williams; Akhilesh Kumar Bhagat; Sukanth K; Arunav Rajesh; Prashant  Pal; Krishanan Chandran; Rajashekhar  V S; Anandu Ramesh; Gowdham Prabhakar,jainkratika498@gmail.com; allwinwilliams.info@gmail.com; akhileshkrbhagat1@gmail.com; sukanth.original@gmail.com; arunavr23@iitk.ac.in; prashpal23@iitk.ac.in; krishnan.chandran@tu-dresden.de; raja23@iitk.ac.in*; anandurpt23@iitk.ac.in; gowdhampg@iitk.ac.in,Interactive sound art and sound installations,Musical mapping strategies ; Sonic interaction design ; Technologies or systems for collaborative music-making,Accept as Medium Paper (up to 4000 words),poster,in person,,posters-3,,nime2025_300.pdf,,,300.jpg,"data sonification, musical mapping, audio synthesis, musical mathematics, sound design"
94,A Computer Application to Explore 53-Tone Equal Temperament Harmonies Through Modal Interchange,"We present a novel computer application for real-time exploration and manipulation of microtonal harmonies through an intuitive interface visualization and integrated MIDI controllers, allowing real-time interaction and bridging the gap between theoretical concepts and practical musical applications. We investigate how modern harmonic principles can be applied and extended in the 53-tone equal temperament (53-TET) resolution, which facilitates a richer harmonic repertoire. Our approach starts from 12-TET modes disposition of standard functional chords, borrowing 31-TET principles that extend minor/major interval qualities with subminor, neutral, and supermajor distinctions. Subsequently, we extend to 53-TET's harmonic landscape with detailed interval qualities. Our application implements modal interchange techniques and parallel chord substitutions to offer a user access to parallel harmonic trajectories in microtonality. Our implementation employs MIDI Polyphonic Expression (MPE) through a custom MaxForLive application called the Bridge that connects conventional MIDI protocols with the construction of 53-TET chords, allowing precise microtonal control while maintaining compatibility with modern digital audio workstations, using Ableton Live (11/12) as a platform. The application includes a real-time visualization system, interactive chord manipulation tools, and a comprehensive Scale Editor to explore novel harmonic structures and trajectories. We demonstrate through practical examples and theoretical analysis how the proposed approach enables discovering new harmonic possibilities while preserving meaningful connections to established modal frameworks. This research contributes to the growing microtonal music field by providing theoretical foundations and practical tools that incorporate extended tuning systems into contemporary musical practice.",David Dalmazzo,dalmazzo@kth.se,David Dalmazzo (KTH)*; Ken Déguernel ( UMR 9189 – CRIStAL ); Bob Sturm (KTH),"Dalmazzo, David*; Déguernel, Ken; Sturm, Bob",David Dalmazzo; Ken Déguernel; Bob Sturm,dalmazzo@kth.se*; ken.deguernel@univ-lille.fr; bobs@kth.se,Interface protocols and data formats supporting musical interaction,Musical mapping strategies ; Music-related human-computer interaction,Accept as Medium Paper (up to 4000 words),poster,in person,,posters-1,,nime2025_94.pdf,,,94.jpg,"Microtonality, Music Theory, Modal Interchange, Harmony, Navigation, 53 Tone Equal Temperament"
146,ViVo: Piano Learning Through Visualizing Vocalizations on a Lighted Keyboard,"Vocalization and visualization are two powerful methods for internalizing music which have been shown to be effective with beginner and skilled musicians alike. Despite the well-researched benefits of each practice, integrated vocalization and visualization for instrument learning has seen little attention in the music technology community. This paper introduces the design and implementation of ViVo, a piano learning tool that combines the embodied sense of pitch offered by vocalization with the spatial intuition provided by in situ visualization. ViVo offers two modes: a real-time mode that listens to live user vocalizations and illuminates the corresponding piano keys, and a practice mode that visualizes recorded vocalizations for repeated practice. By providing an integrated system to foster and visualize vocalizations, ViVo aims to make learning piano more effective, intuitive, and engaging.",Maya Caren,mayacaren7@gmail.com,Maya Caren (Independent Researcher)*,"Caren, Maya*",Maya Caren,mayacaren7@gmail.com*,Music-related human-computer interaction,"Novel controllers, interfaces or instruments for musical expression",Accept as Short Paper (up to 2000 words),poster,remote,,posters-2,,nime2025_146.pdf,,,146.jpg,"Vocalization, Visualization, Learning, Piano, HCI"
49,Introducing EG-IPT and ipt~: a novel electric guitar dataset and a new Max/MSP object for real-time classification of instrumental playing techniques,"This paper presents two key contributions to the real-time classification of Instrumental Playing Techniques (IPTs) in the context of NIME and human-machine interactive systems: the EG-IPT dataset and the ipt~ Max/MSP object. The EG-IPT dataset, specifically designed for electric guitar, encompasses a broad range of IPTs captured across six distinct audio sources (five microphones and one direct input) and three pickup configurations. This diversity in recording conditions provides a robust foundation for training accurate models. We evaluate the dataset by employing a Convolutional Neural Network-based classifier (CNN), achieving state-of-the-art performance across a wide array of IPT classes, thereby validating the dataset’s efficacy. The ipt~ object is a new Max/MSP external enabling real-time classification of IPTs via pre-trained CNN models. While in this paper it's demonstrated with the EG-IPT dataset, the ipt~ object is adaptable to models trained on various instruments. By integrating EG-IPT and ipt~, we introduce a novel, end-to-end workflow that spans from data collection, model training to real-time classification and human-computer interaction. This workflow exemplifies the entanglement of diverse components (data acquisition, machine learning, real-time processing, and interactive control) within a unified system, advancing the potential for dynamic, real-time music performance and human-computer interaction in the context of NIME.",Marco Fiorini,marco.fiorini@ircam.fr,"Marco Fiorini (IRCAM, Sorbonne Université, CNRS)*; Nicolas Brochec (Tokyo University of the Arts); Joakim Borg (IRCAM); Riccardo Pasini (UNIFE Università di Ferrara)","Fiorini, Marco*; Brochec, Nicolas; Borg, Joakim; Pasini, Riccardo",Marco Fiorini; Nicolas Brochec; Joakim Borg; Riccardo Pasini,marco.fiorini@ircam.fr*; nicolas.brochec@pm.me; joakim.borg@ircam.fr; riccardo01.pasini@edu.unife.it,Machine learning and artificial intelligence in NIMEs,"Entangled NIME: Intertwined, multilayer contexts in NIME research; Music-related human-computer interaction ; New music performance paradigms",Accept as Long Paper (up to 6000 words),poster,in person,Wants to borrow electric guitar for the poster session.,posters-1,,nime2025_49.pdf,,,49.jpg,"Instrumental Playing Techniques, Electric Guitar, Music Classification, Real-Time, Music AI, Python, Max/MSP, NIME"
154,LIMITER: A Gamified Interface for Harnessing Just Intonation Systems,"This paper introduces LIMITER, a gamified digital musical instrument for harnessing and performing microtonal and justly intonated sounds. While microtonality in Western music remains a niche and esoteric system that can be difficult both to conceptualize and to perform with, LIMITER presents a novel, easy to pickup interface that utilizes color, geometric transformations, and game-like controls to create a simpler inlet into utilizing these sounds as a means of expression. We report on the background of the development of LIMITER, as well as explain the underlying musical and engineering systems that enable its function. Additionally, we offer a discussion and preliminary evaluation of the creativity-enhancing effects of the interface.",Antonis Christou,antchris@mit.edu,Antonis Christou (MIT)*,"Christou, Antonis*",Antonis Christou,antchris@mit.edu*,"Novel controllers, interfaces or instruments for musical expression",Musical mapping strategies,Accept as Medium Paper (up to 4000 words),poster,Does not know yet,,posters-1,,nime2025_154.pdf,,,154.jpg,"Just Intonation, microtonality, gamified interfaces, grid interfaces"
155,The Anonymized Instrument: An Iterative Journey in Digital-Acoustic String Instrument Augmentation,"Many experiments in bowed string augmentation have been explored, with each approach reflecting the values and interests of the builder. The anonymized instrument takes a unique approach, with the convolution of a synthesized and acoustic string signal at the foundation of its design. Through an iterative hardware and software development process, three versions of the instrument have been created, each building toward the goal of a robust compositional and performative platform for exploring the shared boundaries of electronic and acoustic music. Spatialization and physical modeling algorithms have furthered the instrument’s engagement with the interaction between physical and virtual acoustics. This paper examines the iterative design process behind the instrument and an evolving relationship between digital augmentation and acoustic resonance.",Brian Lindgren,bklindgren1@gmail.com,Brian Lindgren (Brian Lindgren)*,"Lindgren, Brian*",Brian Lindgren,bklindgren1@gmail.com*,"Augmented, embedded and hyper instruments","Novel controllers, interfaces or instruments for musical expression",Accept as Long Paper (up to 6000 words),poster,remote,,posters-2,,nime2025_155.pdf,,,155.jpg,"chordophone, string controller, convolution, cross-synthesis, augmented stringed instrument"
234,The Drum Machine of Tao,"The Drum Machine of Tao (Tao) is a machine learning-based digital system that returns a sampled drum loop in audio waveform to sequencer parameters and one-shot percussive samples, restoring low-level editability to the drum loops that would otherwise be frozen once sampled. The philosophy behind the design of this drum machine is inspired from Taoism - that which returns to its primal state is the great Way of Tao. In this paper, we introduce the system design of Tao which comprises a state-of-the-art drum source separation model, a sequencer parameter estimation model, and a bespoke one-shot sample extraction algorithm.",Xiaowan Yi,x.yi@qmul.ac.uk,"Xiaowan Yi (Centre for Digital Music, Queen Mary Univeristy of London)*; Mathieu Barthet (Centre for Digital Music, Queen Mary University of London)","Yi, Xiaowan*; Barthet, Mathieu",Xiaowan Yi; Mathieu Barthet,x.yi@qmul.ac.uk*; m.barthet@qmul.ac.uk,Machine learning and artificial intelligence in NIMEs,"Novel controllers, interfaces or instruments for musical expression",Accept as Short Paper (up to 2000 words),poster,in person,,posters-3,,nime2025_234.pdf,,,234.jpg,"drum machine, sequencer parameter estimation, one-shot sample extraction"
163,Towards the Continuous Harmonium: Replicating the Continuous Keyboard,"In our effort to develop an augmented harmonium to enable the performance of continuous pitch ornamentation while preserving typical harmonium gestures, we have replicated the continuous keyboard presented by McPherson et al. in prior work. We present our adaptations to the design of the sensing system, outline our preliminary mapping design, and present a report on the replication process highlighting the obstacles we faced, in hopes of helping with subsequent replications.",Ninad Puranik,ninad.puranik@mail.mcgill.ca,Travis West (McGill University); Ninad Puranik (McGill University )*; Marcelo Wanderley (McGill University); Gary Scavone (McGill University),"West, Travis; Puranik, Ninad*; Wanderley, Marcelo; Scavone, Gary",Travis West; Ninad Puranik; Marcelo Wanderley; Gary Scavone,travis.west@mail.mcgill.ca; ninad.puranik@mail.mcgill.ca*; marcelo.wanderley@mcgill.ca; gary.scavone@mcgill.ca,"Augmented, embedded and hyper instruments","Gesture to sound mapping; Musical mapping strategies ; Novel controllers, interfaces or instruments for musical expression",Accept as Medium Paper (up to 4000 words),poster,Does not know yet,,posters-2,,nime2025_163.pdf,,,163.jpg,"free-reed instruments, harmonium, Hindustani music, keyboard interfaces"
156,Live Improvisation with Fine-Tuned Generative AI: A Musical Metacreation Approach,"This paper presents a pipeline to integrate a fine-tuned open-source text-to-audio latent diffusion model into a workflow with Ableton Live for the improvisation of contemporary electronic music. The system generates audio fragments based on text prompts provided in real time by the performer, enabling dynamic interaction. Guided by Musical Metacreation as a framework, this case study reframes generative AI as a co-creative agent rather than a mere style imitator. By fine-tuning Stable Audio Open on a dataset of the first author’s compositions and field recordings, this approach demonstrates the ethical and practical benefits of open-source solutions. Beyond showcasing the model’s creative potential, this study highlights the model’s significant challenges and the need for democratized tools with real-world applications.",Misagh Azimi,misagh.azimi@vuw.ac.nz,Misagh Azimi (Victoria University of Wellington)*; Mo H. Zareei (Victoria University of Wellington),"Azimi, Misagh*; Zareei, Mo H.",Misagh Azimi; Mo H. Zareei,misagh.azimi@vuw.ac.nz*; mo.zareei@vuw.ac.nz,Machine learning in musical performance,Interactive sound art and sound installations ; Music-related human-computer interaction ; Practice-based research approaches/methodologies/criticism,Accept as Short Paper (up to 2000 words),poster,in person,,posters-2,,nime2025_156.pdf,,,156.jpg,"Generative AI, Improvisation, Real-time Interaction, Musical Metacreation, Democratizing AI"
140,Mixer Metaphors: audio interfaces for non-musical applications,"The NIME conference traditionally focuses on interfaces for music and musical expression. In this paper we reverse this tradition to ask, can interfaces developed for music be successfully appropriated to non-musical applications? To help answer this question we designed and developed a new device, which uses interface metaphors borrowed from analogue synthesisers and audio mixing to physically control the intangible aspects of a Large Language Model. We compared two versions of the device, with and without the audio-inspired augmentations, with a group of artists who used each version over a one week period. Our results show that the use of audio-like controls afforded more immediate, direct and embodied control over the LLM, allowing users to creatively experiment and play with the device over it's non-mixer counterpart. Our project demonstrates how cross-sensory metaphors can support creative thinking and embodied practice when designing new technological interfaces.",Jon McCormack,jon.mccormack@monash.edu,Tace McNamara (Monash University); Jon McCormack (Monash University)*; Maria Teresa Llano (University of Sussex),"McNamara, Tace; McCormack, Jon*; Llano, Maria Teresa",Tace McNamara; Jon McCormack; Maria Teresa Llano,Tace.McNamara@monash.edu; jon.mccormack@monash.edu*; m.t.llano-rodriguez@sussex.ac.uk,Music-related human-computer interaction,Machine learning and artificial intelligence in NIMEs ; Musical mapping strategies ; Practice-based research approaches/methodologies/criticism,Accept as Long Paper (up to 6000 words),poster,in person,,posters-2,,nime2025_140.pdf,,,140.jpg,"audio mixing, creativity, interface design, LLM"
148,Composition and Notation Strategies for Collaborative Mobile Music Performance,"This paper explores the design and performance of Tiny Touch Instruments (TTIs), a set of mobile instruments, and their role in facilitating collaborative, unrehearsed music-making. Through the composition and performance of two pieces, Skating and Skipping, this work investigates how multimodal notation and instrument design can shape performer experience, interaction, and engagement. Performances were documented through participant observation, interviews, and a survey, revealing key themes such as the role of notation in guiding improvisation, the balance between agency and unpredictability in digital instruments, and the recontextualization of mobile devices as musical tools. Findings suggest that multimodal notation enhances accessibility, structured prompts encourage collaboration, and mobile instruments offer new opportunities for interactive performance. This work contributes to ongoing discourse in digital instrument design, new music notation, and mobile ensemble performance, offering insights into how mobile technology can foster playful, collaborative, and emergent musical experiences.",Rebecca Abraham,rebecca.s.abraham.gr@dartmouth.edu,Rebecca Abraham (Dartmouth College)*,"Abraham, Rebecca*",Rebecca Abraham,rebecca.s.abraham.gr@dartmouth.edu*,New performance paradigms for mobile music-making,Musical mapping strategies ; Technologies or systems for collaborative music-making ; User studies and evaluations of NIMEs,Accept as Medium Paper (up to 4000 words),poster,in person,,posters-2,,nime2025_148.pdf,,,148.jpg,"mobile music-making,collaboration,touch instruments"
64,Sculpting the Sound Atom: Towards Per-Grain Parameterisation and Interfacing in Granular Synthesiser Design,"This paper presents a number of custom-designed granular synthesisers built around interfacing with sound grains on an ‘atomic' level. Developed by the first author using Max/MSP, these synthesisers explore per-grain voice parameterisation that uniquely interfaces with individual grain signal processing properties in larger granular sequences. Conceptually inspired by ancient Epicurean physics and painterly expositions on atomism in Lucretius' poem ‘The Nature of Things' (c. 55 BC), the paper outlines how these synthesisers provided the sound materials to compose the first author's major soundscape work [anonymous work].",Nathan Carter,carternath1@myvuw.ac.nz,Nathan Carter (Victoria University of Wellington - Te Kōkī New Zealand School of Music)*; Jim Murphy (Victoria University of Wellington - Te Kōkī New Zealand School of Music ); Mo Zareei (Victoria University of Wellington - Te Kōkī New Zealand School of Music ),"Carter, Nathan*; Murphy, Jim; Zareei, Mo",Nathan Carter; Jim Murphy; Mo Zareei,carternath1@myvuw.ac.nz*; jim.murphy@vuw.ac.nz; mo.zareei@vuw.ac.nz,"Novel controllers, interfaces or instruments for musical expression",Practice-based research approaches/methodologies/criticism,Accept as Short Paper (up to 2000 words),poster,in person,,posters-1,,nime2025_64.pdf,,,64.jpg,"Granular Synthesiser, Per-Grain Parameterisation, Atomism, Max/MSP, Practice-Based Research"
135,The Shadow Harvester: Auralizing the Body Through Light,"This paper explores the use of a violinist’s shadow as an input source for a NIME called the Shadow Harvester. For centuries, shadows have captivated humanity’s imagination and this project follows the many artists, philosophers, and researchers equally captivated by the potential of shadows. This interface consists of a semi-translucent screen embedded with light-detecting sensors.  These sensors register the movement of the violinist’s shadow and produce data that can be mapped to generate or process the violinist’s live sound. An analysis of the violinist’s movement informs sensor placement and encourages the performer and composer to reexamine the link between motion and sound production. The Shadow Harvester  turns a human shadow into a real-time, life-size avatar, splitting the attention of the violinist between their shadow-self and carnal-self. They are ensnared in a web of sensors that require the same attention as the visceral joints in their body because any movement produces sound through either their physical-body playing the violin or their shadow-body “playing’’ this interface. The Shadow Harvester creates a highly entangled feedback loop between the violinist, centuries of violin performance practice, and composition, that requires new ways of incorporating movement into the compositional, notational, and performance process.",Darlene Castro,dmcastro@uchicago.edu,Darlene Castro (University of Chicago)*,"Castro, Darlene*",Darlene Castro,dmcastro@uchicago.edu*,"Novel controllers, interfaces or instruments for musical expression","Entangled NIME: Intertwined, multilayer contexts in NIME research; Explorations of relationships between motion, gesture and music; Musical mapping strategies",Accept as Medium Paper (up to 4000 words),poster,in person,,posters-2,,nime2025_135.pdf,,,135.jpg,"NIME, Augmented Instruments, Performer-Technology Interaction, Light Sensors"
189,PanMan - a modular tangible controller for sound spatialization,"PanMan is a performance-oriented modular midi controller, conceived as a tangible interface for panning multiple sound sources in multichannel audio systems. It consists of four independent control units and a docking base - the modular design allows each of the units to be either physically attached to the base, or connected to it via extension cords, allowing up to four users to participate in an interactive sound installation experience or a collaborative performance setting.  The physical controls on a single module consist of a joystick/trackball hybrid – a dome-shaped control device designed to be operated with a single finger – and a thumbwheel for additional parameter control, positioned at the edge of the module, allowing for one-handed operation of three parameters. The design facilitates operation by both right- and left-handed users, also allowing a single user to operate two controllers simultaneously, controlling up to six parameters at once.",Krzysztof Cybulski,kcybulski@pangenerator.com,Krzysztof Cybulski (Feliks Nowowiejski Academy of Music in Bydgoszcz)*; Szczepan Busko (Feliks Nowowiejski Academy of Music in Bydgoszcz); Zachariasz Zalewski (Feliks Nowowiejski Academy of Music in Bydgoszcz),"Cybulski, Krzysztof*; Busko, Szczepan; Zalewski, Zachariasz",Krzysztof Cybulski; Szczepan Busko; Zachariasz Zalewski,kcybulski@pangenerator.com*; 4031@mail.amfn.pl; 4160@mail.amfn.pl,"Novel controllers, interfaces or instruments for musical expression",Gesture to sound mapping,Accept as Short Paper (up to 2000 words),poster,Does not know yet,,posters-2,,nime2025_189.pdf,,,189.jpg,"tangible, spatial, controller, interaction"
200,Sonicolour: Exploring Colour Control of Sound Synthesis with Interactive Machine Learning,"This paper explores crossmodal mappings of colour to sound. The instrument presented analyses the colour of physical objects via a colour light-to-frequency sensor and maps the corresponding red, green and blue data values to parameters of a synthesiser. Interactive machine learning is used to facilitate the discovery of new relationships between sound and colour. The role of interactive machine learning is to find unexpected relationships between the visual features of the objects and the sound synthesis.The performance is evaluated by its ability to provide the user with a playful interaction between the visual and tactile exploration of coloured objects, and the generation of synthetic sounds. We conclude by outlining the potential of this approach for musical interaction design and music performance.",Charalampos Saitis,c.saitis@qmul.ac.uk,Tug F. O'Flaherty (Queen Mary University of London); Luigi Marino (Queen Mary University of London); Charalampos Saitis (Queen Mary University of London)*; Anna Xambó Sedó (Queen Mary University of London),"O'Flaherty, Tug F.; Marino, Luigi; Saitis, Charalampos*; Xambó Sedó, Anna",Tug F. O'Flaherty; Luigi Marino; Charalampos Saitis; Anna Xambó Sedó,t.f.oflaherty@se24.qmul.ac.uk; l.marino@qmul.ac.uk; c.saitis@qmul.ac.uk*; a.xambosedo@qmul.ac.uk,"Novel controllers, interfaces or instruments for musical expression",Machine learning and artificial intelligence in NIMEs ; Practice-based research approaches/methodologies/criticism,Accept as Medium Paper (up to 4000 words),poster,remote,,posters-2,,nime2025_200.pdf,,,200.jpg,"supervised learning, crossmodal mapping, additive synthesis, synaesthesia"
202,A Spherical Tape Topology for Non-linear Audio Looping,"There have been many physical design formats used in the field of audio recording. As audio has an inherently a linear, time-based structure, these have generally followed logical layouts such as tape, or grooved records and cylinders. This project explores magnetic recording technology and digital analogues for recording and playback that are instead on spherical topology. This instrument expands the concept of the audio loop through a more tangible and randomized approach than traditional record playback techniques of tape, while maintaining a familiarity with historic techniques of audio looping and scrubbing. Through it, one can not only create linear time-loops but blends between different times of the recording non-sequentially. The size and mass of the spheres enhances the performative elements through the physics of inertia. The movement possibilities allow for non-linear circles, circuits, spirals and other patterns of sound not traditionally possible through linear tape or digital loop, including accelerations and decelerations – akin to a turntable, but with greater freedom of direction, thus offering surreal record/playback possibilities.",Kevin Blackistone,kevin@blackistone.com,"Kevin Blackistone (Tangible Music Lab, Kunstuniversität Linz, Hauptplatz 6, 4020 Linz)*","Blackistone, Kevin*",Kevin Blackistone,kevin@blackistone.com*,"Novel controllers, interfaces or instruments for musical expression",Interactive sound art and sound installations ; New music performance paradigms; Sonic interaction design,Accept as Medium Paper (up to 4000 words),poster,remote,,posters-2,,nime2025_202.pdf,,,202.jpg,"loops, playback, spheres, non-linearity"
208,Exploiting Latency In The Design Of A Networked Music Performance System For Percussive Collective Improvisation,"We present the design, prototype implementation, and informal testing of a distributed web-based networked music performance (NMP) system for collaborative improvisation and experimentation. Influenced by composition and interaction design techniques from a wide range of work on collaborative virtual music environments, rather than treating latency as inherently disruptive to the types of musical and social engagement that characterize traditional performance, we incorporate and exploit network delay to facilitate and visualize them, providing a novel approach to creating ""jam session""-like experiences. During sessions, users collaboratively perform semi-improvised music in quasi-real time. The production and interpretation of individual musical gestures (""drum hits"") are visualized in a continuously devised feedback network. The music produced can be treated as a starting point for compositions developed asynchronously, or as complete pieces of music produced live.",Ari Liloia,aliloia@alumni.cmu.edu,Ari Liloia (Carnegie Mellon University)*; Roger Dannenberg (Carnegie Mellon University),"Liloia, Ari*; Dannenberg, Roger",Ari Liloia; Roger Dannenberg,aliloia@alumni.cmu.edu*; rbd@cs.cmu.edu,Web-based music performance,"Entangled NIME: Intertwined, multilayer contexts in NIME research; New music performance paradigms; Technologies or systems for collaborative music-making",Accept as Long Paper (up to 6000 words),poster,remote,,posters-3,,nime2025_208.pdf,,,208.jpg,"Networked Music Performance, Web-Based Music Systems, Network Latency, Collective Improvisation, Percussion"
214,Hybrid Hand Drum,"This paper presents a hybrid frame drum that entangles acoustic and digital elements, merging the expressive depth of traditional percussion with the expanded possibilities of digital sound processing. Designed with four key principles - portability, hybridity, simplicity and low latency - the instrument allows for a fluid interplay between physical and real-time digital augmentation.   Equipped with piezoelectric sensors, an FSR, and a DSP algorithm the drum extends its sonic landscape while preserving its acoustic presence. The design maintains an organic relationship between physical interaction and digital processing, and the three potentiometers provide intuitive yet flexible control, maintaining a balance between minimalism and expressivity. The bela platform ensures very low latency (7.55 ± 0.13 ms), making it highly responsive for live performance.  User evaluation highlights its potential for expressive control and seamless hybrid performance while suggesting ergonomic and functional refinements. Future enhancements, such as feedback control and DSP presets, could deepen the entanglement between performer, instrument, and sound. This research explores the intersection of acoustic and digital sound, contributing to the design of hybrid instruments that blur the boundaries between physical resonance and electronic transformation, expanding possibilities for musical interaction.",Casper Preisler,cpreis24@student.aau.dk,Casper Preisler (AAU)*; Daniel Overholt (Aalborg University Copenhagen),"Preisler, Casper*; Overholt, Daniel",Casper Preisler; Daniel Overholt,cpreis24@student.aau.dk*; dano@create.aau.dk,"Augmented, embedded and hyper instruments","Novel controllers, interfaces or instruments for musical expression ; Pedagogical perspectives and/or student projects in NIME-related courses",Accept as Medium Paper (up to 4000 words),poster,Does not know yet,,posters-3,,nime2025_214.pdf,,,214.jpg,"augmented drum, digital musical instruments, electronic percussion, hybrid instrument"
33,Waveform Autoencoding at the Edge of Perceivable Latency,"We introduce an audio plugin implementation of BRAVE, a waveform autoencoder presented recently, that affords Neural Audio Synthesis with low latency and jitter. As a redesign of the well-known RAVE model, BRAVE introduces a series of architectural modifications for supporting instrumental interaction with almost imperceptible latency (<10 ms) and jitter (~ 3 ms). By comparing both designs, we highlight key architectural differences between the models that impact their instrumental performance capability, arguing that no model fits all purposes, and calling for their careful selection for each interactive design. Finally, we discuss challenges and opportunities for leveraging low-latency waveform autoencoders to develop interactive systems, such as Digital Musical Instruments, that can foster control intimacy through enhanced responsiveness and space for nuance.",Franco Caspe,f.s.caspe@qmul.ac.uk,Franco Caspe (Queen Mary University of London - Centre For Digital Music)*; Andrew McPherson (Imperial College London); Mark Sandler ( Queen Mary University of London - Centre For Digital Music),"Caspe, Franco*; McPherson, Andrew; Sandler, Mark",Franco Caspe; Andrew McPherson; Mark Sandler,f.s.caspe@qmul.ac.uk*; andrew.mcpherson@imperial.ac.uk; mark.sandler@qmul.ac.uk,Performance rendering and generative algorithms,"New music performance paradigms; Novel controllers, interfaces or instruments for musical expression",Accept as Short Paper (up to 2000 words),poster,in person,,posters-1,,nime2025_33.pdf,,,33.jpg,"Instrumental Performance, Timbre Transfer, Low Latency, Neural Audio Synthesis"
226,Sonifying the MindCube through AI-Generated Music,"In this work, we explore the musical interface potential of the MindCube, an interactive device designed to study emotions. Embedding diverse sensors and input devices, this interface resembles a fidget cube toy commonly used to help users relieve their stress and anxiety. As such, it is a particularly well-suited controller for musical systems that aim to help with emotion regulation. In this regard, we present two different mappings for the MindCube, with and without AI. With our generative AI mapping, we propose a way to infuse meaning within a latent space and techniques to navigate through it with an external controller. We discuss our results and propose directions for future work.",Lancelot Blanchard,lancelot@media.mit.edu,Fangzheng Liu (MIT Media Lab); Lancelot Blanchard (MIT Media Lab)*; Don D Haddad (MIT Media Lab); Joseph Paradiso (MIT Media Lab),"Liu, Fangzheng; Blanchard, Lancelot*; Haddad, Don D; Paradiso, Joseph",Fangzheng Liu; Lancelot Blanchard; Don D Haddad; Joseph Paradiso,fzliu@media.mit.edu; lancelot@media.mit.edu*; ddh@mit.edu; joep@media.mit.edu,"Novel controllers, interfaces or instruments for musical expression","Gesture to sound mapping; Machine learning and artificial intelligence in NIMEs ; Sensor and actuator technologies, including haptics and force feedback devices",Accept as Medium Paper (up to 4000 words),poster,remote,,posters-3,,nime2025_226.pdf,,,226.jpg,"sensors, emotions, generative AI, gesture, controller"
71,Collaboration and Recursion: reflections on Chinese calligraphy and feedback,"This paper delivers reflections on collaboration between a sound artist and an artist specializing in Chinese calligraphy, that resulted in the creation of a sound installation combining contemporary Chinese calligraphy with electroacoustic feedback. Adopting the “reflection-on-action” approach, the authors engaged in an in-depth discussion, revisiting the details of the creative process based on the extensive documentation arranged in a form of visual diary. The paper highlights three orders of recursivity that are either physically present in the work (feedback), defined the creative process (collaboration) and used as analytical tools (ecology) to discuss the dynamics of collaboration and cultural influences in NIME practice.",Iurii Kuzmin,ikuzmin@connect.ust.hk,Iurii Kuzmin (The Hong Kong University of Science and Technology)*; Raul Masu (The Hong Kong Unversity of Science and Technology (Guangzhou)); Omar Al Kanawati (China Academy of Art ),"Kuzmin, Iurii*; Masu, Raul; Al Kanawati, Omar",Iurii Kuzmin; Raul Masu; Omar Al Kanawati,ikuzmin@connect.ust.hk*; raul@raulmasu.org; omarkanawati@post.sk,Practice-based research approaches/methodologies/criticism,"Discussions about the artistic, cultural, and social impact of NIME technology ; Interactive sound art and sound installations",Accept as Long Paper (up to 6000 words),poster,in person,,posters-1,,nime2025_71.pdf,,,71.jpg,"Chinese calligraphy, collaboration, feedback, sound installation"
184,Metabow: Gesture Mapping in Immersive Sonic Environment,"This paper explores the design and implementation of interactive systems for musical performance using the MetaBow, an augmented violin bow that transmits real-time motion data for expressive control of digital sound processing. Adopting an autoethnographic approach, we analyze challenges in mapping Inertial Movement Unit (IMU) data to sound parameters in immersive environments, leveraging machine learning tools such as FluCoMa. The study examines how gesture-driven performance can be adapted across various spatial settings, with a focus on real-time audio manipulation and responsive interaction strategies. Case studies include the 9 Shards, Remote Gestures, and Exo Signals projects, developed for multi-speaker ambisonic settings. The paper discusses mapping paradigms, technical constraints, and artistic implications of using the MetaBow as a performative interface.",Davor Vincze,vincze@hkbu.edu.hk,Davor Vincze (Hong Kong Baptist University)*; Roberto Alonso (Hong Kong Baptist University); Peter Nelson (Hong Kong Baptist University),"Vincze, Davor*; Alonso, Roberto; Nelson, Peter",Davor Vincze; Roberto Alonso; Peter Nelson,vincze@hkbu.edu.hk*; robertoalonso@hkbu.edu.hk; peteracnelson@hkbu.edu.hk,Practice-based research approaches/methodologies/criticism,"Augmented, embedded and hyper instruments ; Machine learning in musical performance; Musical mapping strategies",Accept as Short Paper (up to 2000 words),poster,in person,,posters-3,,nime2025_184.pdf,,,184.jpg,"augmented violin bow, machine learning and gesture, mapping strategies in immersive environments"
246,Tapping Into a New Paradigm: A Synthetic Strategy for Automatic Drum TapScription,"We introduce Automatic Drum TapScription (ADTS), a novel paradigm for rhythmic interaction consisting of transcribing arbitrarily-timbred taps into drum representations. Our approach targets taps produced on a variety of surfaces without other controlled timbral characteristics other than playing style. Our long-term goal is to enable more accessible and creative percussive exploration but presents significant challenges due to the minimal timbre variation between taps intended to represent different drum classes. To address these challenges, we take the first steps toward achieving ADTS by designing an effective dataset synthesis strategy. This strategy enables new opportuneties for musical expression by considering drumming at a more semantic or functional level as opposed to a simple collection of timbres. We present initial results, comparing three different models: one trained on drum data, another trained on a small dataset of quasi-aligned tapped performances, and another trained on our synthetic dataset. Our synthetic approach shows promise, demonstrating progress in this untapped domain.",André Santos,andresantos@dei.uc.pt,André Santos (CISUC - UC)*; Amílcar Cardoso (CISUC - UC); Matthew  E. P. Davies (SiriusXM); Roger B. Dannenberg (CMU),"Santos, André*; Cardoso, Amílcar;  E. P. Davies, Matthew; B. Dannenberg, Roger",André Santos; Amílcar Cardoso; Matthew  E. P. Davies; Roger B. Dannenberg,andresantos@dei.uc.pt*; amilcar@dei.uc.pt; mepdavies@gmail.com; rbd@andrew.cmu.edu,"Software frameworks, interface protocols, and data formats, for supporting musical interaction","Machine learning in musical performance; Musical mapping strategies ; Novel controllers, interfaces or instruments for musical expression",Accept as Long Paper (up to 6000 words),poster,remote,,posters-3,,nime2025_246.pdf,,,246.jpg,"Automatic Drum Transcription, Rhythmic Interaction, Synthetic Dataset, Musical Expression"
250,Towards a Repository Template for Music Technology Research,"Documenting and sharing research output is essential to construct the critical discourse on new music technology. Documentation feeds the knowledge and the values with which to evaluate and discuss current achievements and musical creations as well as to plan for the future. Besides publishing our research in conferences and journals, sharing research materials and outcomes like software, hardware, instruments, and datasets is important. This allows others to use the latest technology and improve it. For this purpose, the repository is increasingly commonly used by researchers and artists to store and share their works. However, creating repositories does not follow a clear and organised structure like the one we find, for example, in papers. The heterogeneity of repositories makes it hard to use both practically and for analysis. Although the variety and differences of research products in the field of new musical technologies are obvious, we believe that defining repositories with common guidelines could significantly improve the critical discourse in this area. This issue has been discussed at the NIME conference through workshops and papers. In this article, we want to continue this discussion and propose a flexible repository template to organise and present research materials and outcomes in the field of musical technologies research. The article provides a short and focused review of how repositories are currently used at the NIME conference, with special attention to the platforms used. Based on this study, we introduce a repository template that will be applied to case studies. We hope this proposal will encourage further discussion and advancement on this issue and, at the same time, support and facilitate the creation of new repositories.",Alessandro Fiordelmondo,fiordelmondo@dei.unipd.it,Alessandro Fiordelmondo (CSC Padova)*; Matteo Spanio (CSC Padova); Patricia Cadavid (Univesity of West of England); Xinran Chen (The Hong Kong University of Science and Technology (Guangzhou)); Sergio Canazza (CSC Padova); Raul Masu (The Hong Kong University of Science and Technology (Guangzhou)),"Fiordelmondo, Alessandro*; Spanio, Matteo; Cadavid, Patricia; Chen, Xinran; Canazza, Sergio; Masu, Raul",Alessandro Fiordelmondo; Matteo Spanio; Patricia Cadavid; Xinran Chen; Sergio Canazza; Raul Masu,fiordelmondo@dei.unipd.it*; spanio@dei.unipd.it; Patricia.Cadavidhinojosa@uwe.ac.uk; xchen805@connect.hkust-gz.edu.cn; canazza@dei.unipd.it; raul@raulmasu.org,Technologies or systems for collaborative music-making,"Discussions about the artistic, cultural, and social impact of NIME technology ; Practice-based research approaches/methodologies/criticism ; User studies and evaluations of NIMEs",Accept as Medium Paper (up to 4000 words),poster,remote,,posters-3,,nime2025_250.pdf,,,250.jpg,"Documentation, repository, shareability"
124,The Sparksichord: Practical Implementation of a Lorentz-Force Electromagnetic Actuation and Feedback System,"In line with a sustained community interest in electromagnetic actuation of musical instruments, we describe practical considerations for Lorentz Force actuation in conductive strings, exemplified by the Sparksichord – an augmented harpsichord that uses Lorentz Force actuation, optical feedback, and analog circuitry to sustain vibrations of its brass strings. Electromagnetically-actuated and feedback instruments have grown increasingly popular in NIME, though most systems rely on the use of solenoid-style electromagnetic coils. By running current through the string itself, Lorentz Force actuation offers an alternate arrangement of magnets and wire that can afford new modes of interaction, a broader frequency response, and cheaper implementation.  We aim to empower practitioners with a toolbox for designing and building actuated instruments of this style and describe our specific implementation for this instrument.",Adam Schmidt,a.schmidt24@imperial.ac.uk,Adam Schmidt (Imperial College London)*; Jeffrey Snyder (Princeton University); Gian Torrano Jacobs (Montclair State University); Joyce Chen (Princeton University); Joseph Gascho (University of Michigan); Andrew McPherson (Imperial College London),"Schmidt, Adam*; Snyder, Jeffrey; Torrano Jacobs, Gian; Chen, Joyce; Gascho, Joseph; McPherson, Andrew",Adam Schmidt; Jeffrey Snyder; Gian Torrano Jacobs; Joyce Chen; Joseph Gascho; Andrew McPherson,a.schmidt24@imperial.ac.uk*; josnyder@princeton.edu; torranojacog1@montclair.edu; joyce.chen@princeton.edu; jgascho@umich.edu; andrew.mcpherson@imperial.ac.uk,"Sensor and actuator technologies, including haptics and force feedback devices","Augmented, embedded and hyper instruments",Accept as Long Paper (up to 6000 words),poster,in person,,posters-2,,nime2025_124.pdf,,,124.jpg,"Electromagnetic actuation, sustain, augmented instruments, analog circuits, optical sensing"
263,AI Harmonizer: Expanding Vocal Expression with a Generative Neurosymbolic Music AI System,"Vocals harmonizers are powerful tools to help solo vocalists enrich their melodies with harmonically supportive voices. These tools exist in various forms, from commercially available pedals and software to custom-built systems, each employing different methods to generate harmonies. Traditional harmonizers often require users to manually specify a key or tonal center, while others allow pitch selection via an external keyboard–both approaches demanding some degree of musical expertise. The AI Harmonizer introduces a novel approach by autonomously generating musically coherent four-part harmonies without requiring prior harmonic input from the user. By integrating state-of-the-art generative AI techniques for pitch detection and voice modeling with custom-trained symbolic music models, our system arranges any vocal melody into rich choral textures. In this paper, we present our methods, explore potential applications in performance and composition, and discuss future directions for real-time implementations. While our system currently operates offline, we believe it represents a significant step toward AI-assisted vocal performance and expressive musical augmentation.",Lancelot Blanchard,lancelot@media.mit.edu,Lancelot Blanchard (MIT Media Lab)*; Cameron Holt (MIT); Joseph Paradiso (MIT Media Lab),"Blanchard, Lancelot*; Holt, Cameron; Paradiso, Joseph",Lancelot Blanchard; Cameron Holt; Joseph Paradiso,lancelot@media.mit.edu*; camholt@mit.edu; joep@media.mit.edu,Machine learning and artificial intelligence in NIMEs,"Machine learning in musical performance; Novel controllers, interfaces or instruments for musical expression ; Software frameworks, interface protocols, and data formats, for supporting musical interaction",Accept as Short Paper (up to 2000 words),poster,remote,,posters-3,,nime2025_263.pdf,,,263.jpg,"machine learning, generative ai, vocal performance, harmonizer, voice modeling"
31,pybela: a Python library to interface scientific and physical computing,"Workflows to obtain, examine and prototype with sensor data often involve a back and forth between environments, platforms and programming languages. Usually, sensors are connected to physical computing platforms, and solutions to transmit data to the computer often rely on low-bandwidth communicating channels. It is not obvious how to interface physical computing platforms with data science environments, which also operate under distinct constraints and programming styles. We introduce pybela, a Python library that facilitates real-time, high-bandwidth, bidirectional data streaming between the Bela embedded computing platform and Python, bridging the gap between physical computing environments and data-driven workflows. In this paper, we outline its design, implementation and applications, including deep learning examples.",Teresa Pelinski,t.pelinskiramos@qmul.ac.uk,Teresa Pelinski (Queen Mary University of London)*; Andrew McPherson (Imperial College London),"Pelinski, Teresa*; McPherson, Andrew",Teresa Pelinski; Andrew McPherson,t.pelinskiramos@qmul.ac.uk*; andrew.mcpherson@imperial.ac.uk,"Software frameworks, interface protocols, and data formats, for supporting musical interaction",Machine learning and artificial intelligence in NIMEs ; Machine learning in musical performance,Accept as Long Paper (up to 6000 words),poster,in person,,posters-1,,nime2025_31.pdf,,,31.jpg,"prototyping, physical computing, machine learning, embedded AI"
278,A Gesture-Based Approach to Spatialization in Dolby Atmos,"This paper presents a system for the spatialization of sound objects in the Dolby Atmos format, implemented through the integration of an infrared sensor with a chain of three software tools. The setup enables translating hand gestures into spatialization data within the constraints of the Atmos format. The design and parameter mapping are described and evaluated, along with its usability, strengths, and limitations, as assessed through an autoethnographic experience conducted by the author. Beyond the technical aspects, this article reflects on the author's experience using the system as a mixing engineer and connects these evaluations to the conceptual framework of related works. This perspective offers a critical reflection on spatialization as a performative practice within studio workflows, highlighting how such devices may be integrated into the multimodal studio environment to introduce new means of interaction in sound spatialization.",Vitor Pinheiro,pinheiroproducaomusical@gmail.com,Vitor Pinheiro (UFPR)*,"Pinheiro, Vitor*",Vitor Pinheiro,pinheiroproducaomusical@gmail.com*,Gesture to sound mapping,"Explorations of relationships between motion, gesture and music; Novel controllers, interfaces or instruments for musical expression",Accept as Short Paper (up to 2000 words),poster,Does not know yet,,posters-3,,nime2025_278.pdf,,,278.jpg,"Spatialization, Dolby Atmos, Leap Motion, Music Mixing, Gesture based systems"
280,ChuMP: The Zen and Art of Package Management,"ChuMP stands for “ChucK Manager of Packages”, designed to automate the process of installing, upgrading, and removing software components for the ChucK programming ecosystem. ChuMP manages libraries, tools, audio and graphics plugins in a centralized, structured, and versioned manner. This project originated out of the recent ChucK development “renaissance” alongside a growing user community, now entering its third decade. The time, as the ChucK slogan goes, is now.  What began as a practical project has expanded into broader reflections on tool-building, service, and community. As we labored on what seemed like a “no-brainer” tool that everyone wanted but that no one wanted to build, questions arose: “how did we get here?”, “what is the role of service-based tool-building in our field–and what, if any, is its research value?”—in short, “should we even write a paper about a package manager?”. Meanwhile, we couldn’t help but notice that the act of creating a package manager seems to unify not only disparate software fragments, but also something of community. In other words, there may be more than meets the eye. This paper chronicles the making of a package manager and all that goes along with it. This is the story of ChuMP.",Nicholas Shaheed,nshaheed@ccrma.stanford.edu,Nicholas Shaheed (Stanford University)*; Ge Wang (Stanford University),"Shaheed, Nicholas*; Wang, Ge",Nicholas Shaheed; Ge Wang,nshaheed@ccrma.stanford.edu*; ge@ccrma.stanford.edu,"Software frameworks, interface protocols, and data formats, for supporting musical interaction",Practice-based research approaches/methodologies/criticism,Accept as Long Paper (up to 6000 words),poster,in person,,posters-3,,nime2025_280.pdf,,,280.jpg,"package manager, philosophy, community, craft, ""is this research?"""
282,Synthesizing Music with Logic Gate Networks,"Small digital circuits consisting of basic logic gates (AND, XOR, etc.) are capable of generating surprisingly complex musical output. In this paper, we present physical and web-based interfaces for exploring the space of audio-generating logic gate networks and 'bending' such networks via touch (or mouse) gestures to interfere with their operation and change their output while they are running. This work follows in the vein of bytebeat practices, in which music is generated by short code snippets at the level of individual audio samples, but takes things further by relying on an even lower-level form of computation. In addition to presenting our system, we offer some preliminary analysis of why these logic gate networks tend to produce musical output.",Ian Clester,ijc@gatech.edu,Ian Clester (Georgia Institute of Technology)*,"Clester, Ian*",Ian Clester,ijc@gatech.edu*,"Novel controllers, interfaces or instruments for musical expression",Gesture to sound mapping; Interactive sound art and sound installations ; Web-based music performance,Accept as Medium Paper (up to 4000 words),poster,remote,,posters-3,,nime2025_282.pdf,,,282.jpg,"computer music, logic gate synthesis, bytebeat, live coding, circuit bending"
284,SMucK: Symbolic Music in ChucK,"SMucK (Symbolic Music in ChucK) is a library and workflow for creating music with symbolic data in the ChucK programming language. It extends ChucK by providing a framework for symbolic music representation, playback, and manipulation. SMucK introduces classes for scores, parts, measures, and notes; the latter encode musical information such as pitch, rhythm, and dynamics. These data structures allow users to organize musical information sequentially and hierarchically in ways that reflect familiar conventions of Western music notation. SMucK supports data interchange with formats like MusicXML and MIDI, enabling users to import notated scores and performance data into SMucK data structures. SMucK also introduces SMucKish, a compact high-level input syntax, designed to be efficient, human-readable, and live-codeable. The SMucK playback system extends ChucK’s strongly-timed mechanism with dynamic temporal control over real-time audio synthesis and other systems including graphics and interaction. Taken as a whole, SMucK’s design philosophy treats symbolic music data not only as static representations but also as mutable, recombinant building blocks for algorithmic and interactive processing. By integrating symbolic music into a strongly-timed, concurrent programming language, SMucK’s workflow goes beyond data representation and playback, and opens new possibilities for algorithmic composition, instrument design, and musical performance.",Alexander Han,tae1han@stanford.edu,Alexander Han (Stanford University)*; Kiran Bhat (Stanford University); Ge Wang (Stanford University),"Han, Alexander*; Bhat, Kiran; Wang, Ge",Alexander Han; Kiran Bhat; Ge Wang,tae1han@stanford.edu*; kvbhat@stanford.edu; ge@ccrma.stanford.edu,"Software frameworks, interface protocols, and data formats, for supporting musical interaction",Interface protocols and data formats supporting musical interaction,Accept as Long Paper (up to 6000 words),poster,in person,,posters-3,,nime2025_284.pdf,,,284.jpg,"ChucK, programming, notation, symbolic music"
291,Melia: An Expressive Harmonizer at the Limits of AI,"We present Melia, a harmonizer-like digital instrument that explores how common failure modes of machine learning and artificial intelligence (ML/AI) systems can be used in expressive and musical ways. The instrument is anchored by an audio-to-audio neural network trained on a hand-curated dataset to perform pitch-shifting and dynamic filtering. Biased training data and poor out-of-distribution generalization are deliberately leveraged as musical devices and sources of instrument-defining idiosyncrasies. Melia features a custom hardware interface with a MIDI keyboard that polyphonically allocates instances of the model to harmonize live audio input, and integrated hardware controls that manipulate model parameters and various audio effects in real-time. This paper presents an overview of related work, the instrument's hardware and software, and how audio-to-audio AI models might fit into the long-standing tradition of musicians, artists, and instrument-makers finding inspiration in a medium's shortcomings.",Matthew Caren,mcaren@mit.edu,Matthew Caren (Massachusetts Institute of Technology)*; Joshua Bennett (Massachusetts Institute of Technology),"Caren, Matthew*; Bennett, Joshua",Matthew Caren; Joshua Bennett,mcaren@mit.edu*; joshuab@mit.edu,"Novel controllers, interfaces or instruments for musical expression",Machine learning and artificial intelligence in NIMEs ; New music performance paradigms,Accept as Short Paper (up to 2000 words),poster,remote,,posters-3,,nime2025_291.pdf,,,291.jpg,"Harmonizer, AI, Voice, NIME"
167,SwimTunes: A gamified music performance system for co-creating with a novice audience,"This paper presents SwimTunes, a prototype game system designed for accessible multi-user music-making in live performance settings. The system features a digital game and a public web app that allows audience members to participate using their mobile devices. After connecting via QR code, participants create and pilot virtual ‘fish’ that generate music as they bump into one another. The performer then enters the game as a ‘shark’, using camera-based hand tracking to chase and consume the participants’ fish. The result is a performance dynamic that evolves from playful co-creation to one of gameful contest between the performer and audience. SwimTunes explores how this shifting interaction context can shape the instantiation of a fixed set of musical parameters, and further how performers can harness gameplay metaphors to conduct live audiences in shared acts of musical expression. The paper details the design considerations and conceptual motivations that informed SwimTunes before describing its implementation via Node.js, Open Sound Control, Unreal Engine 5, and MetaSounds. It discusses technical challenges and opportunities unearthed during development and outlines future directions for the project and gamified music performance at large.",Thomas Studley,thomas.studley@qut.edu.au,Thomas Studley (Queensland University of Technology)*,"Studley, Thomas*",Thomas Studley,thomas.studley@qut.edu.au*,Technologies or systems for collaborative music-making,"New music performance paradigms; Software frameworks, interface protocols, and data formats, for supporting musical interaction ; Web-based music performance",Accept as Medium Paper (up to 4000 words),poster,in person,,posters-2,,nime2025_167.pdf,,,167.jpg,"Music game, gamified performance, audience participation, Unreal Engine 5, MetaSounds"