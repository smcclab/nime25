id,track,format,type,session_code,session_name,duration,presence,title,authors,abstract,paper_url,video_url,slides_url,image_url,image_credit,problems,location
68,paper,oral,short,papers-1,Collective and Embodied,5,remote,Between Garment and Prosthesis:  The Design of an E-Textile Musical Interface,Qiaosheng Lyu; Ryo Ikeshiro,"This paper presents Noisy Flesh, an e-textile musical interface designed to control sound through body movement in performance. Unlike traditional wearable interfaces that function as garments, this work reimagines the textile interface as a prosthetic extension that augments the performer’s body. The paper discusses both the design of the interface and its sonification method, emphasising how the flexibility of e-textiles can transform bodily movement and shape interactive experiences. This work explores the potential of e-textile interfaces to challenge conventional notions of wearability and embodiment in performance.",nime2025_68.pdf,,,68.jpg,,,
78,paper,oral,long,papers-1,Collective and Embodied,15,remote,Towards Neurodiverse Sensemaking: Pluralizing Agency in Wearable Music and Participatory Workshopping,Seth Thorn; Anani Vasquez; Corey Reutlinger; Margarita Pivovarova; Mirka Koro,"We, a team of teachers and researchers, share examples of collectively playable instruments that challenge normative assumptions about intention and agency in digital musical instruments. These instruments enliven neurodiverse sensemaking in participatory design and STEAM learning. Through a multiyear research-practice partnership (RPP), we collaborated with teaching fellows to co-design a curriculum for neurodiverse middle school students that activates computational thinking (CT). This collaboration led to a web-based, quasi-modular interface connected to wearable music sensors. We situate our work within the growing literature on participatory design of collaborative accessible digital musical instruments (CADMIs). We describe how our co-design methods address the complex demands of ecosystemic thinking, sensitive to the varied entanglements that complicate traditional human-computer interaction (HCI) design and evaluation methods. Our pedagogical and methodological approach diverges from deficit-focused strategies that aim to develop neurotypical communication skills in neurodivergent individuals. Instead, we promote cross-neurotype collaboration without presuming a single mode of ""correct"" communication. Furthermore, we surface the potential of CADMIs by linking this notion to a pluralization of agency that extends beyond one-to-one body-sensor relationships. We develop accessible instruments within neurodiversity and autism contexts, avoiding reification of mindbody relations and recognizing them as dynamic, field-like, and embedded in facilitative relations for these communities.",nime2025_78.pdf,,,78.jpg,,,
138,paper,oral,long,papers-1,Collective and Embodied,15,in person,Visually-Led Design for Gestural Audiovisual Instruments,Sam Trolland; Alon Ilsar; Jon McCormack,"In this paper we present our visually-led design method for creating gestural mappings in a new audiovisual percussion work titled Cymbalism. Unlike most audiovisual works, Cymbalism was inspired by the creation of a series of interactive visual scenes that respond to the performer’s real-time movements. In leading with the visual interaction, we discuss how this approach fostered a union between the physical, audio and visual elements of the work, creating a performance where the visualisation is not simply a feedback mechanism but fundamental in inspiring compositional concepts and new ways of interacting with sound. Through practice-based research, we use the insights gained through creative development and performance outcomes to guide the continued evolution of an established wearable gestural DMI.",nime2025_138.pdf,,,138.jpg,,,
279,paper,oral,medium,papers-1,Collective and Embodied,10,remote,Hyperwilding: Sonic Perplexity as Urban Acupuncture to Promote Environmental Kinship,Shomit Barua,"This paper invites discussion of how sound art installations, specifically those situated in urban environments, can serve as respite from urban stressors as well as advocate for increased awareness and engagement of acoustic ecology. The author invokes the theoretical framework of Karen Barad to juxtapose the Urban Acupuncture movement with the Solarpunk ethos, arguing that sound installations may be crafted as agential cuts to the entangled relationship of humans and their built environments. This paper surveys sound artists that have specifically engaged the urban space—an environment that one could argue is more “natural” to humans than the remote picturesque landscapes commonly associated with the concept. Finally, the author describes some of his past sonic interventions and expounds on his current project, “Standing Wave,” commissioned by the city government and non-profits to address Extreme Urban Heat. He discusses how this installation, coupled with targeted community engagement through “Environmental Listening” workshops, urges us to rethink the temporality of intervention, recognizing that long-term strategies, while not immediate solutions, are crucial for future cooling and remediating the effects of climate change.",nime2025_279.pdf,,,279.jpg,,,
323,paper,oral,long,papers-1,Collective and Embodied,15,in person,Drawing Space with Rain: The Umbrella as a Flow Interface,Kana Yamaguchi; Yuga Tsukuda; Yoichi Ochiai,"This study explores new possibilities for transforming perceived space by using an umbrella as a dynamic spatial auditory interface. While spatial audio technologies have been widely applied across various domains, there are few opportunities in daily life to consciously perceive the boundary between personal and external space. Due to its physical structure and everyday usage, the umbrella has a unique ability to render such boundaries perceptible. Focusing on the “flow” of raindrops across the umbrella’s surface, the system detects continuous rain movement in real time rather than merely capturing impact sounds. Spatial auditory feedback encourages users to actively perceive dynamic spatial boundaries, as the rain draws auditory contours through interaction with the umbrella. To this end, the umbrella is conceptualized as an interactive interface that senses raindrop movement and applies spatial audio processing. In addition, users can dynamically alter the virtual size of the umbrella, enabling perceptual shifts in spatial scale. Rather than treating the umbrella as a mere protective object, this system reimagines it as a medium for perceiving environmental change through sound. By integrating natural phenomena with spatial audio, this approach suggests new directions for embodied perception and expression.",nime2025_323.pdf,,,323.jpg,,,
50,paper,oral,long,papers-2,Novel Techniques and Technologies,15,in person,Drum Modal Feedback: Concept Design of an Augmented Percussion Instrument,Lewis Wolstanholme; Jordie Shier; Rodrigo Constanzo; Andrew McPherson,"We here outline the concept design of an augmented percussion instrument, conceived for and used as part of a variety of distinct performances and compositions. Throughout the curation of this project, each creative act has enabled us to contextualise, examine and reflect upon the design of this augmented instrument. In accordance with Stolterman and Wiberg’s concept driven design methodology, we do not present a singular instrument design, but instead an overarching design concept alongside its developmental and evaluative narrative. This augmentation centres upon the use of a drum trigger and a tactile transducer, which when coupled together can be used to feedback or resonate a drum. The resultant soundworld develops upon the idiomatic sonority of a drum, and allows for the duration and timbre of a drum strike to be continuously manipulated and shaped throughout a performance. In exploring the soundworld which results from this approach, we have experimented with numerous configurations of these pieces of hardware, and have also employed various pieces of software to parametrise the sonic subtleties that this approach engenders. Most prominently, we have developed a bespoke piece of software which analyses the modes of a drum prior to performance, and uses this modal analysis to shape the overall feedback and resonance. Throughout this design process, we have consistently been met with new creative criteria that challenge our approach and ideas, in response to the particularities of the musicians we are working alongside, as well as the performative and aesthetic environments we are working within.",nime2025_50.pdf,,,50.jpg,,,
115,paper,oral,medium,papers-2,Novel Techniques and Technologies,10,in person,"Evolving the Living Looper: Artistic Research, Online Learning, and Tentacle Pendula",Victor Shepardson; Halla Steinunn Stefánsdóttir; Thor Magnusson,"The Living Looper is a neural audio synthesis looper system for live input. It combines online learning with pre-trained neural network models to resynthesize incoming audio into ""living loops"" that transform over time. This paper describes new features of the Living Looper and musician perspectives on its use. A new graphical interface facilitates use of the instrument by non-programmers and visualizes each loop to aid performers in tracking which loop is making which sound.  We also describe a new living loop algorithm including incremental learning with partial least squares regression. Finally, we report on an artistic project using the Looper and lessons learned, resulting in an increased importance of training data and a developing sense of relationality.",nime2025_115.pdf,,,115.jpg,,,
147,paper,oral,long,papers-2,Novel Techniques and Technologies,15,in person,Audiation Development System for Gugak's Fluid Musical Parameters Utilizing Audio Feedback Stimuli,Michaella Moon; Dale Carnegie; Jim Murphy,"Gugak, the traditional music of Korea, is defined by its distinctive musical characteristics, including flexible tuning, non-metric rhythms, and intricate ornamentation. These unique features, while artistically rich, pose significant challenges for novice learners, particularly when approached through conventional, textbook-based methods. To bridge this gap, we introduce the Audiation Development System for Gugak, an interactive platform that leverages algorithmic analysis to support both learning and teaching. Central to this system is the development of signal processing functions for rhythmic guidance, pitch detection, and additive harmonic synthesis—algorithms specifically designed to capture the expressive nuances of Gugak. These functions generate real-time auditory and visual feedback, providing a responsive learning environment aligned with the updated Korean music curriculum. The system not only enables student-centred exploration of Gugak's fluid structures but also supports educators through dynamic, visualized feedback tools. Beyond its technical foundation, this research sets the stage for future development of user interfaces and investigates the educational efficacy of computer-driven learning compared to traditional methods. By integrating music technology and pedagogy, this work contributes to both the accessibility and sustainability of Korea’s musical heritage.",nime2025_147.pdf,,,147.jpg,,,
161,paper,oral,short,papers-2,Novel Techniques and Technologies,5,in person,Threading the Sound: The Carpet Tufting Gun as an Electroacoustic Performance Interface,Joseph Burgess; Toby Gifford,"This paper explores the carpet tufting gun as a novel electroacoustic performance interface. Leveraging its distinctive acoustic properties and electromechanical kinetics, the tufting gun presents a range of physical affordances that can be creatively repurposed for musical expression. While prior intersections between textile production processes and musical practices exist, the tufting gun remains largely underexplored as a tool for structured musical composition. This work reimagines the gun’s mechanical gestures and performative affordances, transforming its utilitarian motions into expressive sonic gestures. By positioning the tufting gun as both an acoustic source and an interactive performance interface, this project works at the intersection of fibre craft and experimental sound art, where both historico-cultural context of textile making, and the ergonomics of the gun, present musical affordances.",nime2025_161.pdf,,,161.jpg,,,
198,paper,oral,long,papers-2,Novel Techniques and Technologies,15,in person,Designing Percussive Timbre Remappings: Negotiating Audio Representations and Evolving Parameter Spaces,Jordan Shier; Rodrigo Constanzo; Charalampos Saitis; Andrew Robertson; Andrew McPherson,"Timbre remapping is an approach to audio-to-synthesizer parameter mapping that aims to transfer timbral expressions from a source instrument onto synthesizer controls. This process is complicated by the ill-defined nature of timbre and the complex relationship between synthesizer parameters and their sonic output. In this work, we focus on real-time timbre remapping with percussion instruments, combining technical development with practice-based methods to address these challenges. As a technical contribution, we introduce a genetic algorithm -- applicable to black-box synthesizers including VSTs and modular synthesizers -- to generate datasets of synthesizer presets that vary according to target timbres. Additionally, we propose a neural network-based approach to predict control features from short onset windows, enabling low-latency performance and feature-based control. Our technical development is grounded in musical practice, demonstrating how iterative and collaborative processes can yield insights into open-ended challenges in DMI design. Experiments on various audio representations uncover meaningful insights into timbre remapping by coupling data-driven design with practice-based reflection. This work is accompanied by an annotated portfolio, presenting a series of musical performances and experiments with reflections.",nime2025_198.pdf,,,198.jpg,,,
11,paper,oral,long,papers-3,Body and Motion,15,remote,Simulated EEG-Driven Audio Information Mapping Using Inner Hair-Cell Model and Spiking Neural Network,Pasquale Mainolfi,"This study presents a framework for mapping audio information into simulated neural signals and dynamic control maps. The system is based on a biologically-inspired architecture that traces the sound pathway from the cochlea to the auditory cortex. The system transforms acoustic features into neural representations by integrating Meddis's Inner Hair-Cell (IHC) model with spiking neural networks (SNN). The mapping process occurs in three phases: the IHC model converts sound waves into neural impulses, simulating hair cell mechano-electrical transduction. These impulses are then encoded into spatio-temporal patterns through an Izhikevich-based neural network, where spike-timing-dependent plasticity (STDP) mechanisms enable the emergence of activation structures reflecting the acoustic information's complexity. Finally, these patterns are mapped into both EEG-like signals and continuous control maps for real-time interactive performance control. This approach bridges neural dynamics and signal processing, offering a new paradigm for sound information representation. The generated control maps provide a natural interface between acoustic and parametric domains, enabling applications from generative sound design to adaptive performance control, where neuromorphological sound translation explores new forms of audio-driven interaction.",nime2025_11.pdf,,,11.jpg,,,
107,paper,oral,short,papers-3,Body and Motion,5,remote,EMMA: Enhancing Real-Time Musical Expression through Electromyographic Control,João  Coimbra; Luís Aly; Henrique  Portovedo; Sara Carvalho; Tiago  Bolaños,"This paper presents the Electromyographic Music Avatar (EMMA), a digital musical instrument (DMI) designed to enhance real-time sound-based composition through gestural control. Developed as part of a doctoral research project, EMMA combines electromyography (EMG) and motion sensors to capture nuanced finger, hand, and arm movements, treating each finger as an independent instrument. This approach bridges embodied performance with computational sound generation, enabling expressive and intuitive interaction. The system features a glove-based design with EMG sensors for each finger and motion detection for the wrist and arm, allowing seamless control of musical parameters. By addressing key challenges in DMI design, such as action-sound immediacy and performer-instrument dynamics, EMMA contributes to developing expressive and adaptable tools for contemporary music-making.",nime2025_107.pdf,,,107.jpg,,,
129,paper,oral,long,papers-3,Body and Motion,15,in person,Aquapella- Gestural Interactions with Liquid Turbulence as Musical Expression,John Lettang; August Black,"The Aquapella is a hand-held gestural instrument for exploring the unique relationship of liquid turbulence and musical expression. The device consists of eight conductive water-level sensors in a custom 3D printed container. As a musician moves the device, it generates a chaotic flow of water within the container and translates the motion into real-time midi signals for audio-visual interpretation. In our initial performances and tests with the Aquapella, we have focused on turning the flowing characteristics of the device into ambient and glitch soundscapes that move between noise and harmonics. We present the primary findings in developing the Aquapella including related works, description of the project development, and ideas for future iterations.",nime2025_129.pdf,,,129.jpg,,,
229,paper,oral,medium,papers-3,Body and Motion,10,remote,From Performance to Installation: How Interactive Reinforcement Learning Reframes the Roles of Performers and Audiences,Danny Perreault; Victor Drouin-Trempe; Vincent Cusson; David Drouin; Sofian Audry;,"This paper explores how interactive reinforcement learning (IRL) reconfigures the roles of performers and audiences in audiovisual performance and immersive installation. We adapt the Co-Explorer (a software tool originally developed for musical co-creation) to audiovisual immersive contexts and examine its creative potential using a reflexive research-creation approach. Our study reveals how IRL splits the role of the performer into three distinct positions: (1) the designer, who defines the parametric space; (2) the guide, who reinforces the agent’s behavior; and (3) the machine performer, whose actions are shaped by interactive training. As IRL introduces agency into the creative process, it transforms traditional notions of authorship and control, enabling unexpected emergent outcomes. By showcasing an interactive installation/performance, we further explore how audiences contribute to collective creation through reinforcement-based interaction. Our findings underscore the challenges of balancing the temporality of IRL with the demands of public-facing works and of adapting RL-based systems to different exhibition contexts. Our work contributes to the discourse on co-creative systems, emphasizing the evolving roles of artists, artificial agents, and audiences in hybrid creative ecosystems.",nime2025_229.pdf,,,229.jpg,,,
10,paper,oral,long,papers-4,"Environment, Sustainability, Longevity",15,in person,Looping slowly: Diffraction through the lens of nostalgia,Benedict Gaster; Nathan Renney; Jasmine Butt,"This paper concerns magnetic tape and the nostalgia of media, finding new relevance in old technology, remaking and adapting practices to fit within a modern workflow. Pushing against the driving force of economic structures, which emphasises a continuous cycle of replacement, musicians and instrument designers are drawing on a shared history to create new pieces of art and machines. This can be read as reflecting NIME's Code of Practice and, more generally, the unfolding climate crisis. For some, NIME may convey a focus on new musical instruments, but here, we focus on the notion of new through the diffracted lens of the old. Defined in recent NIME conferences by zooming in on the 'O' in NIME through the importance of reusing and repurposing old musical instruments and, in our case, old practices and processes. This paper considers magnetic tape and the machines that process it as the material and instrument. Following a survey, we present a diffracted reading through an intra-related process of how musicians, producers, and others who work with audio integrate tape into their practice. Drawing on post-humanist theories, we explore how slowness, community, and the old can inform NIME as a methodology. It provides insight for NIME to continue moving forward while focusing, through its Code of Practice, on sustainability, connection with our past, our history, years of artistic practice, and workflows that are not simply optimised for efficiency or the new.",nime2025_10.pdf,,,10.jpg,,,
90,paper,oral,medium,papers-4,"Environment, Sustainability, Longevity",10,Does not know yet,Longevity of Deep Generative Models in NIME: Challenges and Practices for Reactivation,Isaac Clarke; Francesco Ardan  Dal Rí; Raul Masu,"In this paper, we present an investigation into the longevity, reproducibility, and documentation quality of Deep Generative Models (DGMs) introduced in previous editions of the NIME conference. We begin by assessing whether DGM presented at NIME are still available in terms of code, data, and weights; afterward, we present the recreation process of seven unavailable models, to the end of investigation of the issues related to longevity and documentation. We examine the availability and completeness of resources needed to recreate DGM models, and discuss specific challenges encountered during such recreation. Drawing from  this experience, we highlight key obstacles that hinder the long-term viability and reuse of DGMs in the NIME context, and propose guidelines to improve their documentation and future reuse within the community.",nime2025_90.pdf,,,90.jpg,,,
96,paper,oral,long,papers-4,"Environment, Sustainability, Longevity",15,in person,Embodying Sustainability: Paving Opportunities for NIME Research,Xinran Chen; Iurii Kuzmin; Mela Bettega; Raul Masu,"While sustainability has gained attention in NIME research, primarily focusing on instrument longevity and durability, the role musical interfaces play in promoting environmental awareness remains unexplored. This paper investigates how musical interfaces can foster sustainability through designing embodied experiences. We present a literature review that examines the integration of sustainability and embodiment in sonic interaction, in which we synthesize practical points on how sound, materials, data, and interactions can aesthetically support embodying sustainability. We further explore these concepts through a design case study. Our findings suggest that embodied musical experiences offer unique opportunities to cultivate environmental consciousness, contributing to a deeper understanding of sustainable musical interfaces relying on artistic expressions.",nime2025_96.pdf,,,96.jpg,,,
153,paper,oral,short,papers-4,"Environment, Sustainability, Longevity",5,in person,Ongoing Production of a “Growing Instrument” Using Mycelium-Based Materials,Taisei GOTO; Kazuhiro JO,"This study explores “growing instruments” made from fungal mycelium, highlighting their natural unpredictability and role as musical instruments. Mycelium’s growth and interactions with the environment create unique features not found in traditional instruments. Positioned within non-human-centric approaches in design and art, the research emphasizes the mutual creation and interconnectedness of diverse actors. By shaping mycelium into tubes and adding recorder heads, playable flute-like instruments were created. However, their condition and playability were highly influenced by conditions such as temperature and humidity. The study emphasizes embracing uncertainty and suggests that these imperfections can offer new insights into musical instrument design.",nime2025_153.pdf,,,153.jpg,,,
39,paper,oral,long,papers-5,Machine Learning and Co-Creativity,15,in person,"Synthetic Ornithology: Machine learning, simulations and hyper-real soundscapes",Frederick Rodrigues,"This paper presents Synthetic Ornithology, an interactive sound-based installation that uses machine learning to simulate sonic representations of localised Australian ecological futures, extending work in soundscape composition to engage in a speculative domain. Central to Synthetic Ornithology is a bespoke ML model, Environmental Audio Generation for Localised Ecologies (EAGLE), capable of generating high-quality, birdsong-focused soundscapes, up to 23 seconds in length. This paper outlines the development of the installation and how its design aims to influence audience perception of the sonic content of the work, extending established practices in NIME and sonic arts to a parafictional approach, and hyperreal aesthetics. Additionally, the paper examines the design and capabilities of the EAGLE model, and reflecting on how generative tools are positioned within a creative context, re-imagines the technical processes of training and configuring ML models as sites of artistic authorship in an expanded creative audio practice.",nime2025_39.pdf,,,39.jpg,,,
131,paper,oral,long,papers-5,Machine Learning and Co-Creativity,15,in person,Touching Wires: tactility and a quilted musical interface for human-AI musical co-creation,Sandy Ma; Charles Patrick Martin,"Interactions with computers have traditionally been mediated by rigid materials, but as technology evolves, there is increasing potential to rethink these relationships. This paper explores how a soft, textile-based interface can reshape human-AI interaction, particularly in musical co-creation. We introduce a textile-based human-AI system used both for musical performance and public interaction. This system enables embodied, tactile engagement with an AI agent, offering users a more unique and participatory experience in human-AI musical co-creation. We aim to examine the potential for soft materiality to mediate more dynamic human-AI interactions. Our findings reveal that users’ choices when interacting with novel systems are informed by their expectations and biases, that embodied learning is built iteratively on layered multi-sensory experiences, and that there is a desire for familiarity and understanding when interacting with AI systems. We found that the materiality of our textile human-AI interfaces influenced how users choose to interact, and that users sought clarity in the AI’s role in collaborative creation. This work contributes to our understanding of how entanglement, embodiment, and materiality impact our relationships in human-AI collaborations.",nime2025_131.pdf,,,131.jpg,,,
160,paper,oral,long,papers-5,Machine Learning and Co-Creativity,15,in person,Adaptation and Perceived Creative Autonomy in Gesture-Controlled Interactive Music,Jason Smith; Jason Freeman,"With the variety and rapid pace of developments in Artificial Intelligence (AI), musicians can face difficulty when working with AI-based interfaces for musical expression as understanding and adaptation to AI behaviors takes time. In this paper, we explore the use of AI in an interactive music system designed to adapt to users as they learn to perform with it. We present GestAlt, an AI-based interactive music system that collaborates with a performer by analyzing their gestures and motion to generate audio changes. It uses computer vision, online machine learning, and reinforcement learning to adapt to a user's hand motion patterns and allow a user to communicate their musical goals to the system. It communicates its decision-making to the user through visualizations and its musical output. We conducted a study in which five musicians performed using this software over multiple sessions. Participants discussed how their preferences for the system’s behavior were influenced by their experiences as musicians, how adaptive reinforcement learning affected their expectations for the system’s autonomy, and how their perceptions of the system as a creatively autonomous, collaborative partner evolved as they learned how to perform with the system.",nime2025_160.pdf,,,160.jpg,,,
222,paper,oral,medium,papers-5,Machine Learning and Co-Creativity,10,in person,Gesture-Driven DDSP Synthesis for Digitizing the Chinese Erhu,Wenqi WU; Hanyu QU,"This paper presents a gesture-controlled digital Erhu system that merges traditional Chinese instrumental techniques with contemporary machine learning and interactive technologies. By leveraging the Erhu’s expressive techniques, we develop a dual-hand spatial interaction framework using real-time gesture tracking. Hand movement data is mapped to sound synthesis parameters to control pitch, timbre, and dynamics, while a differentiable digital signal processing (DDSP) model, trained on a custom Erhu dataset, transforms basic waveforms into authentic timbre which remians sincere to  the instrument’s nuanced articulations. The system bridges traditional musical aesthetics with digital interactivity, emulating Erhu bowing dynamics and expressive techniques through embodied interaction. The study contributes a novel framework for digitizing Erhu performance practices, explores methods to align culturally informed gestures with DDSP-based synthesis, and offers insights into preserving traditional instruments within digital music interfaces.",nime2025_222.pdf,,,222.jpg,,,
230,paper,oral,medium,papers-5,Machine Learning and Co-Creativity,10,in person,Enabling Embodied Music-Making for Non-Musicians,Lucía Montesínos; Halfdan Hauch Jensen; Anders Løvlie,"We present a Research through Design exploration of the potential for using tangible and embodied interactions to enable active music experiences - musicking - for non-musicians. We present the Tubularium prototype, which aims to facilitate music-making to non-musicians by not requiring any initial skill while still eliciting agency and overall, providing a meaningful experience. We present the design of the prototype and the features implemented and reflect on insights from a public event in which the prototype was trialed.",nime2025_230.pdf,,,230.jpg,,,
37,paper,oral,long,papers-6,Accessibility,15,remote,Chimera: Prototyping a New DMI for Congenital One-Handed Musicianship Through an Autoethnographic Lens,Mat Dalgleish,"Chimera is a Digital Musical Instrument (DMI) prototype developed through an autoethnographic lens. That is, a lens shaped by congenital one-handedness as well as extensive experience as both a disabled player of standard instruments and a designer of DMIs for other players. Leveraging Eurorack synthesizer modules as a flexible prototyping toolkit enables an iterative prototyping process that explores the distinctive possibilities of one-handed musicianship. Reflection on a three-month period of iteration and refinement highlights a series of design issues, but also the interconnectedness of physical impairments, and the difficulties of designing for a body in flux. Some directions for future work are outlined. Finally, by discussing the various entangled layers of this instrument prototype, and starting to tease out what Koutsomichalis calls its “stories of a sort”, this paper contributes an until now underrepresented perspective to the dialogue around accessible and inclusive musical instrument design, and disability and musicianship more broadly.",nime2025_37.pdf,,,37.jpg,,,
88,paper,oral,medium,papers-6,Accessibility,10,remote,The Slide-A-Phone: a Tactile Accessible Musical Instrument,Andrew McMillan; Fabio Morreale,"AMIs, Inclusive design, autobiographical design This paper details the design and development of the Slide-A-Phone, an Accessible Musical Instrument (AMI). The first author's spinal cord injury in 2004 hindered their ability to play traditional instruments, which motivated the development of the Slide-A-Phone. The Slide-A-Phone utilises tactile interfaces coupled with analogue and digital sensors to replicate the playability and expressive control of a saxophone, the instrument the first author used to play before the incident. The design process incorporated phenomenological perspectives and a blend of design methodologies, with the specific goal of fostering a robust musician-instrument relationship. We report insights into how personal experiences shape design and functionality and the importance of accessible instruments in enabling creative practice and performance for individuals with limited functionality. We also describe the design and technical implementation of the Slide-A-Phone evaluate the instrument's effectiveness and reflect on its potential to enhance musical engagement, social connections, cultural participation, and professional development.",nime2025_88.pdf,,,88.jpg,,,
187,paper,oral,medium,papers-6,Accessibility,10,in person,Designing Sensory NIME for Autism,Aditya Arora; Erica Tandori; James Marshall; Stuart Favilla,"This paper explores how sensory NIME design principles may inform the design of musical interfaces tailored for children with Autism Spectrum Disorder (ASD), focusing on their sensory processing challenges. Given the prevalence of sensory over-responsivity (SOR) and under-responsivity (SUR) in ASD, traditional sensory interventions often fail to accommodate the highly individualized and fluctuating sensory needs of autistic individuals. The authors highlight the potential for multisensory NIME to address the diverse range of sensory needs, promoting emotional regulation and sensory balance through new creative musical opportunities and activities. This paper presents research in the form of a narrative review and comparative case study of recent NIME and sensory intervention research, exploring emerging approaches, rhythm-based interventions, generative algorithms, play-centered designs and other possibilities for enhancing sensory engagement and emotional regulation. Drawing on insights from 30 recent NIME papers, this research explores the boundaries of current approaches and seeks to establish an understanding of multisensory NIME for ASD. The research underscores the profound variability in sensory profiles for ASD, necessitating a shift from clinician-directed interventions to creative, inclusive, multisensory solutions. Finally, a set of sensory NIME design principles are offered, emphasizing the importance of sensory perception, sensory equilibrium and the promotion of emotional regulation for ASD.",nime2025_187.pdf,,,187.jpg,,,
253,paper,oral,medium,papers-6,Accessibility,10,in person,The Sound Tree Project: Developing Personal and Collective Expression with Accessible Digital Musical Instruments,Steph OHara; Alon Ilsar,"The Sound Tree Project investigates how accessible digital musical instruments (ADMIs) can champion both personal and collective musical expression. Through a sustained six-month ethnographic engagement with five performers and two support artists, we explored how to create personalised instruments for a public per- formance outcome. The technical framework combined multiple wireless motion sensor devices placed inside different objects and the development of a real-time movement-to-sound pro- cessing hub within a live coding environment. The performance was centred on an accessible sound sculpture, the Sound Tree, where digital instruments coexisted with traditional sound mak- ing objects. Drawing from our shared process of experimentation, improvisation, and personalised instrument creation, we present some key ‘magic moments’ that were woven into the final perfor- mance and discuss how they might serve as evidence of personal expression and validation of the design process. The emergence of these moments demonstrate the value of real-time system adaptation in encouraging individual expression, the importance of sustained engagement in developing personalised instruments and having effective strategies for balancing personal and collec- tive music-making. These insights have implications in developing accessible mu- sic technology and broader approaches to designing technologies that support diverse forms of creative collaboration.",nime2025_253.pdf,,,253.jpg,,,
21,paper,oral,long,papers-7,Entangled NIME,15,remote,Entangling with Light and Shadow: layers of interaction with the pattern organ,Jasmine Butt; Benedict Gaster; Nathan Renney; Maisie Palmer,"This paper explores the design and use of a camera-based digital musical instrument as a thinking tool for considering entangled, post-human perspectives. The design of the pattern organ, inspired by experimental optical sound-on-film practices, employs a method of visual-to-audio synthesis that responds closely to the material behaviours captured by its camera input. Drawing on findings from exploratory workshops and short material experiments, we describe how interactions emerge and are shaped by both the physical configuration of the instrument and the material behaviours captured by its camera. We consider how frugal mappings and the ‘rawness’ of data can give rise to instruments whose inputs remain open to material complexity, extending the sound engine beyond their enclosures. In the case of the pattern organ, this complexity emerges through overlapping and interfering interactions, where structural forms, human influence, light, shadows, lens distortions, and system quirks all contribute to the shifting harmonic content of the wavetable. We reflect on the instrument as a fluid assemblage, composed of human and non-human entanglements, encouraging us to think beyond traditional notions of human-centred control.",nime2025_21.pdf,,,21.jpg,,,
75,paper,oral,long,papers-7,Entangled NIME,15,remote,(De)Constructing Timbre at NIME: Reflecting on Technology and Aesthetic Entanglements in Instrument Design,Charalampos Saitis; Courtney N. Reed; Ashley Laurent Noel-Hirst; Giacomo Lepri; Andrew McPherson,"Timbre, pitch, and timing are often relevant in digital musical instrument (DMI) design. Compared with the latter two, timbre is neither easy to define nor discretise when negotiating audio representations and gesture-sound mappings. We conduct a corpus assisted discourse analysis of ``timbre'' in all NIME proceedings to date (2001--2024). Combining this with a detailed review of 18 timbre-focused papers at NIME, we examine how definitions of timbre and timbre interaction methods are constructed through, for instance, Wessel's numerical timbre control space, synthesis tools and programming languages, machine learning and AI approaches, and other trends in digital lutherie practices. While acknowledging the practical utility of technical constructions of timbre in NIME (and other digital music research communities), we contribute discussion on the entanglement of technology and aesthetics in instrument design, which constitutes what ``timbre'' becomes in NIME research and reflect on the tension between technoscientific and constructivist understandings of timbre: how DMIs and musical practices have been reconstituted around particular timbral values operationalised in NIME. In response, we propose ways that the NIME community can embrace more critical approaches and awareness to how our methods and tools shape and co-create our notions of timbre, as well as other musical concepts, connecting more openly with diverse types of sonic phenomena.",nime2025_75.pdf,,,75.jpg,,,
295,paper,oral,long,papers-7,Entangled NIME,15,Does not know yet,Negotiating Entanglements in the Composition and Curation of an Ultrasonic Art Installation,Nicole Robson; Andrew  McPherson; Nick  Bryan-Kinns,"This paper explores the entangled activities of composing and curating the sound installation 'Sonographies' at a contemporary art gallery. The installation extends our work with an ultrasonic technology that sonifies and magnifies the physical entanglement of a listener with a spatial sound field to produce rich movement-sound interaction without the use of sensors. Taking a research through practice approach, we examine the process of creating 'Sonographies' while deliberately allowing the nonhuman influences of site and technology to inform creative ideation and decision-making. We propose that an attunement to entanglement foregrounds the co-production of aesthetic qualities by the entire musical assemblage and fosters a sensitivity to fragile and changeable qualities of NIMEs, contingent on specific technical, material and social situations.",nime2025_295.pdf,,,295.jpg,,,
74,paper,oral,medium,papers-8,Historical and Cultural Reflections,10,Does not know yet,"Hacking Sound, Hacking History: Patricia Cadavid and the Electronic_Khipu_",Margaret Needham,"To better understand researcher and artist Patricia Cadavid Hinojosa’s instrument the Electronic\_Khipu\_, we must define the project as an instance of hacking. Cadavid deconstructs colonial understandings of the Andean device known as the khipu, pulling apart the academic view of khipus as artifacts to be deciphered, the strict delineation between administrative and ritualistic uses of the khipu, and the separation of the oral tradition from the object. Through deliberate design choices and musical expression in performance, Cadavid emphasizes the inextricability of coding, art, and ritual by creating a tactile device that re-tells history and challenges the false oppositional binary between Indigeneity and technology. Understanding this project of digital lutherie as an act of creation through hacking -- specifically as the deconstruction and reconfigurement of artistic and historical components, utilizing scholars Astrida Neimani's and Vít Bohal's definitions -- allows us to appreciate its power.",nime2025_74.pdf,,,74.jpg,,,
134,paper,oral,long,papers-8,Historical and Cultural Reflections,15,in person,NIME: A Mis-User's Manual,Sally Jane Norman; Paul Stapleton; John Bowers,"Ever since the initiating workshop at the 2001 ACM CHI'01 Conference, annual New Interfaces for Musical Expression conferences have seen a proliferation of work featuring different forms of music, research values, philosophical, ethical and political standpoints. The 2025 ‘Entangled’ theme celebrates this diversity of creative, technical, and social ‘intelligencings’ (Thrift [68, p153-154]). It is precisely the non- or pluri-paradigmatic character of NIME that is its strength. Drawing on Maria Lugones [41], we characterise NIME less as an entangled weave—where threads maintain their separate yet assembled and interconnected character—than as a ‘curdling’ where relationships are more complex, varied, mutually interrupting and shaping, indeterminate and unknown without careful dialogue. We do not consider it appropriate to offer unifying frameworks or mappings with often hidden authoritarian implications. Rather, following Rancière [5], we prefer a radically democratic dissensus and, following Lugones, a spirit of ‘festive resistance’ where we poke at the limits of our inherited metaphors to undermine attempts to provide a fixed orderliness, (re)framing topics to kickstart exchange on new fertile grounds for collaboration. Multiple kinds and collisions of agency, and the lively openness of what some might deem ‘failure’ are prioritised over the often inhibiting closure and certainty of ‘success’ [e.g. 10, 11, 12, 40]. Our topics include: multiple ways of making as a means of maximising exposure to possible failure; shifting from interfaces to interfacing to create arenas for action rather than tools for purposes; foregrounding risk, inefficiency and forgetting; formulating improvisation as knowing-when and composing-the-now; performance practice, settings and contingencies; alternative resourcings/reframings for research; a wild spirit of tactical oppositionalism, dynamic uncompromise, and existential pluralism, to embrace the independence of divergent voices.",nime2025_134.pdf,,,134.jpg,,,
137,paper,oral,long,papers-8,Historical and Cultural Reflections,15,remote,The Imperfect Copy: Role Playing Reenactments of Historical Electronic Sound Instruments,Derek Holzer; Henrik Frisk; André Holzapfel,"Reenactment forms a unique method of exploring the social, political, historical, conceptual, contextual and other aspects of electronic sound instruments from the past, without necessarily reproducing the instrument’s physical, functional or sonic characteristics. Rather, the reenactment presents a novel instrument, realized through contemporary means, reflecting on contemporary concerns and within a contemporary context. We find reenactment complementary to conservation, maintenance, reconstruction and emulation in working with archival and museum objects. Our paper presents an analytic framework developed for use in workshop scenarios. This series of questions helps determine and understand which aspects of an instrument might be reenacted. To illustrate the process in action, we describe an example workshop wherein participants use methods of media archaeology, design fiction and role playing to imagine and reenact new features, affordances, contexts and applications of electronic instruments from a museum exhibition.",nime2025_137.pdf,,,137.jpg,"Pär Fredin / The Swedish Museum of Performing Arts, used with
permission.",,
195,paper,oral,medium,papers-8,Historical and Cultural Reflections,10,remote,The Memory Cloud: Personal media libraries as affordance and constraint,Yann Seznec,"The Memory Cloud is a musical instrument that uses a player’s own library of personal recordings as sonic material. This paper presents the design of the instrument, situating it within sustainability HCI studies and constraints-based design, before describing the instrument being used by two musicians in a professional context. Over 2000 sounds from the musician's personal cloud library, dating back over 10 years, were placed in the instrument as the only sonic material available for exploring. I argue that a radically small scale and personal approach could be one strategy for addressing the issues of longevity in NIME, and I suggest that using personal media libraries presents a potential affordance and constraint for musical instrument design.",nime2025_195.pdf,,,195.jpg,,,
213,paper,oral,long,papers-8,Historical and Cultural Reflections,15,in person,O一: An Epistemic DMI for Cross-Cultural Reflection on Time and Music,Hanyu Qu; Francesco Dal Rí; Hao Zou; Hanqing Zhou; Raul Masu,"This paper introduces a Digital Musical Instrument (DMI) that inscribes a linear and a circular conception of time, inspired by Western and Eastern time philosophies. The DMI employs two 3D-printed boards equipped with ESP32 chips for wireless communication and WS2812 LEDs providing visual representation feedback, and interactive boxes, each fitted with a light sensor and ESP32 Mini boards. Such interface is designed to be coupled with a software counterpart for sound generation.The project originates from a collaboration with two composers from diverse cultural backgrounds - one Chinese and one Italian. Through collaborative design and co-composing practice, the proposed DMI emerged as an epistemic tool, promoting cultural understanding and critically highlighting the socio-cultural role of technology. Through such process, the significance of rediscovering time in contemporary globalization and philosophy was explored, challenging the conception of time as a mere measurement parameter and striving to reveal the importance of understanding the role of time across different cultural contexts. This project wishes to expand the constitutive role of musical time, demonstrating its diversity and prompting a reflective layer of the perception of performative and musical time in NIME.",nime2025_213.pdf,,,213.jpg,,,
58,paper,oral,long,papers-9,Extended Reality,15,in person,Collaborative Musical Expression Through Interactive VR Scores,David Kim-Boyle,"While the technical affordances of virtual reality (VR) have pro- vided new ways for artists to aestheticize immersion, spectator agency, embodiment and multi-sensory engagement, they have also opened new possibilities for composers interested in ex- ploring how interactive musical scores might become a means through which collaboration itself becomes the locus of aesthetic expression. In this paper, the author will provide an overview of an ongoing project which explores new ways of thinking about musical collaboration in VR through the 3D visualization of interactive, graphic scores adapted from works by composers Earle Brown, Christian Wolff, and Toru Takemitsu. The research demonstrates how VR can transform traditional score interpreta- tion by creating dynamic, interactive environments that enable collaborative musical expression, challenge conventional nota- tion, and offer novel ways of negotiating musical performance through networked, multi-user interactions.",nime2025_58.pdf,,,58.jpg,,,
59,paper,oral,long,papers-9,Extended Reality,15,in person,SVOrk: Stanford Virtual Reality Orchestra,Kunwoo Kim; Andrew Zhu; Eito Murakami; Marise van Zyl; Yikai Li; Max Jardetzky; Ge Wang,"This paper chronicles the creation of the Stanford Virtual Reality Orchestra (SVOrk), a new computer music ensemble where both performers and audience engage in a shared, fully immersive virtual reality (VR) chamber-esque concert experience. Motivated to explore group-based live performance within VR, SVOrk has designed and crafted virtual musical interfaces, fantastical 3D-modeled environments, and a network infrastructure to support real-time shared participation. Inherent within this initiative is a reimagining of conventional concert experiences, introducing virtual lobbies, customizable avatars, and immersive audience interactions. These experimental features explore new forms of social presence, audience identities, and expressive communication to help address the overarching question, “What does it mean to participate in a VR musical performance?” SVOrk’s premiere concert took place in June 2024 with five performers and approximately 60 audience members (across five sessions), featuring a program of six musical works. This paper describes the motivations behind SVOrk, its research and development process—including designs for networking, avatar, and audience interaction—and takeaways from the premiere concert. We also present audience feedback and reflect on our experiences in curating group VR performances.",nime2025_59.pdf,,,59.jpg,,,
232,paper,oral,long,papers-9,Extended Reality,15,remote,AR Matchmaking: The Compatibility of Musical Instruments with an AR Interface,Hyunkyung Shin; Henrik von Coler,"Augmented Reality (AR) interfaces offer new possibilities for musical expression by extending the capabilities of acoustic, electronic, and electroacoustic instruments. This study investigates the usability of the ARCube, an AR-based spatial audio controller, with twelve distinct musical instruments played by experienced musicians. We identify usability challenges specific to certain instruments, particularly for two-handed playing, as well as issues related to gesture recognition and cube stability. Our analysis shows that interaction patterns, such as cube placement, sound effect usage, and gesture strategies, vary significantly between instruments. These differences are driven by the physical form of the instruments, the required playing techniques, and user expectations for control and responsiveness. Based on these insights, we suggest directions for developing adaptable AR interfaces that better accommodate diverse instruments and support broader integration of AR technologies into musical practice.",nime2025_232.pdf,,,232.jpg,,,
266,paper,oral,medium,papers-9,Extended Reality,10,in person,Physical Music Albums in the Digital Era: Exploring Experiential Value Through the Integration of AR,Ena Fumihira; Andrew Johnston,"This study explores physical music albums in the digital age, as well as the creation of new music experiences through the integration of Augmented Reality (AR) into physical albums. An online survey was conducted to examine the differences in experiences between digital and physical albums, and this informed the development of a physical music album incorporating AR. We provided 8 K-POP fans, who engage with physical albums more frequently than fans of other genres, the opportunity to test existing AR-integrated albums and a new prototype featuring AR packaging animations, multiplayer virtual concerts, and interactive photo features. The results underscored the importance of understanding and respecting fan culture when using AR. The results suggest that, compared to digital albums, physical albums derive significant experiential value from traditional supplementary materials such as booklets and lyric cards. However, AR has the potential as a complementary new material that provides users with novel experiences.  This work leads to a reconsideration of Walter Benjamin’s concept of “aura,” which critiques the reproducibility of art.",nime2025_266.pdf,,,266.jpg,,,
277,paper,oral,medium,papers-9,Extended Reality,10,in person,Exploring the impact of spatial awareness on large-scale AR DMIs,Qiance Zhou; Charles Patrick Martin,"Large-scale Digital Musical Instruments (DMIs) offer immersive performance experiences and rich forms of expression, but often pose physical challenges and limit accessibility. Traditional large-scale DMIs' size limits performers' ability to interact with the instrument, causing discomfort when engaging with distant components, highlighting the need for more flexible and user-friendly large-scale DMI designs. We present an Augmented Reality (AR) DMI that removes physical constraints by allowing performers to customise the instrument’s size and layout according to their performance environment. We aim to show how AR-based configuration supports immersive performance, promotes expressive gestures, and improves spatial awareness without sacrificing large-scale instrument capabilities. Our user study revealed increased physical engagement and spatial immersion, a strong sense of ownership, and a positive user experience. These findings indicate that our AR DMI is creatively empowering, reasonably addressing the constraints of large-scale instruments. Our research emphasises the potential of AR to enable flexible and customisable DMI design where interfacems can be adapted to suit the neeeds of individual performers.",nime2025_277.pdf,,,277.jpg,,,
8,paper,poster,short,posters-1,,5,remote,Eco-Sonic Interfaces for Embodied AI Sound Exploration,Sabina Hyoju Ahn; Ryan Millett; Seyeon Park,"Neural Tides is a neural network-based granular synthesizer that examines plastiglomerates—hybrid formations of plastic and organic material in marine environments. The system maps sound grains from oceanic field recordings to a navigable latent space using autoencoders and clustering techniques, controlled via hand gestures and touch. This interface physically connects performers with sonic representations of anthropogenic material transformations in coastal environments.",nime2025_8.pdf,,,8.jpg,,,
13,paper,poster,medium,posters-1,,5,remote,Instant Design: Five Strategies for the use of Generative AI in NIME Ideation Workshops,Hugh Aynsley; Pete Bennett; Dave Meckin; Sven Hollowell; Thomas J. Mitchell,"This paper presents five strategies for facilitating workshops that incorporate AI text-to-image (TTI) generators in the conceptual design of new musical instruments. Developed through a series of iterative workshops, this approach examines the integration of generative AI (GenAI) within creative processes, with a particular focus on idea generation and the interplay between AI-driven tools and traditional craft-based activities in workshop contexts. The primary study was conducted at the Artificial Intelligence and Musical Creativity (AIMC) Conference 2023 and the paper shares insights from the workshop, including the combination of physical prototyping and GenAI concept design through image creation. The paper emphasises the practical implications of incorporating AI tools into group design fiction workshops and offers five suggestions for facilitators and practitioners. It considers the tensions and opportunities that arise in the collaboration between AI and human creativity, underscoring the importance of iterative feedback and the benefits of clearly defined design briefs within speculative design practices.",nime2025_13.pdf,,,13.jpg,,,
19,paper,poster,long,posters-1,,5,remote,A Study to Discover Metrics to Measure Kinesthetic Empathy During Interactive Music Performance,Ryan Ingebritsen; Daniel Evans; Christopher Knowlton,"Kinesthetic empathy is a term used in performance and kinesthetic interaction, defined as the ability of participants to “read, decode and react to each other’s input”.  In prior studies, performers of interactive music self-reported sensing the presence of other musicians. The purpose of this exploratory study was to identify kinesthetic empathy between two individuals in a live electronic performance reported as perceived interactivity. Participants viewed eight videos, both real duets, and spliced solos appearing as real duets, rating each video. The questions guiding this study were: (a) is there a difference in perceived interactivity between the live and spliced duets, (b) is there a relationship between performance rating and perceived interactivity. Results showed a significant difference in the perceived interactivity of the video conditions. Further, the results showed a significant relationship between performance rating and perceived interactivity of the performers. The results suggest that perceived interactivity between performers could be a metric to measure kinesthetic empathy between performers facilitated by an interactive performance system that could be used to objectively measure the effectiveness of design and pedagogical interventions for new interfaces for musical expression.",nime2025_19.pdf,,,19.jpg,,,
20,paper,poster,medium,posters-1,,5,in person,XR Musical Keyboard: An Extended Reality Keyboard with an Arbitrary Number of Keys and Pitches,Tatsunori Hirai; Jack Topliss; Thammathip Piumsomboon,"We introduce the Extended Reality (XR) Musical Keyboard, a system allowing users to overlay a virtual keyboard onto a tabletop surface, such as a standard PC keyboard. This virtual keyboard is highly customizable: users can freely program the number of keys and their respective pitches. Modern software instruments offer advanced capabilities, including microtonal scales (pitches outside the standard 12-tone equal temperament). However, playing these instruments often remains challenging due to the lack of corresponding physical hardware. Our proposed solution addresses this gap by projecting a programmable virtual keyboard onto a tangible object within the XR space. This approach combines the software's flexibility with the tactile feedback of a physical surface, enhancing playability. Users can simplify the keyboard layout (e.g., fewer keys than a piano) or expand it beyond conventional limits to explore new expressive possibilities, particularly for microtonal music. We conducted a small pilot study (N=4) involving participants mostly inexperienced with keyboards to gather preliminary feedback on the interface's ease of use for performance.",nime2025_20.pdf,,,20.jpg,,,
25,paper,poster,long,posters-1,,5,remote,Appropriating Technology for Interactive Media in Theatre: Design Strategies and Aesthetic Insights,Ruoxi Jia; Xuebiao Liu,"This paper investigates interactive media design as a narrative agent in theatrical performance through a practice-based design approach. We clarify the role of the Interaction Director in the production team and examine challenges in appropriating technology for theatrical interaction design. A two-layer workflow is proposed, integrating macro-scale conceptual design with micro-scale cue-to-cue interaction mapping. We address the mutual dependency among creative disciplines, highlighting the collaborative processes necessary to resolve conflicts during the design and rehearsal stages. Furthermore, we adopt a scenographic perspective to analyse how interactive media contributes to dramaturgical storytelling by crafting visual and auditory metaphors. We contextualize interaction design with reader- response theory and the horizon of expectations, demonstrating how interactive media fosters collaborative creativity and expands the narrative potential of theatrical storytelling.",nime2025_25.pdf,,,25.jpg,,,
31,paper,poster,long,posters-1,,5,in person,pybela: a Python library to interface scientific and physical computing,Teresa Pelinski; Giulio Moro; Andrew McPherson,"Workflows to obtain, examine and prototype with sensor data often involve a back and forth between environments, platforms and programming languages. Usually, sensors are connected to physical computing platforms, and solutions to transmit data to the computer often rely on low-bandwidth communicating channels. It is not obvious how to interface physical computing platforms with data science environments, which also operate under distinct constraints and programming styles. We introduce pybela, a Python library that facilitates real-time, high-bandwidth, bidirectional data streaming between the Bela embedded computing platform and Python, bridging the gap between physical computing environments and data-driven workflows. In this paper, we outline its design, implementation and applications, including deep learning examples.",nime2025_31.pdf,,,31.jpg,,,
33,paper,poster,short,posters-1,,5,in person,Waveform Autoencoding at the Edge of Perceivable Latency,Franco Caspe; Andrew McPherson; Mark Sandler,"We introduce an audio plugin implementation of BRAVE, a waveform autoencoder presented recently, that affords Neural Audio Synthesis with low latency and jitter. As a redesign of the well-known RAVE model, BRAVE introduces a series of architectural modifications for supporting instrumental interaction with almost imperceptible latency (<10 ms) and jitter (~ 3 ms). By comparing both designs, we highlight key architectural differences between the models that impact their instrumental performance capability, arguing that no model fits all purposes, and calling for their careful selection for each interactive design. Finally, we discuss challenges and opportunities for leveraging low-latency waveform autoencoders to develop interactive systems, such as Digital Musical Instruments, that can foster control intimacy through enhanced responsiveness and space for nuance.",nime2025_33.pdf,,,33.jpg,,,
44,paper,poster,medium,posters-1,,5,remote,Out-of-Control Feedback Systems and Collaborative Influence with the Instrumentalist Mixer Feedback Transmutation System,Nolan Hildebrand; Timothy Roth,"This paper explores the Instrumentalist Mixer Feedback Transmutation (IMFT) system, a modification of the typical no-input mixer paradigm meant for collaborative improvisatory performance (formally called NIMB+)[9]. IMFT occurs when an instrumentalist is patched into a mixing board with feedback loops (a.k.a no-input mixer). The instrumentalist interacts and influences the mixer’s feedback together with another performer operating the mixer. Introducing an instrument into the no-input mixer’s previously closed system creates possibilities for new collaborative interactions between humans and chaotic feedback systems. In this system, a chaotic, out-of-control relationship can be formed where the output of the mixer and the gestures from the mixer performer can be in battle with the input from the instrumentalist and vice-versa. After a brief historical contextualization of mixer feedback, the IMFT system and the complex relationships that form between human and machine are introduced. No-input mixer performance practices are discussed, followed by exploration of a single feedback loop to illustrate some of the mixer’s possible sound worlds and the nature of the instrument. Performance experiences from two recent compositions by the first author, generative open graphic score #1 (2023) and noise ritual (2023), are described in order to explore different performance interactions created by different instrumentalists working with the IMFT system. This practice-based research provides a useful case study examining the entangled relationship between performers and interfaces in feedback-based music systems and how innovative approaches to an established electronic practice can create new perspectives and collaborative opportunities.",nime2025_44.pdf,,,44.jpg,,,
49,paper,poster,long,posters-1,,5,in person,Introducing EG-IPT and ipt~: a novel electric guitar dataset and a new Max/MSP object for real-time classification of instrumental playing techniques,Marco Fiorini; Nicolas Brochec; Joakim Borg; Riccardo Pasini,"This paper presents two key contributions to the real-time classification of Instrumental Playing Techniques (IPTs) in the context of NIME and human-machine interactive systems: the EG-IPT dataset and the ipt~ Max/MSP object. The EG-IPT dataset, specifically designed for electric guitar, encompasses a broad range of IPTs captured across six distinct audio sources (five microphones and one direct input) and three pickup configurations. This diversity in recording conditions provides a robust foundation for training accurate models. We evaluate the dataset by employing a Convolutional Neural Network-based classifier (CNN), achieving state-of-the-art performance across a wide array of IPT classes, thereby validating the dataset’s efficacy. The ipt~ object is a new Max/MSP external enabling real-time classification of IPTs via pre-trained CNN models. While in this paper it's demonstrated with the EG-IPT dataset, the ipt~ object is adaptable to models trained on various instruments. By integrating EG-IPT and ipt~, we introduce a novel, end-to-end workflow that spans from data collection, model training to real-time classification and human-computer interaction. This workflow exemplifies the entanglement of diverse components (data acquisition, machine learning, real-time processing, and interactive control) within a unified system, advancing the potential for dynamic, real-time music performance and human-computer interaction in the context of NIME.",nime2025_49.pdf,,,49.jpg,,,
55,paper,poster,short,posters-1,,5,remote,Repurposing a Rhythm Accompaniment System for Pipe Organ Performance,Nicholas Evans; Behzad Haki; Sergi Jorda,"This paper presents an overview of a human-machine collaborative musical performance by Raül Refree utilizing multiple MIDI-enabled pipe organs at Palau Güell, as part of the Organic concert series. Our earlier collaboration focused on live performances using drum generation systems, where generative models captured rhythmic transient structures while ignoring harmonic information. For the organ performance, we required a system capable of generating harmonic sequences in real-time, conditioned on Refree's performance. Instead of developing a comprehensive state-of-the-art model, we integrated a more traditional generative method to convert our pitch-agnostic rhythmic patterns into harmonic sequences. This paper details the development process, the creative and technical considerations behind the final performance, and a reflection on the efficacy and adaptability of the chosen methodology.",nime2025_55.pdf,,,55.jpg,,,
57,paper,poster,short,posters-1,,5,remote,Turntable-Based Electronic Music and Embodied Audience Interaction,Tak Cheung Hui; Xiaoqiao  Li; Yu Chia Kuo,"Rings… Through Rings transforms archival maps of Hong Kong’s military fortifications into playable surfaces for turntable-based electronic music. Laser-etched discs encode cartographic data, producing sonic textures manipulated through turntables and enhanced by audio techniques like cross-synthesis, concatenative synthesis, and spatialization. Grounded in theories of transcoding, productive agency, and participatory culture, the project reimagines the turntable as a cultural interface, bridging analog heritage with computational sound. This hybrid system blends pre-composed musical structures with real-time audience interaction, allowing participants to alter playback, swap discs, and influence spatial audio. By merging cartography, sound, and participatory design, the work offers a collaborative, multisensory approach to intangible heritage. Future developments include expanded spatial configurations, real-time disc fabrication, and AI integration to deepen engagement and cultural reinterpretation.",nime2025_57.pdf,,,57.jpg,,,
62,paper,poster,long,posters-1,,5,in person,You’re An Instrument!: Creating active music-making experiences through worldbuilding and storytelling,Ciaran Frame; Erick Mitsak; Alon Ilsar,"This paper outlines the development and key discoveries relating to active audience participation within You’re an Instrument!, an immersive childrens’ theatre show that turns a planted audience member into a musical instrument. We outline the use of wireless gestural instruments in the show, exploring their novel use as hidden props and theatrical devices that help invite audience members into a fictional world. Through the creation of this fictional world, the audience members became more actively involved in music making with these devices in the last third of the show. This paper is a call for instrument designers to consider using worldbuilding and storytelling techniques to more actively engage audience members in discovering the workings of new instruments.",nime2025_62.pdf,,,62.jpg,,,
64,paper,poster,short,posters-1,,5,in person,Sculpting the Sound Atom: Towards Per-Grain Parameterisation and Interfacing in Granular Synthesiser Design,Nathan Carter; Jim Murphy; Mo Zareei,"This paper presents a number of custom-designed granular synthesisers built around interfacing with sound grains on an ‘atomic’ level. Developed in Max/MSP by the first author (Nathan Carter), these synthesisers explore per-grain voice parameterisation that uniquely interfaces with individual grain signal processing properties in larger granular sequences. The paper outlines how these synthesisers provided the sound materials to compose Carter’s original soundscape work ‘Matter and Void’ – conceptually in- spired by ancient Epicurean physics and painterly expositions on atomism in Lucretius’ poem ‘The Nature of Things’ (c. 55 BC).",nime2025_64.pdf,,,64.jpg,,,
66,paper,poster,medium,posters-1,,5,in person,Harmonix Series: Accessible Digital Musical Instruments for Mindfulness and Creativity,Wing Hei Cheryl Hui; Patrick Hartono," This paper introduces the Harmonix series, a collection of Accessible Digital Musical Instruments (ADMIs) designed to enhance mood stability and mindfulness through intuitive and interactive music-making. Recognising the barriers posed by traditional digital musical instruments, including steep learning curves, high costs, and uninspiring outputs, Harmonix prioritises affordability, portability, and user-friendly interfaces to cater to individuals with no prior musical training. The study evaluates two instruments, ZenithChimes and Equilibrio. ZenithChimes employs touch-sensitive keys mapped to meditative tones in the Aeolian mode, promoting creativity and relaxation. Equilibrio, a 3D-printed ""stone stack,"" uses tilt gestures to modulate soundscapes, symbolising balance and harmony. Both instruments integrate calming auditory outputs and minimalist design aesthetics to create an engaging and meditative experience. A workshop-based study with 10 participants, spanning diverse backgrounds, demonstrated the instruments' accessibility and therapeutic potential. Results showed that participants found the instruments easy to use, aesthetically appealing, and suitable for mindfulness practices, with 70 percent identifying their integration into meditation or yoga sessions as beneficial. However, feedback highlighted the need for more customisation options, particularly in Equilibrio's soundscapes. By bridging art, technology, and mindfulness, Harmonix fosters creative exploration and emotional regulation, with implications for therapeutic, educational, and artistic applications. Future work will explore sustainability, inclusivity, and multi-sensory feedback to enhance the instruments’ design and impact. This study underscores the potential of ADMIs to transcend conventional music-making, offering innovative tools for well-being and self-expression.",nime2025_66.pdf,,,66.jpg,,,
70,paper,poster,short,posters-1,,5,in person,Creating a White Noise Instrument for Collaborative Improvisation,Austin Oting Har; Kurt Mikolajczyk,"This paper introduces shiki, a virtual instrument developed for performing Renga for White Noise, an interdisciplinary project that transmediates Japanese renga poetry principles into a framework for collaborative improvisation with human and AI agents. We discuss two areas: (1) our transmediation of renga’s structuring principles into shiki’s design (2) the technical aspects behind the AI agent’s performance with shiki. Through its interdisciplinary and intercultural entanglements with renga, transmediation as a method can illuminate new perspectives on the design and performance of NIMEs.",nime2025_70.pdf,,,70.jpg,,,
71,paper,poster,long,posters-1,,5,in person,Collaboration and Recursion: reflections on Chinese calligraphy and feedback,Iurii Kuzmin; Raul Masu; Omar Al Kanawati,"This paper delivers reflections on collaboration between a sound artist and an artist specializing in Chinese calligraphy, that resulted in the creation of a sound installation combining contemporary Chinese calligraphy with electroacoustic feedback. Adopting the “reflection-on-action” approach, the authors engaged in an in-depth discussion, revisiting the details of the creative process based on the extensive documentation arranged in a form of visual diary. The paper highlights three orders of recursivity that are either physically present in the work (feedback), defined the creative process (collaboration) and used as analytical tools (ecology) to discuss the dynamics of collaboration and cultural influences in NIME practice.",nime2025_71.pdf,,,71.jpg,,,
94,paper,poster,medium,posters-1,,5,in person,A Computer Application to Explore 53-Tone Equal Temperament Harmonies Through Modal Interchange,David Dalmazzo; Ken Déguernel; Bob Sturm,"We present a novel computer application for real-time exploration of microtonal harmonies through intuitive visualization and integrated MIDI controllers, bridging theoretical concepts and practical musical applications. We extend modern harmonic principles from 12-tone equal temperament (12-TET), incorporating interval distinctions from 31-TET (subminor, neutral, supermajor), and further expanding into detailed harmonic possibilities of 53-tone equal temperament (53-TET). Our application leverages modal interchange and parallel chord substitutions, offering intuitive navigation through microtonal harmonic trajectories. The implementation utilizes MIDI Polyphonic Expression (MPE) via our custom MaxForLive application, The Bridge, ensuring precise microtonal control and compatibility with digital audio workstations (Ableton Live 11/12). The system includes real-time visualization, interactive chord manipulation, and a comprehensive Scale Editor for harmonic experimentation. Through practical examples and theoretical analysis, we demonstrate how this approach reveals new harmonic possibilities while maintaining connections to established modal frameworks. This research contributes to the growing microtonal music field by providing theoretical foundations and practical tools that incorporate extended tuning systems into contemporary musical practice.",nime2025_94.pdf,,,94.jpg,,,
123,paper,poster,medium,posters-1,,5,in person,Making the Immaterial Material: A Diffractive Approach Toward a Politics of Material Culture Within NIME,Brittney Allen; Andrew Mcpherson; Alexandria Smith; Jason Freeman,"Traditional Human-Computer Interaction has often been critiqued for its ostensibly opaque position on ethical, ontological, and epistemological concerns, particularly in relation to completed design artifacts. More recently, similar criticisms have been directed at the New Interfaces for Musical Expression (NIME) community for its relative silence on contemporary political issues. However, it is possible that an implicit ethics of material culture is already embedded within NIME discourse — one that could be critically examined and potentially mobilized as a foundation for a more explicitly political ethics. Inspired by feminist discourse, namely Karen Barad's theory of agential realism, and contextualized through Bruno Latour's remarks regarding the ethics of design, this paper explores the possibilities of entanglement in DMI design. We begin with a discussion of diffraction and entanglement followed by a brief overview of values-oriented and ""world-building"" theoretical models and methodologies of design research. We continue with our generative ""DMI-as-apparatus"" approach to diffractive methodology and conclude with a case study BRAIDS_, a digital music instrument based upon the Black American cultural practice of hair braiding, that examines critical design decisions that are otherwise deemed invisible by traditional methods of scientific inquiry.",nime2025_123.pdf,,,123.jpg,,,
154,paper,poster,medium,posters-1,,5,remote,LIMITER: A Gamified Interface for Harnessing Just Intonation Systems,Antonis Christou,"This paper introduces LIMITER, a gamified digital musical instrument for harnessing and performing microtonal and justly intonated sounds. While microtonality in Western music remains a niche and esoteric system that can be difficult both to conceptualize and to perform with, LIMITER presents a novel, easy to pickup interface that utilizes color, geometric transformations, and game-like controls to create a simpler inlet into utilizing these sounds as a means of expression. We report on the background of the development of LIMITER, as well as explain the underlying musical and engineering systems that enable its function. Additionally, we offer a discussion and preliminary evaluation of the creativity-enhancing effects of the interface.",nime2025_154.pdf,,,154.jpg,,,
67,paper,poster,medium,posters-2,,5,remote,Decoupling Physical and Virtual Spaces in Co-Located Collaborative Mixed Reality Instruments with gRAinyCloud,Pierrick Uro; Florent Berthaut; Thomas Pietrzak; Marcelo Wanderley,"Collaborative co-located Mixed Reality musical instruments combine some of the expressive opportunities of 3D interaction and communication and cooperation of physical multi-user instruments. However in existing instruments, the fixed coupling between the virtual and physical environments constrains the affordances brought by Mixed Reality, such as per-musician free navigation in or multi-scale control of virtual structures. We designed gRAinyCloud, as a way to reintegrate these lost affordances to a co-located instrument. It allows for the expressive exploration of a set of sounds represented by a virtual structure of shapes placed in the physical space and shared between musicians. Above all, gRAinyCloud enables each musician to freely manipulate their own viewpoint, changing its scale, position and rotation, effectively decoupling the physical and virtual spaces, and to switch between self, other's and absolute viewpoint while playing. We describe the implementation of this decoupling of spaces and analyse its uses and implications for collective musical expression, by relying on a first-person approach.",nime2025_67.pdf,,,67.jpg,,,
73,paper,poster,medium,posters-2,,5,remote,VentHackz: Exploring the Musicality of Ventilation Systems,Maham Riaz; Ioannis Theodoridis; Çağrı Erdem; Alexander Refsum Jensenius,"Ventilation systems can be seen as huge examples of interfaces for musical expression, with the potential of merging sound, space, and human interaction. This paper explores conceptual similarities between ventilation systems and wind instruments and explores approaches to “hacking” ventilation systems with components that produce and modify sound. These systems enable the creation of unique sonic and visual experiences by manipulating airflow and making mechanical adjustments. Users can treat ventilation systems as musical interfaces by altering shape, material, and texture or augmenting vents. We call for heightened attention to the sound-making properties of ventilation systems and call for action (#VentHackz) to playfully improve the soundscapes of our indoor environments.",nime2025_73.pdf,,,73.jpg,,,
124,paper,poster,long,posters-2,,5,in person,The Sparksichord: Practical Implementation of a Lorentz Force Electromagnetic Actuation and Feedback System,Adam Schmidt; Jeffrey Snyder; Gian Torrano Jacobs; Joyce Chen; Joseph Gascho; Andrew McPherson,"In line with a sustained community interest in electromagnetic actuation of musical instruments, we describe practical considerations for Lorentz Force actuation in conductive strings, exemplified by the Sparksichord – an augmented harpsichord that uses Lorentz Force actuation, optical feedback, and analog circuitry to sustain vibrations of its brass strings. Electromagnetically-actuated and feedback instruments have grown increasingly popular in NIME, though most systems rely on the use of solenoid-style electromagnetic coils. By running current through the string itself, Lorentz Force actuation offers an alternate arrangement of magnets and wire that can afford new modes of interaction, a broader frequency response, and cheaper implementation.  We aim to empower practitioners with a toolbox for designing and building actuated instruments of this style and describe our specific implementation for this instrument.",nime2025_124.pdf,,,124.jpg,,,
125,paper,poster,medium,posters-2,,5,remote,Creating a Real-Time Responsive Handbalancing Interface with HAND★CS,Linnea Kirby; Christiana Rose; Jeremy Cooperstock; Marcelo Wanderley,"This paper introduces HAND★CS, a new interface for interdisci- plinary expression for music, movement, and light. Our interface augments a pedagogical interface for hand-balancing, Haptics- Assisted iNversions Device (HAND), and transforms it into one for artistic expression. It draws upon Licklider’s concept of man- computer symbiosis, specifically the commensalism form of sym- biosis. HAND★CS strives to embody a performance apparatus and system with symbiotic connectivity between performer and interface. This paper discusses the inspiration and background for such a system pulling from the fields of human-computer interaction (HCI), music technology and new interfaces for mu- sical expression (NIME), and circus arts. In addition, it defines the design and implementation, evaluation of the prototype of HAND★CS, and future work.",nime2025_125.pdf,,,125.jpg,,,
135,paper,poster,medium,posters-2,,5,in person,The Shadow Harvester: Sonifying the Body Through Light,Darlene Castro,"This paper explores the use of a violinist’s shadow as input for a NIME prototype called the Shadow Harvester. For centuries, shadows have captivated humanity’s imagination, and this project follows many artists, philosophers, and researchers equally captivated by the potential of shadows and silhouettes. This interface consists of a semi-translucent screen embedded with light-detecting sensors. These sensors register the movement of the violinist’s shadow and produce data that can be mapped to generate, trigger, or process sound in Max/MSP. The Shadow Harvester turns a human shadow into a real-time, life-size avatar, splitting the attention of the violinist between their shadow self and carnal self. They are ensnared in a web of sensors that require the same attention as the visceral joints in their body because any movement carries sonic repercussions through either their physical body playing the violin or their shadow body “playing” this interface. The Shadow Harvester creates a highly entangled feedback loop between the violinist, centuries of violin performance practice, and composition. As such, it carries the potential to encourage new ways of incorporating movement into the folds composition, notation, and performance.",nime2025_135.pdf,,,135.jpg,,,
139,paper,poster,medium,posters-2,,5,in person,Exploring Musical Creation Through Brain-Body Digital Musical Instruments,Yue Wang; Meiling Wu,"Brain-Body Digital Musical Instruments (BBDMI) merge physiological signals with real-time sound processing, enabling performers to use Electromyographic (EMG) data for musical expression. This study explores the creative and technical potential of BBDMI, focusing on signal acquisition, calibration, and mapping for use in composition and performance. Demonstrations with flute, piano showcase its ability to enhance expressivity through gestural control. While key advancements include improved signal stability and refined mapping, challenges such as connectivity issues and notation limitations remain. This research highlights BBDMI’s promise as a transformative tool in contemporary music.",nime2025_139.pdf,,,139.jpg,,,
140,paper,poster,long,posters-2,,5,in person,Mixer Metaphors: audio interfaces for non-musical applications,Tace McNamara; Jon McCormack; Maria Teresa Llano,"The NIME conference traditionally focuses on interfaces for music and musical expression. In this paper we reverse this tradition to ask, can interfaces developed for music be successfully appropriated to non-musical applications? To help answer this question we designed and developed a new device, which uses interface metaphors borrowed from analogue synthesisers and audio mixing to physically control the intangible aspects of a Large Language Model. We compared two versions of the device, with and without the audio-inspired augmentations, with a group of artists who used each version over a one week period. Our results show that the use of audio-like controls afforded more immediate, direct and embodied control over the LLM, allowing users to creatively experiment and play with the device over it's non-mixer counterpart. Our project demonstrates how cross-sensory metaphors can support creative thinking and embodied practice when designing new technological interfaces.",nime2025_140.pdf,,,140.jpg,,,
146,paper,poster,short,posters-2,,5,remote,ViVo: Piano Learning Through Visualizing Vocalizations on a Lighted Keyboard,Maya Caren,"Vocalization and visualization are recognized as two powerful methods for internalizing music that are effective with beginner and skilled musicians alike. Despite the well-researched benefits of each practice, integrated visualization of vocalizations for instrument learning has seen little attention in the music technology community. This paper introduces the design and implementation of ViVo, a piano learning tool that connects the embodied sense of pitch offered by vocalization with the spatial intuition provided by in situ visualization. ViVo offers two modes: a real-time mode that hears live user vocalizations to concurrently illuminate the corresponding piano keys, and a practice mode that visualizes recorded vocalizations for repeated practice. By providing an integrated system to foster and visualize vocalizations, ViVo aims to leverage the noted benefits of both practices to make learning piano more effective, intuitive, and engaging.",nime2025_146.pdf,,,146.jpg,,,
148,paper,poster,medium,posters-2,,5,in person,Tiny Touch Instruments: Composing for Collaborative Mobile Performance,Rebecca Abraham,"This paper explores Tiny Touch Instruments (TTIs), a set of mobile instruments, and their role in facilitating collaborative, unrehearsed music-making. Through the composition and performance of two pieces, Skating and Skipping, this work investigates how multimodal notation and instrument design can shape performer experience, interaction, and engagement. Performances were documented through participant observation, interviews, and a survey, revealing key themes such as the role of notation in guiding improvisation, the balance between agency and unpredictability in digital instruments, and the recontextualization of mobile devices as musical tools.",nime2025_148.pdf,,,148.jpg,,,
155,paper,poster,long,posters-2,,5,remote,The EV: An Iterative Journey in Digital-Acoustic String Instrument Augmentation,Brian Lindgren,"Numerous experiments in bowed string augmentation have been undertaken, each reflecting the values and interests of the builder. The EV takes a unique approach, with the convolution of a synthesized and acoustic string signal at the foundation of its design. Through an iterative hardware and software development process, three versions of the instrument have been created, each building toward the goal of a robust compositional and performative platform for exploring the shared boundary of electronic and acoustic sound. Spatialization and physical modeling algorithms have furthered the instrument’s engagement with the interaction between physical and virtual acoustics. This paper examines the iterative design process behind the instrument and its relationship between digital augmentation and acoustic resonance.",nime2025_155.pdf,,,155.jpg,,,
156,paper,poster,short,posters-2,,5,in person,Live Improvisation with Fine-Tuned Generative AI: A Musical Metacreation Approach,Misagh Azimi; Mo H. Zareei,"This paper presents a pipeline to integrate a fine-tuned open-source text-to-audio latent diffusion model into a workflow with Ableton Live for the improvisation of contemporary electronic music. The system generates audio fragments based on text prompts provided in real time by the performer, enabling dynamic interaction. Guided by Musical Metacreation as a framework, this case study reframes generative AI as a co-creative agent rather than a mere style imitator. By fine-tuning Stable Audio Open on a dataset of the first author’s compositions and field recordings, this approach demonstrates the ethical and practical benefits of open-source solutions. Beyond showcasing the model’s creative potential, this study highlights the model’s significant challenges and the need for democratized tools with real-world applications.",nime2025_156.pdf,,,156.jpg,,,
163,paper,poster,medium,posters-2,,5,Does not know yet,Towards the Continuous Harmonium: Replicating the Continuous Keyboard,Travis West; Ninad Puranik; Marcelo Wanderley; Gary Scavone,"In our effort to develop an augmented harmonium to enable the performance of continuous pitch ornamentation while preserving typical harmonium gestures, we have replicated the continuous keyboard presented by McPherson et al. in prior work. We present 1) our adaptations to the design of the sensing system, 2) our preliminary novel mapping design, and 3) a report on our replication process.",nime2025_163.pdf,,,163.jpg,,,
165,paper,poster,medium,posters-2,,5,in person,Maximum Silence to Noise: Sound synthesis for responsive gestural control,Andrew Brown,"Modulation synthesis has been a foundational technique in the development of electronic musical instruments since their inception. This paper presents a novel approach to ring modulation synthesis, termed Maximum Silence to Noise (MSN), along with an associated method of gestural control facilitated by a pressure-sensitive multi-touch controller. The primary objective of this research is to develop an instrument capable of producing a broad and diverse range of audio spectra that can be expressively articulated through responsive touch-based interaction. Integrating the synthesis process with gestural parameter mapping is crucial for the performative capabilities of New Interfaces for Musical Expression (NIMEs). The technical development of the MSN instrument was subject to an iterative design process with mixed method evaluation. The usability and practical application of the MSN instrument was refined through performance experiences, which illustrate the effectiveness of the synthesis-gesture mappings in providing dynamic and expressive control over the diverse generated audio spectra.",nime2025_165.pdf,,,165.jpg,,,
167,paper,poster,medium,posters-2,,5,in person,SwimTunes: A gamified music performance system for co-creating with a novice audience,Thomas Studley,"This paper presents SwimTunes, a prototype game system designed for novice multi-user music-making in live performance settings. The system features a digital game and a public web app that allows audience members to participate using their mobile devices. After connecting via QR code, participants create and pilot virtual fish that generate music as they bump into one another. The performer then enters the game as a shark, using camera-based hand tracking to chase and consume the participants’ fish. The result is a performance dynamic that evolves from playful co-creation to one of gameful contest between the performer and audience. SwimTunes explores how this shifting interaction context can shape the instantiation of a set of musical parameters, and further how performers can harness gameplay metaphors to conduct live audiences in shared acts of musical expression. The paper details the design considerations and conceptual motivations that informed SwimTunes before describing its implementation via Node.js, Open Sound Control, Unreal Engine 5, and MetaSounds. It discusses technical challenges and opportunities unearthed during development and outlines future directions for the project and gamified music performance at large.",nime2025_167.pdf,,,167.jpg,,,
172,paper,poster,short,posters-2,,5,in person,A Synthetic Cicada Soundscape Controlled by Breath,Toby Gifford,"This paper describes an interactive installation featuring a generative soundscape with breath control, that aims to capture the feeling of being in a forest full of cicadas. Inspired by a period of deep listening to cicada stridulations – in which I found the spatio-temporal pulsation of the sound mass reminiscent of breathing – this installation uses breath control to give a sense of breathing with a forest. The sound mass consists of multiple generative sources, each loosely modelled on an individual cicada stridulating. Each ‘cicada’ comprises a temporal hierarchy of pulse trains modulating a carrier frequency, with a simple sonic spatialization algorithm applied to give the sense of immersion in the sound mass. The algorithm is implemented in the Extempore audiovisual programming language, and utilizes an architecture in which each sonic parameter is inherently stochastic, much as the sound production mechanisms of actual Cicadas exhibit natural variation.",nime2025_172.pdf,,,172.jpg,,,
179,paper,poster,medium,posters-2,,5,in person,Augmentation of a Historical Harpsichord Keyboard Replica for Haptic-Enabled Interaction in Museum Exhibitions,Matthew Hamilton; Michele Ducceschi; Roberto Livi; Catalina Vicens; Andrew McPherson,"This paper describes the design and creation of an electronically augmented replica of a historical harpsichord keyboard with a typical 17th-century Italian layout to create a digital musical instrument. The keyboard was commissioned for exhibition in a musical instrument museum to enhance the visitor experience by providing an interface to digitised versions of instruments within the collection. The replica balances the competing de- mands of historical authenticity, public accessibility, and preser- vation. It replicates the original instrument’s tactile feedback and mechanical resistance using historically informed construction techniques. Optical sensors integrated within the mechanism capture the jacks’ motion data, enabling MIDI message gener- ation. This work situates itself within broader discussions on the role of technology in museums. A keyboard interface of this type o￿ers an opportunity to enhance visitor interaction with musical heritage while safeguarding delicate artefacts. The paper examines the keyboard’s design principles, technical implemen- tation, and implications, emphasising its contribution to public engagement and the long-term preservation of musical heritage.",nime2025_179.pdf,,,179.jpg,,,
189,paper,poster,short,posters-2,,5,Does not know yet,PanMan - a modular tangible controller for sound spatialization,Krzysztof Cybulski; Szczepan Busko; Zachariasz Zalewski,"PanMan is a performance-oriented modular midi controller, conceived as a tangible interface for panning multiple sound sources in multichannel audio systems. It consists of four independent control units and a docking base - the modular design allows each of the units to be either physically attached to the base (in which case it might be used as a single controller by a single user) or connected to it via extension cords, allowing up to four users to participate in an interactive sound installation experience or a collaborative performance setting. The physical controls on a single module consist of a joystick/trackball hybrid – a dome-shaped control device designed to be operated with a single finger – and a thumbwheel for additional parameter control, positioned at the edge of the module, allowing for one-handed operation of three parameters. The design facilitates operation by both right- and left-handed users, also allowing a single user to operate two or more controllers simultaneously, controlling a number of parameters at once.",nime2025_189.pdf,,,189.jpg,,,
200,paper,poster,medium,posters-2,,5,remote,Sonicolour: Exploring Colour Control of Sound Synthesis with Interactive Machine Learning,Tug F. O'Flaherty; Luigi Marino; Charalampos Saitis; Anna Xambó Sedó,"This paper explores crossmodal mappings of colour to sound. The instrument presented analyses the colour of physical objects via a colour light-to-frequency sensor and maps the corresponding red, green, and blue data values to parameters of a synthesiser. Interactive machine learning is used to facilitate the discovery of new relationships between sound and colour. The role of interactive machine learning is to find unexpected relationships between the visual features of the objects and the sound synthesis. The performance is evaluated by its ability to provide the user with a playful interaction between the visual and tactile exploration of coloured objects, and the generation of synthetic sounds. We conclude by outlining the potential of this approach for musical interaction design and music performance.",nime2025_200.pdf,,,200.jpg,,,
202,paper,poster,medium,posters-2,,5,remote,A Spherical Tape Topology for Non-linear Audio Looping,Kevin Blackistone,"There have been many physical design formats used in the field of audio recording. As audio has an inherently a linear, time-based structure, these have generally followed logical layouts such as tape, or grooved records and cylinders. This project explores magnetic recording technology and digital analogues for recording and playback that are instead on spherical topology. This instrument expands the concept of the audio loop through a more tangible and randomized approach than traditional record playback techniques of tape, while maintaining a familiarity with historic techniques of audio looping and scrubbing. Through it, one can not only create linear time-loops but blends between different times of the recording non-sequentially. The size and mass of the spheres enhances the performative elements through the physics of inertia. The movement possibilities allow for non-linear circles, circuits, spirals and other patterns of sound not traditionally possible through linear tape or digital loop, including accelerations and decelerations – akin to a turntable, but with greater freedom of direction, thus offering surreal record/playback possibilities.",nime2025_202.pdf,,,202.jpg,,,
184,paper,poster,short,posters-3,,5,in person,Metabow: Gesture Mapping in Immersive Sonic Environments,Davor Vincze; Roberto Alonso; Peter Nelson,"This paper presents the MetaBow, an augmented violin bow designed to control digital sound processing through real-time motion tracking. We discuss the challenges of mapping Inertial Measurement Unit (IMU) data to audio parameters in immersive multi-speaker environments and propose hybrid strategies using both direct mapping and machine learning models. We reflect on design choices, trade-offs, and performer experience, drawing from technical development and performance contexts. Three condensed case studies illustrate the system’s versatility in spatial and interactive musical performance.",nime2025_184.pdf,,,184.jpg,,,
208,paper,poster,long,posters-3,,5,remote,Exploiting Latency In The Design Of A Networked Music Performance System For Percussive Collective Improvisation,Ari Liloia; Roger Dannenberg,"We present the design, prototype implementation, and informal testing of a distributed web-based networked music performance (NMP) system for collaborative improvisation and experimentation. Influenced by composition and interaction design techniques from a wide range of work on collaborative virtual music environments, rather than treating latency as inherently disruptive to the musical and social engagement that characterizes traditional performance, we incorporate and exploit network delay to facilitate and visualize them, providing a novel approach to creating ""jam session""-like experiences without a separate audience. During sessions, users collaboratively perform semi-improvised music in quasi-real time. The production and interpretation of individual musical gestures (""drum hits"") are visualized in a continuously devised feedback network. The music produced can be treated as a starting point for compositions developed asynchronously, or as complete pieces of music produced live.",nime2025_208.pdf,,,208.jpg,,,
214,paper,poster,medium,posters-3,,5,Does not know yet,Hybrid Hand Drum: Where Tradition Resonates Through Technology,Casper Preisler; Daniel Overholt,"This paper presents a hybrid frame drum that entangles acoustic and digital elements, merging the expressive depth of traditional percussion with the expanded possibilities of digital sound processing. Designed with 4 key principles - portability, hybridity, simplicity and low latency - the instrument allows for a fluid interplay between physical and real-time digital augmentation. Equipped with piezoelectric sensors, an FSR, and a DSP algorithm the drum extends its sonic landscape while preserving its acoustic presence. The design maintains an organic relationship between physical interaction and digital processing, and the three potentiometers provide intuitive yet flexible control, maintaining a balance between minimalism and expressivity. The bela platform ensures very low latency (7.55 ± 0.13 ms), making it highly responsive for live performance. User evaluation highlights its potential for expressive control and seamless hybrid performance while suggesting ergonomic and functional refinements. Future enhancements, such as feedback control and DSP presets, could deepen the entanglement between performer, instrument, and sound. This research explores the intersection of acoustic and digital sound, contributing to the design of hybrid instruments that blur the boundaries between physical resonance and electronic transformation, expanding possibilities for musical interaction.",nime2025_214.pdf,,,214.jpg,,,
215,paper,poster,long,posters-3,,5,in person,Embedded Comparo: Small DSP Systems Side-by-Side,Francesco Di Maggio; Bart Hengeveld; Atau Tanaka,"This paper presents a comparative analysis of four embedded platforms designed for real-time audio processing: Bela, Daisy, OWL, and Raspberry Pi. These platforms have become integral tools in the field of digital musical instrument design, offering a variety of workflows, programming environments, and deployment methods. Although each system carries its own distinct strengths and constraints, the current workflow to embed DSP code across multiple devices lacks standardized approaches. To address this challenge, we develop a methodology that focuses on deploying Pure Data patches across all four platforms. Our study is structured around four test patches. Our findings highlight the trade-offs in latency, processing power, and memory constraints across the selected platforms. As a result, we propose a streamlined workflow to deploy Pd patches on each board using Plugdata, the Heavy Compiler, and their respective Web IDEs. As an ongoing contribution to the NIME community, we document our methodologies, workflows, and best practices in an open source repository, which serves as a continuously evolving resource for future research in the hands of musicians, researchers, and developers working with embedded musical systems.",nime2025_215.pdf,,,215.jpg,,,
226,paper,poster,medium,posters-3,,5,remote,Two Sonification Methods for the MindCube,Fangzheng Liu; Lancelot Blanchard; Don D Haddad; Joseph Paradiso,"In this work, we explore the musical interface potential of the MindCube, an interactive device designed to study emotions. Embedding diverse sensors and input devices, this interface resembles a fidget cube toy commonly used to help users relieve their stress and anxiety. As such, it is a particularly well-suited controller for musical systems that aim to help with emotion regulation. In this regard, we present two different mappings for the MindCube, with and without AI. With our generative AI mapping, we propose a way to infuse meaning within a latent space and techniques to navigate through it with an external controller. We discuss our results and propose directions for future work.",nime2025_226.pdf,,,226.jpg,,,
228,paper,poster,long,posters-3,,5,in person,Acoustic-digital hybrid synthesizer,Levin Schnabel; Dan Overholt,"This paper explores the design and evaluation of an acoustic-digital hybrid instrument that aims to address key criticisms of Digital Musical Instruments (DMIs), particularly the separation of control and sound generation. By integrating an interactable physical string with coupled Finite Difference Schemes (FDS) for physical modeling synthesis, the instrument creates a tactile and responsive playing experience. The instrument was evaluated through a mixed-methods approach, combining qualitative think-aloud protocols with the Musician’s Perception of the Experiential Quality of Musical Instruments Questionnaire (MPX-Q). Results indicate that the instrument fosters curiosity and creativity but highlights challenges in achieving traditional acoustic playability, such as latency and perceptual dissonance. These findings emphasize the potential and limitations of acoustic-digital hybrids in reuniting control and sound, offering valuable insights for future developments in musical interface design.",nime2025_228.pdf,,,228.jpg,,,
234,paper,poster,short,posters-3,,5,in person,The Drum Machine of Tao,Xiaowan Yi; Mathieu Barthet,"The Drum Machine of Tao (Tao) is a machine learning–based system that reverse-engineers sequencer parameters and one-shot percussive samples from drum loops, restoring low-level editability to sampled loops that would otherwise be frozen in audio waveforms. The philosophy behind this system is inspired from Taoism: that which returns to its primal state is the great Way of Tao. In this paper, we present the system design of Tao, which includes a state-of-the-art drum source separation model, a sequencer parameter estimation model, and a bespoke one-shot sample extraction algorithm that leverages differentiable audio synthesis. Results from a prototype are available for listening.",nime2025_234.pdf,,,234.jpg,,,
246,paper,poster,long,posters-3,,5,remote,Tapping Into a New Paradigm: A Synthetic Strategy for Automatic Drum TapScription,André Santos; Amílcar Cardoso; Matthew  E. P. Davies; Roger B. Dannenberg,"We introduce Automatic Drum TapScription (ADTS), a novel paradigm for rhythmic interaction consisting of transcribing arbitrarily-timbred taps into drum representations. Our approach targets taps produced on a variety of surfaces without other controlled timbral characteristics other than playing style. Our long-term goal is to enable more accessible and creative percussive exploration but presents significant challenges due to the minimal timbre variation between taps intended to represent different drum classes. To address these challenges, we take the first steps toward achieving ADTS by designing an effective dataset synthesis strategy. This strategy enables new opportuneties for musical expression by considering drumming at a more semantic or functional level as opposed to a simple collection of timbres. We present initial results, comparing three different models: one trained on drum data, another trained on a small dataset of quasi-aligned tapped performances, and another trained on our synthetic dataset. Our synthetic approach shows promise, demonstrating progress in this untapped domain.",nime2025_246.pdf,,,246.jpg,,,
250,paper,poster,medium,posters-3,,5,remote,Towards a Repository Template for Music Technology Research,Alessandro Fiordelmondo; Matteo Spanio; Patricia Cadavid; Xinran Chen; Sergio Canazza; Raul Masu,"Documenting and sharing research output is essential to construct the critical discourse on new music technology. Documentation feeds the knowledge and the values with which to evaluate and discuss current achievements and musical creations as well as to plan for the future. Besides publishing our research in conferences and journals, sharing research materials and outcomes like software, hardware, instruments, and datasets is important. This allows others to use the latest technology and improve it. For this purpose, the repository is increasingly commonly used by researchers and artists to store and share their works. However, creating repositories does not follow a clear and organised structure like the one we find, for example, in papers. The heterogeneity of repositories makes it hard to use both practically and for analysis. Although the variety and differences of research products in the field of new musical technologies are obvious, we believe that defining repositories with common guidelines could significantly improve the critical discourse in this area. This issue has been discussed at the NIME conference through workshops and papers. In this article, we want to continue this discussion and propose a flexible repository template to organise and present research materials and outcomes in the field of musical technologies research. The article provides a short and focused review of how repositories are currently used at the NIME conference, with special attention to the platforms used. Based on this study, we introduce a repository template that will be applied to case studies. We hope this proposal will encourage further discussion and advancement on this issue and, at the same time, support and facilitate the creation of new repositories.",nime2025_250.pdf,,,250.jpg,,,
254,paper,poster,long,posters-3,,5,in person,Designing A Tangible Rhythmic Interface for Digital Drum Talk,Pierre-Valery Tchetgen,"I propose a tangible user interface and communication protocol for computer mediated rhythm-based interaction for educational applications (music, language, mathematics). Thinking beyond the paradigm of keyboard-and-screen-based interfaces, this project is based on previous work on the Drumball. By converting rhythmic input into multimodal output, it creates an entangled ecosystem where the human body, digital musical instruments, and the Internet of Things intersect. Such a digital orality system could offer parents and practitioners a novel method for introducing children to literacy, STEAM skills and multimodal communication in the early years. I present design iterations of 1) a tangible rhythmic interface for digital drum talk inspired by the style of play of the Djembe, 2) a protocol for sending piezo sensor outputs over a custom PCB shield, which can be recognized across multiple platforms and web-based environments without additional customization; and 3) a suite of rhythm-based learning games using the Alphariddims multimodal symbol system based on the Morse code. I argue that such a culturally-grounded approach to music technology design provides a viable avenue for the preservation and revitalization of the vibrant, yet intangible, cultural heritage and traditions of the African talking drum cultural systems.",nime2025_254.pdf,,,254.jpg,,,
263,paper,poster,short,posters-3,,5,remote,AI Harmonizer: Expanding Vocal Expression with a Generative Neurosymbolic Music AI System,Lancelot Blanchard; Cameron Holt; Joseph Paradiso,"Vocals harmonizers are powerful tools to help solo vocalists enrich their melodies with harmonically supportive voices. These tools exist in various forms, from commercially available pedals and software to custom-built systems, each employing different methods to generate harmonies. Traditional harmonizers often require users to manually specify a key or tonal center, while others allow pitch selection via an external keyboard–both approaches demanding some degree of musical expertise. The AI Harmonizer introduces a novel approach by autonomously generating musically coherent four-part harmonies without requiring prior harmonic input from the user. By integrating state-of-the-art generative AI techniques for pitch detection and voice modeling with custom-trained symbolic music models, our system arranges any vocal melody into rich choral textures. In this paper, we present our methods, explore potential applications in performance and composition, and discuss future directions for real-time implementations. While our system currently operates offline, we believe it represents a significant step toward AI-assisted vocal performance and expressive musical augmentation. We release our implementation on GitHub.",nime2025_263.pdf,,,263.jpg,,,
276,paper,poster,medium,posters-3,,5,in person,Luna: An AR Musical Instrument on the Meta Quest 2,Samuel Dietz; Charles Patrick Martin,"Head-mounted augmented reality (AR) computers present the opportunity to develop new musical interfaces that would be impossible to build physically or with conventional computing devices. Unfortunately, typical computer music tools have not been easy to apply within AR development tool chains. Integrating standard computer music tools in AR development would allow more rapid prototyping of new instrument ideas and transfer of knowledge from experienced computer musicians. The goal of this paper is to demonstrate that AR digital musical instruments can be developed using libpd, the library version of the standard computer music environment Pure Data. We present a case study of an AR instrument developed for the Meta Quest 2 integrating libpd in the AR development tool-chain for the interactive audio components. The iterative development process was tracked through autoethnographic reflections and analysed with thematic analysis. We found that Pure Data was an effective way to develop audio interactions on the Quest 2 and that the hand tracking on this platform was capable of complex gestural interactions. This work could enable a broader community of computer musicians to explore AR NIME development, taking advantage of the unique affordances of this medium.",nime2025_276.pdf,,,276.jpg,,,
278,paper,poster,short,posters-3,,5,Does not know yet,A Gesture-Based Approach to Spatialization in Dolby Atmos,Vitor Pinheiro,"This paper presents a system for the spatialization of sound objects in the Dolby Atmos format, implemented through the integration of an infrared sensor with a chain of three software tools. The setup enables translating hand gestures into spatialization data within the constraints of the Atmos format. The design and parameter mapping are described, along with its usability, strengths, and limitations, as assessed through a preliminary evaluation conducted by the author. Beyond the technical aspects, this article reflects on the author's experience using the system as a mixing engineer and connects these insights to the conceptual framework of related works. This perspective offers a critical reflection on spatialization as a performative practice within studio workflows, highlighting how such devices may be integrated into the multimodal studio environment to introduce new means of interaction in sound spatialization.",nime2025_278.pdf,,,278.jpg,,,
280,paper,poster,long,posters-3,,5,in person,ChuMP: The Zen and Art of Package Management,Nicholas Shaheed; Ge Wang,"ChuMP stands for “ChucK Manager of Packages”, designed to automate the process of installing, upgrading, and removing software components for the ChucK programming ecosystem. ChuMP manages libraries, tools, audio and graphics plugins in a centralized, structured, and versioned manner. This project originated out of the recent ChucK development “renaissance” alongside a growing user community, now entering its third decade. The time, as the ChucK slogan goes, is now. What began as a practical project has expanded into broader reflections on tool-building, service, and community. As we labored on what seemed like a “no-brainer” tool that everyone wanted but that no one wanted to build, questions arose: “how did we get here?”, “what is the role of service-based tool-building in our field–and what, if any, is its research value?”—in short, “should we even write a paper about a package manager?”. Meanwhile, we couldn’t help but notice that the act of creating a package manager seems to unify not only disparate software fragments, but also something of community. In other words, there may be more than meets the eye. This paper chronicles the making of a package manager and all that goes along with it. This is the story of ChuMP.",nime2025_280.pdf,,,280.jpg,,,
282,paper,poster,medium,posters-3,,5,remote,Synthesizing Music with Logic Gate Networks,Ian Clester,"Small digital circuits consisting of basic logic gates (AND, XOR, etc.) are capable of generating surprisingly complex musical output. In this paper, I present physical and web-based interfaces for exploring the space of audio-generating logic gate networks and 'bending' such networks via touch (or mouse) gestures to interfere with their operation and change their output while they are running. This work follows in the vein of bytebeat practices, in which music is generated by short code snippets at the level of individual audio samples, but takes things further by relying on an even lower-level form of computation. In addition to presenting our system, I offer some preliminary analysis of why these logic gate networks tend to produce musical output.",nime2025_282.pdf,,,282.jpg,,,
284,paper,poster,long,posters-3,,5,in person,SMucK: Symbolic Music in ChucK,Alexander Han; Kiran Bhat; Ge Wang,"SMucK (Symbolic Music in ChucK) is a library and workflow for creating music with symbolic data in the ChucK programming language. It extends ChucK by providing a framework for symbolic music representation, playback, and manipulation. SMucK introduces classes for scores, parts, measures, and notes; the latter encode musical information such as pitch, rhythm, and dynamics. These data structures allow users to organize musical information sequentially and hierarchically in ways that reflect familiar conventions of Western music notation. SMucK supports data interchange with formats like MusicXML and MIDI, enabling users to import notated scores and performance data into SMucK data structures. SMucK also introduces SMucKish, a compact high-level input syntax, designed to be efficient, human-readable, and live-codeable. The SMucK playback system extends ChucK’s strongly-timed mechanism with dynamic temporal control over real-time audio synthesis and other systems including graphics and interaction. Taken as a whole, SMucK’s design philosophy treats symbolic music data not only as static representations but also as mutable, recombinant building blocks for algorithmic and interactive processing. By integrating symbolic music into a strongly-timed, concurrent programming language, SMucK’s workflow goes beyond data representation and playback, and opens new possibilities for algorithmic composition, instrument design, and musical performance.",nime2025_284.pdf,,,284.jpg,,,
291,paper,poster,short,posters-3,,5,remote,Melia: An Expressive Harmonizer at the Limits of AI,Matthew Caren; Joshua Bennett,"We present Melia, a harmonizer-like digital instrument that explores how common failure modes of machine learning and artificial intelligence (ML/AI) systems can be used in expressive and musical ways. The instrument is anchored by an audio-to-audio neural network trained on a hand-curated dataset to perform pitch-shifting and dynamic filtering. Biased training data and poor out-of-distribution generalization are deliberately leveraged as musical devices and sources of instrument-defining idiosyncrasies. Melia features a custom hardware interface with a MIDI keyboard that polyphonically allocates instances of the model to harmonize live audio input, and integrated hardware controls that manipulate model parameters and various audio effects in real-time. This paper presents an overview of related work, the instrument's hardware and software, and how audio-to-audio AI models might fit into the long-standing tradition of musicians, artists, and instrument-makers finding inspiration in a medium's shortcomings.",nime2025_291.pdf,,,291.jpg,,,
300,paper,poster,medium,posters-3,,5,in person,GraviTone: A Tangible Musical Interface using Gravity Well for Sound and Music Creation,Kratika Jain; Allwin  Williams; Akhilesh Kumar Bhagat; Sukanth K; Arunav Rajesh; Prashant  Pal; Krishanan Chandran; Rajashekhar  V S; Anandu Ramesh; Gowdham Prabhakar,"We propose a new musical interface, “GraviTone” that produces sounds and musical compositions via interaction with spherical objects in a gravity-well benchtop setup. The interface consists of several spherical objects in different colours orbiting around the centrally placed static object on the spacetime fabric. The spherical objects are launched at a certain angle from the setup's periphery, and the objects' motion is tracked by an overhead camera and mapped to different parameters to generate sound. In GraviTone, users can experience sounds generated from objects’ parameters and have control over different configurations. We use the setup to send Open Sound Control (OSC) signals and Musical Instrument Digital Interface (MIDI) signals to map different sounds and musical scales (Indian classical ragas and Western classical scale). We mapped the moving objects’ parameters to control synth parameters, VSTs, and DAWs. Users can also route generated MIDI data into DAWs using preset configurations or customizing their frameworks for sound generation. We also generate real-time visuals corresponding to the object's movements for further immersion and interactivity. This integrated interface combines various domains like musical mathematics, sonification techniques, sound synthesis, and sound design for live music creation and real-time audiovisual composition.",nime2025_300.pdf,,,300.jpg,,,
43,music,Remote Performance,,concert-1,,5,Remote,State Change: Merging Adaptive Music Technology with Disability Aesthetics,Molly Joyce,"State Change is a forthcoming album that transforms surgical records into musical lyrics and uses aural material generated by accelerometers and motion capture systems. By intertwining the medical and the aural, the album offers a sonic exploration of acquired disability, transforming personal and embodied experiences into a musical language. My interest in disability is rooted in my own lived experience. At the age of seven, I was involved in a car accident that nearly resulted in the amputation of my left hand. This life-altering event set me on a journey of adapting my relationship with music, leading me to experiment with different instruments and, more recently, to explore music technologies that align with my physical capabilities. These include accelerometers and motion capture devices, which have become central to my creative process. The featured track from the album, August 6, 1999, utilizes the MUGIC (Music/User Gesture Interface Control) device, an innovative motion sensor developed by violinist and composer Mari Kimura. The MUGIC translates hand movements into software parameters: “rotation” (pronation and supination) triggers high-frequency sine tones, while “yaw” (vertical axis rotation) triggers low-frequency sine tones. This track integrates lyrical text drawn from the surgical records of my first post-accident operation, alongside layered vocal elements that convey the emotional weight of that experience. Together, these elements create a deeply personal and evocative sonic representation of my journey with disability.",nime2025_43.pdf,,,43.jpg,,,
162,music,Live Performance,,concert-1,,8,In person,(un)Stable (dis)Connection,Misagh Azimi,"(un)Stable (dis)Connection is an improvised performance that merges a fine-tuned text-to-audio latent diffusion model with traditional improvisational techniques in Ableton Live. Acting as a co-creative agent, the AI model instantly generates fresh audio fragments in response to short textual prompts provided by the performer. These fragments are seamlessly imported into Ableton using Max for Live, where they are manipulated and used to improvise in real-time. By fine-tuning an open-source text-to-audio model on the author’s own recordings, the piece achieves some degree of stylistic coherence without sacrificing unpredictability. (un)Stable (dis)Connection highlights the creative potential of ethically sourced, open-source AI in music performance, foregrounding human–machine synergy as both a technical and artistic frontier.",nime2025_162.pdf,,,162.jpg,,,
178,music,Live Performance,,concert-1,,???,In person,Carpet Entanglement,Joseph Burgess,"Textile production, music, and technology are historically entangled. This performance explores the long-standing interaction between textile arts and musical practice through the carpet tufting gun as a novel electroacoustic interface. This historical connection underscores the deep-rooted interplay between material craft and sonic expression, which has continued to evolve alongside technological advancements. Carpet entanglement emphasises  the ongoing generative interplay between media and mediums highlighting the tufting gun as a site of embodied, material engagement.

By leveraging the tufting gun’s distinctive acoustic properties and electromechanical kinetics, this work reimagines the tool as a vehicle for structured musical composition and improvisation. The performance transforms the gun’s utilitarian form and mechanical expressive qualities—such as its rhythmic firing, material resonance, and tactile feedback—into a dynamic paletteof itinerant musical gestures. The tufting gun serves a dual role: as an acoustic sound source and as an interactive performance interface, bridging textile craft and experimental sound art.

Carpet Entanglement reinforces the existing connection between textiles, technology, and music, highlighting the tufting gun’s potential as a tool for artistic expression while situating it within a broader historical and technological narrative. Through this performance, the tufting gun becomes a mediator between the tactile, material world of textile production and the ephemeral, sonic world of music, embodying the interwoven processes of making and performing.",nime2025_178.pdf,,,178.jpg,,,Concert 1
238,music,Live Performance,,concert-1,,10,In person,{riversynth},Leah Barclay; Toby Gifford,"{riversynth} is both an immersive performance and musical interface that mixes live hydrophone streams from six river systems with live processing through a gestural controlled water instrument. Building on a decade of research from the River Listening project, {riversynth} creates an entangled performance ecosystem where unpredictable aquatic soundscapes become both the source material and score for a live performance. The interface consists of a transparent tank filled with water and embedded sensors that detect water movement and light. These parameters modulate, filter and mix the live hydrophone streams via the performer’s hands in the water, creating a dynamic relationship between the real-time river soundscapes and the performer's physical manipulation of water. {riversynth} demonstrates multiple layers of entanglement: temporal (connecting historical River Listneing research with real-time audio), spatial (linking geographically distant river ecosystems in a spatial performance environment), and material (using water to control and mix aquatic soundscapes). This multilayered approach directly engages with NIME 2025's theme by exploring how musical interfaces can bridge environmental monitoring with live artistic experimentation. The technical implementation combines custom-designed sensors, a low-latency streaming network, and a granular synthesis performance tool for live manipulation of the hydrophone streams in surround sound. The interface enables intuitive control while maintaining complexity in the sonic output, allowing for both composed sections and improvisatory responses to the unpredictability of live streams. {riversynth} creates an entangled network where environmental data becomes musical material, and human gestural control is mediated through live engagement with the hydrophones. This creates a feedback loop between performer, technology, and environment that emphasises our interconnected relationship with aquatic ecosystems and their health. While the performers appear to have agency through their gestural engagement with the water, it is ultimately the live river soundscapes that dictate and control how the performance will unfold, mirroring the unpredictability of the natural environment. 
",nime2025_238.pdf,,,238.jpg,,,Performance 1
292,music,Live Performance,,concert-1,,12,In person,Playing the sound image – Individual immersive sound performance by a performer wearing parametric speakers -,risako shibata; miki kanda; kenta tanaka; ryoho kobayashi; yuta uozumi; shinya fujii,"This is a sound performance in which dynamic and improvisational spatial movement of sound is created by a performer wearing parametric  speakers that have the characteristic of directing sound.

The direct sound from the parametric speakers has a super narrow directivity, which creates the sensation that the sound field is close by, as well as tactile auditory stimulation. It is also easily reflected by walls and objects, creating a virtual sound source on the spot.

In this work, parametric speakers are attached to both palms and shins of the performer and made wireless, making the sound source mobile. The composer also remotely selects and adjusts the parameters of the sound source played from the parametric speakers.

This allows the performer to move between the audience, changing the distance and direction from the  audience and the wall, creating a movement  of the sound image. In addition, the switching between direct and reflected sound creates an auditory experience in which the localization changes abruptly.

In this work, the choreography of the performers and the music are composed in parallel, for example, by taking advantage of the changes in pitch created by the high-speed movement of the parametric speakers.
The movement of the performer changes the hearing, response, and relationship between the performer themselves, the composer, and the viewer, bringing to light a more complex sound source movement and improvisational acoustic experience.",nime2025_292.pdf,,,292.jpg,,,Drama Theatere
17,music,Remote Performance,,concert-2,,21,In person,Interstellar,Ivica Bukvic; Thomas Tucker,"""Interstellar"" is the latest work co-created by the members of the L2Ork Tweeter International Ensemble. Led by its founder and Director Dr. Ivica Ico Bukvic, the performance features live performers over 5,000 miles apart and integrates projection mapping co-developed by a visual artist Thomas Tucker and Bukvic. Tightly integrated sync of the ensuing telematic electronic music that blends EDM and Ambiental is made possible using L2Ork Tweeter free and open-source software platform that also interfaces with the MadMapper software responsible for the visual projection mapping. ""Interstellar"" is commissioned by the Alexandria VA Office of the Arts. It is inspired by StudioKCA's ""Interstellar Influencer (Make an Impact)"" installation on display in Alexandria's Waterfront Park. Like the installation, this piece tells the story of an asteroid whose impact shaped Chesapeake Bay over 35 million years ago. L2Ork Tweeter International Ensemble members who originally co-created the work were (listed in alphabetical order): Ivica Ico Bukvic (Director, on site), Uma Futoransky (Buenos Aires, Argentina), Gala Gonzalez Barrios (on site), Justin Kerobo (on site), Joaquín Montecino (Buenos Aires, Argentina), Jacob Alan Smith (North Carolina), Lauti Sosa (Buenos Aires, Argentina), Caden Vandervort (Virginia), and Lane Wills (Virginia). The final roster of community members who will perform at NIME 2025 will be announced at the concert. They are joined by a visual projection artist Thomas Tucker.",nime2025_17.pdf,,,17.jpg,,,
79,music,Remote Performance,,concert-2,,6,Remote,Diffractive Constellations: A Modular System for Acoustic Violin Programmed in Max/Gen,Seth Thorn,"Diffractive Constellations is a live performance and research project that combines an acoustic violin with a set of bespoke Eurorack modules programmed in Max/Gen and embedded on the Daisy Seed SOM. By capturing, transforming, and re-sequencing violin gestures, these modules operate as compositional fragments that can be patched together in myriad ways. Each module began as a “composition made parametric,” evolving through hardware constraints to support improvisatory practice. This document details the technical design and performance methodologies of six custom modules—called Arvo, Glacial, Detritus, Volution, Widgets, and Forest—highlighting how each processes violin-derived or other input to create layered, time-stretched, and/or autonomous sonic behaviors. These modules collectively shape a system in which performer and electronics co-create emergent musical structures.",nime2025_79.pdf,,,79.jpg,,,
257,music,Live Performance,,concert-2,,7,In person,Breathing I,Sophie Rose,"Breathing I is the first in a three-part series (Breathing I-III) that externalizes emotional responses to traumatic experiences through breath and bilateral coordination [1], [2]. Bilateral movement techniques, widely used in trauma therapy [3], [4], engage both hemispheres of the brain, promoting bodily unification and focused attention [5], [6], [7]. This work integrates trauma-informed movement practices with wearable gestural music technology to explore sonic representations of psychological states in a multi-channel spatial audio environment. The performance sonifies panic through asymmetric arm and hand movements, breath-based vocalizations, and visual projections that depict physiological dysregulation. Datagloves capture movement data, modulating live and sampled breath sounds to create a dynamically evolving soundscape. Visual projections, generated in real-time, use torus meshes that expand and contract — an analogy to hemoglobin’s role in oxygen transport (see Fig 1.). The integration of movement, sound, and visuals reinforces the connection between breath, blood flow, and the body’s autonomic responses. As Adriana Cavarero notes, “Nothing more than the act of breathing is able to testify to the proximity of human beings to one another; nothing else better confirms their communication…” to signify the essence of being alive [8, p. 31]. Breathing I amplifies this concept, transforming breath into both a personal and collective sonic expression.",nime2025_257.pdf,,,257.jpg,,,
258,music,Live Performance,,concert-2,,10,In person,Breathing III,Sophie Rose,"“Time is the objectification of a biological organism’s act of breathing, which is sensitive and conscious” [1, p. 25] Breath is more than survival—it is memory, movement, and connection. Breathing III is the final piece in a three-part series that externalizes emotional responses to trauma, exploring the body’s search for reconnection and resolution. Through midline-crossing and sequenced movement patterns, the performer traces a path from psychological fragmentation to unity. MiMU datagloves capture gestural movement in real time, transforming hand and arm motions into a layered soundscape of live vocal sampling. Open-hand gestures extend outward and draw inward, mapping to sustained melodic lines, while bird-like formations—fingers flexing and unfolding—trigger harmonic layers and spatialized echoes. Prayer-hand movements send reverberant waves through the sound field, reinforcing themes of unity and self-reintegration. Real-time particle cloud projections, generated from movement data, swirl and converge around the performer, evoking shamanic ritual and animist traditions. As breath, movement, and sound merge, Breathing III invites the audience into a space of transformation—where embodied memory dissolves, and a new sense of self begins to take shape.",nime2025_258.pdf,,,258.jpg,,,
307,music,Live Performance,,concert-2,,7,In person,Gaia is hanging by a thread...,Taurin Barrera,"Gaia is hanging by a thread... is an interactive audiovisual performance that explores the delicate balance between technology, humanity, and our planet’s ecosystem. This approximately 6-minute performance aligns primarily with solarpunk aesthetics, offering a critical contemplation of electronic art, sustainability, and collective eco-consciousness through audience interaction and connection.

At the heart of this performance is the Solar Punk Console, a solar-powered capacitive synthesizer that embodies the solarpunk ethos of sustainable technology and community interconnectedness. This synthesizer not only harnesses solar energy but also invites audience participation, as conductive threads connected to the Solar Punk Console’s synthesis control inputs are deployed into the audience during performance. Participants are encouraged to hold, manipulate, or pass these threads, forming a complex electronic circuit within the performance venue. This interaction directly impacts both the audio output and the visual atmosphere of the performance space using audiovisual processing software created in Max. This piece utilizes feedback in both audio and visual systems to blur the lines between performer, audience, and environment, as collective actions of the audience shape a interwoven synesthetic sensory experience in real-time.",nime2025_307.pdf,,,307.jpg,,,"25, 26, not 27"
40,music,Live Performance,,concert-3,,16,In person,Sonic Crucible Entanglements,Jordan Lacey; Toby Gifford; Mick Harding,"Sonic Crucible Entanglements is an ongoing ‘new materialist’ (Bennet 2010) investigation of singing bowls that critiques new age narratives (Barnett 2010; Brown 2020) by exploring the ‘spiritual’ potential of inter-cultural material-led collaborations. This live singing bowl performance, also featuring vocal and visual projections, will immerse audiences in an atmospheric cultural encounter combining performance, visuals, sound art and instruments.  Taungurung Kulin artist Mick Harding will present a contextual introduction that explores the experimental approach of the collaboration. The artists acknowledge the Ngunnawal and Ngambri peoples, the traditional custodians of the Canberra region. Our intention of bringing smoke to this performance is not as a welcome to country or as an acknowledgement, but rather to introduce the element of smoke into the performance alongside light, sound, air and voice as part of our exploration of the vibrancy of materiality. Through a song of generosity, knowledge and sharing sung and performed by Mick Harding we embrace Indigenous Research Methods through a combination of traditional and new technologies of sound (Wilson 2008).",nime2025_40.pdf,,,40.jpg,,,Performance #0 (must be outside)
89,music,Live Performance,,concert-3,,10,In person,Housework Commons: Textile Rhetorics II,Jocelyn Ho,"Housework Commons, a feminist activist project under the Women’s Labor initiative, transforms domestic tools into Embedded Acoustic Instruments (EAIs) using sensor technologies. It reimagines unpaid, undervalued domestic labor—traditionally private—as a shared act of activism, addressing global gender inequality in domestic work through public engagement with gendered objects.
Housework Commons includes two custom instruments:
1.	Embedded Iron v.3, based on an early-20th-century ironing board and iron, uses machine learning and sensors to alter pitch based on the iron’s position and sound quality (timbre) depending on fabric color and texture. The board acts as a resonator with a transducer and speaker.
2.	Rheostat Rotary Rack, inspired by a rotary dryer, features rheostats, a rotary encoder, and an 8-speaker base. Hanging clothing triggers pitches based on weight, while rotating the rack by hand or wind adds select frequencies.

The performance of new composition, Textile Rhetorics II for two performers, will feature Embedded Iron and Rheostat Rotary Rack with other objects from the domestic sphere. Central to Textile Rhetorics are woven textile scores and fabric banners that contain living testimonies from women. These testimonies were collected during a past workshop with a mothers’ group, and additional banners will be created during the proposed NIME workshop (separate application). These create a “living archive” installation, a dynamic site where the performance unfolds.
",nime2025_89.pdf,,,89.jpg,,,
116,music,Live Performance,,concert-3,,6,In person,Echoes of Nature's Heritage - Composed for a custom-made data-driven instrument,Yixuan Jin,"Inspired by the experience of orthodontic treatment, this piece explores the similarities between the alignment of teeth and the pursuit of environmental harmony. Utilizing a data-driven musical instrument, the performance captures the balance between nature and technology, echoing the stages of free growth, intervention, and equilibrium. As the performer's movements shape the soundscape and visual transformations, the audience contemplates the dynamic tension and potential harmony between the natural world and the mechanical systems we create. It reminds us of our responsibility to seek sustainable balance, promoting the coexistence and mutual prosperity of nature and innovation, paving the way towards a greener future.",nime2025_116.pdf,,,116.jpg,,,
182,music,Live Performance,,concert-3,,20,In person,"My sunset is your sunrise, yet we touch",Dirk Stromberg; Jocelyn  Ho; MIchal Seta,"In “My sunset is your sunrise, yet we touch,” we propose a three-way hybrid-telematic improvisation, by three displaced musicians, featuring 2 DMIs, and an acoustic-prepared piano. The musicians are located in Montreal, Singapore and Canberra at the NIME conference. This performance explores the concept of a multimedia meta-instrument, a collaborative process that integrates audio, video, projection, and lighting into a cohesive whole. The meta-instrument redefines presence and interaction, challenging traditional notions tied to physical co-location. Our approach draws on Karen Barad’s concept of touch as an entangled, relational act. Here, touch extends beyond the physical to encompass the mediated interplay of sound, visuals, gestures, and light across networked spaces. In this environment, the tactility of capacitive keys, the resonances of the prepared piano, and the shifting gestural logic of controller-based sound art merge into a dynamic, co-constitutive system. This telematic improvisation demonstrates how the tactile interactions with instruments, the resonances of sound, and the dynamic interplay of light and video create new forms of connectivity and shared expression across a blurring of physical and virtual spaces. Through this meta-instrument, we aim to push the boundaries of telematic collaboration and artistic interaction.
",nime2025_182.pdf,,,182.jpg,,,
183,music,Live Performance,,concert-3,,8mins,In person,Becoming,Lilian Zhao,"Becoming' recontextualizes a custom playable device as a living entity that responds to both interaction through an analog interface, and audiovisual stimuli detected from the surrounding environment. Using adaptive animations and ambient soundscapes, the piece creates a volatile space that evokes thoughts of transformation, growth, and destruction. It tells the story of a machine grappling with fate and embraces human nature through the eyes of something artificial. The visual component arises through layers upon layers of blood red lines that coalesce into abstract shapes, mirroring processes of growth and decay. The accompanying soundscape layers mechanical noises with ambient sounds like human voices from the surrounding space, emphasizing the tension between artificial systems and organic environments. Through its unpredictable behavior, the work challenges the audience to reflect on questions of agency, determinism, and the fluid nature of identity.",nime2025_183.pdf,,,183.jpg,,,
194,music,Live Performance,,concert-3,,11,In person,We’re in this together: Expanding Interactive Sonic Spaces,Davor Vincze; Maurice Oeser,"We’re in this together builds on the technical and conceptual frameworks of the Freedom Collective web app but transposes its application from a theatre setting into performance for solo electronics. Hence, the videos provided serve as documentation of previous performances exploring similar concepts; they offer reviewers insight into the musical aesthetics and the functionality of the web-app technology but do not constitute representations of the specific project proposed here. The interaction design leverages the variability of smartphone audio latency and quality to generate distributed sound fields that resonate in real-time with the performance's central sonic themes (e.g. granulation, density, layering and spatial sound distribution). Ideally, the performance space should ideally be set up with an immersive ambisonic sound system for supporting low-frequency content and reinforcing the performer’s electronic sounds. Audience smartphones act as distributed sound sources, enabling a shared participatory instrument. This distributed design also mitigates the need for specialized hardware, making the experience accessible across diverse demographics and venues. In addition to the sonic engagement, the interface includes simple visual prompts on participants' screens, aligning with the performance’s dramaturgy. These features encourage active yet unobtrusive engagement, ensuring that the audience's role complements, rather than distracts from, the sonic focus.",nime2025_194.pdf,,,194.jpg,,,
274,music,Live Performance,,concert-4,,11,In person,Adaptive Elusion,Palle Dahlstedt,"A continuation of my experiments with minimal algorithms, investigating how small an interactive musical algorithm can be and still invoke the feeling of ""somebody there"". Here, a small set of adaptive algorithms react to a live pianist, trying to imitate, elude and counteract his playing, while at the same time being completely dependent on it as a source of patterns and sounds. The piece explores real-time training as a primary modus of interaction, in a cat-and-mouse game of sorts. It is also an example of what I call entangled musicianship. What the pianist plays is a reaction to what the algorithm plays, and at the same time shapes the future playing of the algorithm, hence entangling performance and control. The musical response is generated by a small machine-learning algorithm that starts empty and is trained in real-time on what I am playing. It can also gradually forget what it has learnt.",nime2025_274.pdf,,,274.jpg,,,
275,music,Live Performance,,concert-4,,15,In person,What We Do (Differently) Together,Palle Dahlstedt,"In this performance-lecture, the pianist performs and improvises on (or with) five different interactive algorithms, and simultaneously discusses how it feels playing with them, how their different interactive qualities affect the musical outcome, and what the human-machine situation does to us. It is a statement about the essence of process, the nature of agency, and what different types of algorithms bring to human creative process, told from a situation of being entangled with the algorithms, while trying to make music.",nime2025_275.pdf,,,275.jpg,,,
302,music,Live Performance,,concert-4,,6,In person,Mulholland Revisited,Héloïse Garry,"This project explores the intersection of real-time algorithmic sound generation and live piano performance through ChucK and MIDI-based interaction. Inspired by the interplay between dream and reality in David Lynch’s ""Mulholland Drive"" (2001), the piece utilizes a series of ChucK scripts that dynamically respond to the pianist’s input, shaping an evolving soundscape. The system employs MIDI-triggered processes to generate and manipulate electronic textures in real-time, expanding the piano’s expressive range beyond its traditional acoustic boundaries.

The work is structured around four key sonic gestures, each corresponding to a moment in ""Mulholland Drive"": (1) a synthesized telephone bell signifying Diane’s psychological rupture, (2) a dynamically generated arpeggio that mirrors the tension of her conversation with Camilla, and (3-4) progressively complex textural layers that blur the line between live performance and algorithmic sound synthesis. 

By integrating reactive electronic sound with live piano, this piece demonstrates how real-time programming can transform the piano from a traditional interface into a multidimensional, interactive system. ",nime2025_302.pdf,,,302.jpg,,,
30,music,Live Performance,,concert-5,,7,In person,MSN/AV (Maximum Silence to Noise/Audio-Visual),John Ferguson; Andrew Brown,"MSN/AV (Maximum Silence to Noise/Audio-Visual) is an interactive audiovisual system performed by two musicians. The sound world is created through ring modulation synthesis controlled by multi-dimensional touch gestures. This approach provides a rich diversity of sonic potential whilst maintaining clear remnants of physical gestures. Each musician uses a ROLI Lightpad Block and an iPad running TouchOSC to control bespoke software written in Pure data. Interaction data from each performer is passed to a Touch Designer network which generates and/or manipulates visual materials. MSN/AV celebrates physical interplay with gestural interfaces and situates live improvisation within a responsive audiovisual environment. Sensor data is used to generate sound and graphics in real-time, aiming for an audiovisual entanglement that provides sonic and visual connections ranging from the gentle steering of musical improvisation to more autonomous and potentially disruptive behaviour. At the heart of MSN/AV is a novel implementation of ring modulation that features two identical oscillators. These oscillators morph from silence to white noise, passing through sine and square waveforms along the way. This morphing capability enables a smooth transition from simple to complex audio spectra.

MSN Synthesis
The term ‘maximum silence’ refers to the absence of sound in a ring modulator when either modulating oscillator is silent. To maintain modulation independent of the amplitude envelope, a DC offset ‘silence’ signal is introduced, ensuring consistent gain as the modulating oscillator's amplitude decreases. A ROLI Block serves as the primary gestural controller for MSN/AV. Its touchpad maps silence-to-noise transitions on the x-axis and oscillator pitch on the y-axis, while pressure controls the amplitude envelope. Two oscillators, each controlled by a finger on the multi-touch pad, are ring modulated, offering six dimensions of control for expressive sound shaping. An iPad running TouchOSC provides on-screen sliders and buttons to control audio effects and, when applicable, visual parameters. These include volume, panning automation, delay, and reverb.

Research Question(s) 
The goal is to explore gestural control of sound and vision while creating new artistic work that engages with rich audio spectra and the entanglement within audiovisual practices. This expressiveness in gesturally controlled ring modulation synthesis builds on a history of ‘noisy’ interactive electronic instruments like the Crackle Box , BoardWeevil, and Atari Punk Console, which use ring modulation or similar techniques to produce dynamic inharmonic spectra. While ‘live visuals’ are now common in contemporary performance, the role of the musical score is evolving. Yet, aside from a few notable exceptions (Louise Harris, Myriam Bleau, Vicki Bennett, Nonotak Studio), there is little artistic research on improvising musicians in responsive audiovisual environments—an area MSN/AV seeks to explore.
- Our primary research questions are: 
- To what extent can improvised performance with interactive technology be successfully situated in a responsive audio-visual environment? 
- What creative opportunities emerge when ‘musical improvisation and ‘live visuals’ combine in an environment where the line between cause and effect is blurred? 

Audiovisual Context
A useful starting point is visual music: Oskar Fischinger’s (1938) An Optical Poem is an early example that William Moritz (1986) has used to suggest that ‘[s]ince ancient times, artists have longed to create with moving lights a music for the eye comparable to the effects of sound for the ear.’ [4] However, many early exemplars work with fixed musical and visual resources, whereas MSN/AV focuses on improvised live performance in interactive environments. The real genesis might therefore be a work like John Cage’s Variations V (1965), which was created in collaboration with the Merce Cunningham Dance Company and made use of shadows cast on walls to trigger sound via light sensors. This resonates with what Simon Waters (2013) has termed ‘Touching at a Distance’ [5] and might also be considered via John Whitney’s (1994) notion of ‘audio-visual complementarity’. [6] These are all useful historical ideas that have helped develop this project. More recently, Cat Hope and Lindsay Vickery (2010) have furthered discussion of ‘The Aesthetics of the Screen-Score’ [3] and it is clear their Decibel ensemble is a leader in this field. Similarly, Louise Harris (2021) has examined the nature of audio-visual experience [2] with a particular emphasis on media hierarchy; our work builds upon her 2016 article ‘Audiovisual Coherence and Physical Presence’ [1], which emphasises the relationship between audio-visual media and physical (human) presence.",nime2025_30.pdf,,,30.jpg,,,
95,music,Live Performance,,concert-5,,12,Remote,The Walkable Instrument: Modular Patches as Entangled Environments in OpenSoundLab,Ludwig Zeller; Anselm Bauer,"What happens if a modular synthesizer patch is no longer constrained to a flat, rigid configuration, but can instead be copied infinitely (and at no expense) and arranged in midair all around the performer? The performance showcases two modular patch designs assembled in OpenSoundLab, a mixed-reality patching system for Meta Quest that was developed in previous research. This platform allows users to freely position basic sound modules (oscillators, samplers, sequencers, effects, etc.) throughout real-world environments. 

A long-standing challenge in electronic-music performance—whether you’re working with hardware synths, modular rigs, software patches, or full DAW setups—is that the patch architecture and live tweaks remain opaque, even to expert listeners. The system is often too small, too cluttered by patch cables, or simply too complex and hidden in menus, sub-patches, or binary code to be grasped visually. When a musician plays a traditional keyboard or a self-built NIME-style instrument, the audience can readily perceive timing, effort, and skill. In classic synth-tweaking shows, however, such clear audiovisual cues are largely absent. OpenSoundLab resolves this by letting performers position a modular patch at any scale and in any arrangement within the stage space. Freed from rigging and gravity, a modular setup can expose its inner sonic architecture in three dimensions, with less-critical modulation oscillators tucked behind larger, more prominent elements. Multichannel components become visibly entangled around the performers, spreading out in front of—or even encircling—them to map the patch’s full sonic possibility space.

During the performance, Anselm Bauer will present two patches that illustrate this concept. Both pieces inhabit the realm of abstract glitch techno. The first features tightly defined sound structures laid out as a clearly readable tree of audio channels and modulations (see figure 2), while the second explores generative, self-modulating processes, resulting in an even more entangled, rhizomatic patch topology.",nime2025_95.pdf,,,95.jpg,,,
171,music,Live Performance,,concert-5,,6,In person,EDO Artifacts,Gregg Oliva,"EDO Artifacts is a live-performance piece for a computer-sequenced modular synthesizer to explore equal division of the octave (EDO) tuning systems. The composition is written prior to performance using the ChucK music programming language. During the performance, a computer running this program interfaces with the modular synthesizer, converting signals from the code into voltage to drive oscillators and amplifiers in the system. The flexibility offered by the programmatic composition supports complex arrangements of phrases, sections, and modulating tuning systems, while the modular synthesizer provides the performer precise control over the sound through real-time manipulation of timbre-shaping parameters. In this way, ChucK acts as the “orchestration” or the “brain”, whereas the modular synthesizer is the “instrument” or the “body” of the piece. There are four sections—labeled as fragments—each written using a different EDO tuning: 5EDO, 7EDO, 31EDO, and 15EDO, respectively. Each tuning has been selected to suit the stylistic and textural qualities of its respective fragment, shaping both the compositional approach and the resulting sonic character. The fragments are purposefully brief, serving as previews of the musical potential of each tuning.",nime2025_171.pdf,,,171.jpg,,,26th black box
265,music,Live Performance,,concert-5,,20,In person,transcriptions,Takuma Kikuchi; Riki Saito; Risako Shibata; Atsuya Tsuchida; Kenshiro Taira; Nimisha Anand; Ryoho Kobayashi; Yuta Uozumi; Shinya Fujii,"“transcriptions” is an improvisational musical performance piece in which two performers recursively mimic each other's movements, utilizing postural sensing and tactile feedback. The misalignments and errors that occur in the process of imitation and the dynamic changes in the relationship between the performers caused by the interveners (the System Jockey and the Intervener) are intertwined to produce unpredictable movements and sounds. Performers wear special suits equipped with gyro-sensors and exciters. The gyro-sensor converts the postural movement data into vibrations, which are transmitted to the exciter of the other performer. The performers respond to these vibrations and move, mimicking each other's postures. As this chain of imitation is repeated, errors and misalignments due to tactile perception, physical ability, initial position, differences in posture, spatial constraints, and other factors accumulate, and new movements emerge. In this work, all sounds are generated from the performers' movements. Contact microphones are attached to the performers' suits to capture the sound generated by their movements. The sounds are processed and output, and electronic sounds are generated from its volume information, so that the relationships between the performers and the changes in their movements can be expressed sonically. The relationships among the performers, and the interaction between the system and the performance environment generate the performers' movements and sounds, and the system jockey and the intervener intervene in these interactions. The result is an improvisational performance in which nonlinear changes in movement and sound are intertwined with intentional control. “transcriptions” is a work that presents a new form of improvisational expression by actively utilizing creative emergence through chains of imitation.",nime2025_265.pdf,,,265.jpg,,,
310,music,Live Performance,,concert-5,,8,In person,Echoic Defiance of Gravity,Ji Won Yoon; Woon Seung Yeo,"""Echoic Defiance of Gravity"" is a visual music piece featuring ""Bouncy Echo""—a virtual audiovisual instrument based on a computer simulation of the motion of bouncing balls, showcasing its potential as a versatile, expressive tool for real-time music performance. Here, the simulation not only governs the motion of the balls in the visual domain but also determines the musical outcome of the piece in the auditory domain, thereby providing a unique cross-modal experience for both the audience and the performer. In addition to the fact that this piece may invoke intriguing discussions in terms of simulation-based algorithmic composition, it is especially noteworthy that it is a unique amalgamation of a fixed, deterministic set of musical notes and partly uncertain, indeterministic follow-ups generated by computer simulation in the visual domain. Although the exact time and the frequency of occurrence of musical events are chance-based and effectively random, they are perfectly relevant to the motion of the ball presented simultaneously. Regarding the musical organization of the piece, the artists focus on exploring a wide range of parameter combinations, keep evaluating the musical versatility of the system employed, and hope to bring more unique outcomes to the stage at the conference than those featured in the accompanying video.",nime2025_310.pdf,,,310.jpg,,,
126,music,Remote Performance,,concert-6,,9,In person,Kamer/Ton – a system for analog audio-video feedback,Krzysztof Cybulski,"Kamer/Ton is an experimental audio-visual performance tool, based on audio and video feedback phenomena combined into a single feedback loop. It consists of a couple of devices: two custom-made cameras are directed at a CRT monitor - the picture is transformed into sound, which is consequently sent to a speaker; a microphone directed towards the speaker is in turn connected to a custom-made video synthesizer, producing images displayed on the monitor. The resulting audio-visual feedback can be interacted with by modifying camera and microphone positions and parameters, which influences simultaneously generated, perfectly synchronized sonic and visual structures.",nime2025_126.pdf,,,126.jpg,,,
201,music,Live Performance,,concert-6,,11,In person,Diffy,Jordan Shier; Xiaowan Yi,"Diffy is a duo music project comprising a drummer and a sound designer, connected by a set of machine learning-based sound design agents. In this project, we explore and juxtapose a set of three machine learning-based techniques for manipulating the timbral qualities of percussion instruments in real-time with low-latency. These techniques include a neural audio synthesizer trained on non-percussive material, a timbre remapping 808 drum synthesizer, and a modular synthesizer controlled by a neural network. Each sound design agent operates on different modes of timbral understanding -- reacting to the drum performance based on this understanding, and suggesting sonic transformations. Sonic negotiations between the human sound designer and the sound-design agent are relayed back to the drummer, creating a feedback loop that shapes a structured improvisation.",nime2025_201.pdf,,,201.jpg,,,
227,music,Live Performance,,concert-6,,20,In person,Xylocyclos,Alyssa Wixson; Oliver George-Brown,"Our project Xylocyclos features desiccated native branches (Juniperus californica, Yucca schidigera, and Cylindropuntia bigelovii) collected during field research in the Mojave Desert. We attach inexpensive transducers and homemade piezo microphones to create feedback loops within the timber itself. The signal is minimally processed: there are no pre-recorded or synthesized sounds, only the natural frequencies of the wood reinforcing themselves and being amplified through a hand-made cajon. Xylocyclos is a portmanteau deriving from the Greek xylo (wood) and cyclos (cycle). Thinking carefully through the provenance, treatment, and materiality of our salvaged timber, we activate physical and poetic resonances across multiple cyclical scales. The sonic feedback operates at the scale of audible sound-waves. Moving the contact microphone mere millimeters can effect a profound shift in the overall sonic texture. We conceive of this as a sonic reimagination of microfluctuations within the natural environment, where minute variations in desert topology determine the way a rivulet chooses its course; where the play of sun and shadows determines the capacity of a plant to photosynthesize; where a pollinator’s peregrinations determine which plants propagate. At this broader ecological scale, we are also thinking about cycles of life and death: our branches are all biologically dead, all in a transitional phase between aliveness and total decomposition. Transplanted into a foreign environment, we practice an ethics of care in preparing the timber for performance. When reanimating the branches as sonic beings we emphasize their roles as co-performers. They are sonically unpredictable: even adorning them with the target-practice cans we found buried in the desert can generate a profusion of new sonorities and overtones. Rather than attempting to control the branches and their sonic output, we instead afford them a degree of agency, performing a non-hierarchical act of creative collaboration with these organic entities.",nime2025_227.pdf,,,227.jpg,,,27th
243,music,Remote Performance,,concert-6,,10,Remote,Screenless Optical Theremin with Tremolo (ScOTT),Michael Gancz; Justin Berry; Shu Wei; Jake Shaker; Kimberly Hieftje; Asher Marks,"Head-mounted extended-reality (XR) interfaces provide a flexible platform for immersive and embodied musical instrument design. By combining spatial audio, ergonomic first-person gestural control, and networked interactivity, these interfaces can facilitate expressive, interesting, and emotionally resonant performances. Unlike data gloves and hyperinstruments, XR headsets can be calibrated to the physical properties of their individual users. However, these headsets tend to obscure the eyes and other parts of the face, limiting the user’s capacity to establish eye contact and transmit facial expressions. These subtle communicative elements play a crucial role in real-world collaborative musical settings where performers utilize facial cues to negotiate surface parameters such as timing, dynamics, and breath, as well as more complex qualities like atmosphere and interpretive mimesis. In this remote performance, we present the Screenless Optical Theremin with Tremolo (ScOTT), a novel gestural MIDI controller powered by a modified screenless XR headset. ScOTT focuses on the hands and arms, mapping broad gestures to coarse-grained musical parameters (e.g. pitch and velocity) and small movements to more complex musical ornaments (e.g. tremolo width and frequency). In a structured improvisation that highlights the role of ornament in musical texture, we explore the ScOTT’s capacity to balance social presence, embodied interaction, and expressivity.",nime2025_243.pdf,,,243.jpg,,,
293,music,Live Performance,,concert-6,,15,In person,"Improvising with ""GravField"": A Participatory Performance Exploring How Digital Objects Mediate Intercorporeal Movements within Collocated Mixed Reality",Botao Hu; Yuemin Huang; Mingze Chai; Xiaobo Hu; Yilan Tao; Rem RunGu Lin," As mixed reality technologies evolve, they blur the boundaries between the digital and physical worlds, prompting a reevaluation of how we engage with digital objects and mediated environments. This paper investigates the emerging concept of ""digital physics"" in MR, where digital objects, as informational entities, shape both cognitive and somatic experiences. Building on the ""somatic turn"" in human-computer interaction, we explore how the human body, as a site of interaction, adapts to new forms of embodied collaboration in MR spaces. Through the lens of contact improvisation, we introduce GravField, a live performance-based MR system that uses Audio-Visual Virtual Mediators (AVVMs) to facilitate interdependent behaviors among participants. These AVVMs incorporate metaphors such as springs, ropes, and magnetic fields, shaping bodily movement and social dynamics through dynamic, sensory feedback. Drawing from post-phenomenological theories, this research examines the relationships between embodiment, interpretation, alterity, and background in the context of MR. Our findings contribute to the understanding of how digital objects mediate embodied interaction and collective behavior, offering practical insights for designing entangled and embodied experiences in MR environments.",nime2025_293.pdf,,,293.jpg,,,
211,music,Live Performance,,concert-7,,6,In person,O一,Hanyu Qu; Francesco Dal Rí; Hao Zou,"This performance features a Digital Musical Instrument (DMI) that inscribes a linear and a circular conception of time, inspired by Western and Eastern time philosophies. The DMI employs two 3D-printed boards equipped with ESP32 chips for wireless communication and WS2812 LEDs providing visual representation feedback, and interactive boxes, each fitted with a light sensor and ESP32 Mini boards. Such interface is designed to be coupled with a software counterpart for sound generation.The project originates from a collaboration with two composers from diverse cultural backgrounds - one Chinese and one Italian. Through collaborative design and co-composing practice, the proposed DMI emerged as an epistemic tool, promoting cultural understanding and critically highlighting the socio-cultural role of technology. Through such process, the significance of rediscovering time in contemporary globalization and philosophy was explored, challenging the conception of time as a mere measurement parameter and striving to reveal the importance of understanding the role of time across different cultural contexts. This project wishes to expand the constitutive role of musical time, demonstrating its diversity and prompting a reflective layer of the perception of performative and musical time in NIME.",nime2025_211.pdf,,,211.jpg,,,
262,music,Live Performance,,concert-7,,8,In person,Tentacle Orbits,Tara pattenden,"Tentacle Orbits presents a participatory performance composed with malleable, wearable electronic instruments that invite audience engagement through intuitive tactile interaction. The performance features custom-built Tentacle Instruments that respond to physical manipulations such as squeezing and bending, creating an accessible framework for collaborative musical improvisation. These colourful instruments utilise soft circuitry techniques with machine-sewn sensors responding to body contact and pressure, contributing to the growing research area of Textile Interfaces for Musical Expression (TIME). The instrument's sonic capabilities range from bright square waves built with digital logic circuitry to glitchy samplers developed on the Daisy Seed platform. During the performance, audience members are invited to don and play these wearable instruments, with the participation framework adapting to the atmospheric and social aspects of the performance venue. This work contributes to NIME research by addressing identified gaps in non-rigid and deformable musical interfaces through longitudinal development and live performance research. ""Tentacle Orbits"" demonstrates how material properties can provoke intuitive gestures while balancing the simplicity of approach with the complexity of experience, ultimately creating a unique collaborative soundscape that emerges through audience exploration and engagement.",nime2025_262.pdf,,,262.jpg,,,
288,music,Live Performance,,concert-7,,30,In person,Tesseract,Vijay Thillaimuthu,"Tesseract is a live audiovisual work for the superimposition of lasers and video projections as controlled by sound. The same voltage that is amplified as sound, derived from a modular synthesiser and theremin controller, also deflects laser light into visible patterns and becomes the source for real-time video projections. The different display technologies are then mapped together to present a unified experience.

Tesseract is designed to provoke a questioning of the mechanisms of perception, creating interactions between two-dimensional and three-dimensional visualisations. The work is developed in reference to the tesseract, a four-dimensional hypercube beyond our ability to properly visualise, analogous to what a three-dimensional cube is to a two-dimensional square. Tesseract is inspired by notions of quantum processes that take place across dimensions, unable to be perceived, and scarcely understood. We commonly use complex power and communication networks without understanding their processes in totality. For example, transistors use quantum tunnelling while lasers use the stimulated emission process, where photons essentially teleport when interacting with excited electrons (to create laser light). These are phenomena that can only be explained through theoretical physics. This is an ambitious work pushing the possibilities of existing technologies. This work imagines interaction with forces beyond our understanding.
The system has been developed on the principle of Vector Synthesis or Oscilloscope Music, whereby the stereo image of the sound signal articulates the left and right axes of a graphical image on a Cartesian plane. The performer interacts with the system and decides whether to disrupt or accept the generative potential of the system. As both the projections and laser images are derived from the same source, sound voltage, they can be seamlessly connected.",nime2025_288.pdf,,,288.jpg,,,26th black box
4,music,Installation,,"installations-1, installations-2",,N/A*,In person,PANIC!,Benjamin Swift,"PANIC! (Playground AI Network for Interactive Creativity) is an interactive installation that explores the behaviour of connected AI models. Viewers enter text prompts which are transformed as text/audio/images through a ""network"" of generative AI models. Each output becomes the input for the next iteration, creating an endless cycle of AI-mediated transformation.",nime2025_4.pdf,,,4.jpg,,,
38,music,Installation,,"installations-1, installations-2",,N/A*,In person,Synthetic Ornithology,Frederick Rodrigues,"In Synthetic Ornithology, visitors encounter a compelling simulation of how shifting climates might transform Australia’s vibrant avian soundscapes. Powered by a bespoke ML model trained on an extensive archive of birdsong and corresponding climate data, the installation generates future sonic environments based on user-selected scenarios. The resulting soundscapes reflect the richness and complexity of Australia’s ecosystems, while also revealing how these voices may adapt—or fade—under the pressures of climate change. Ultimately, Synthetic Ornithology underscores the profound impact human interventions may have on the soundscapes of tomorrow, inviting reflection on our collective responsibility to preserve these fragile ecological realms.",nime2025_38.pdf,,,38.jpg,,,
52,music,Installation,,"installations-1, installations-2",,N/A*,In person,Traceless,Xuedan Gao; Xinchen Liu,"The Anthropocene, a time defined by human impact since the 1950s, has led to significant environmental changes, often tied to the Earth's declining health. Glacial ice, formed over centuries through the compression of snow into dense, airless layers, symbolizes the vast, slow forces of nature. However, this balance has been disrupted in the past two centuries. Human activities such as fossil fuel combustion, deforestation, urbanization, and industrialization have accelerated glacier melting, contributing to global warming and altering Earth's ecosystems. The contrast between glaciers' slow formation and rapid retreat shows the profound impact of human activities on natural processes.

Traceless is an interactive sound installation with visual projections that invites participants to use handheld controllers for audio and tactile interaction. Ice, a symbol of power in deep time and fragility in the Anthropocene, serves as both medium and collaborator in this work. Traceless includes the installation’s core structure, an interactive audio system, and visual projections. It captures the sounds of ice melting and dripping using contact microphones. The visual projections compare two timescales: the slow, natural progression of glacial formation and the rapid, human-accelerated retreat occurring today. We created animations of various forms of snowflakes combined with the rhythmic sound of dripping water to metaphorically represent the time spectrum of the various stages of glacier formation. To illustrate the impact of human activity on glacier melting, we utilized AI-driven videos generated through Stable Diffusion, along with the Greenland Surface Melt Extent dataset from the NSIDC (National Snow and Ice Data Center). The two visual effects were switched through tactile interactions, bringing different times into the same space.

The interaction emphasizes the harmony between human actions and natural processes. When participants hold the controller, the corresponding musical note will play. When participants speak to the installation, their voices blend with the natural sounds of melting ice, which forms an echo that embraces change as part of the natural system. Human language is not parsed semantically and becomes increasingly blurred in the echo. This is a call-and-response process, where the ice and the system actively respond to the participants in their own rhythm that is not influenced by the participants, forming a mutual dialogue. The sounds metaphorically embed the vibrational frequencies of carbon-based life into the ancient resonance systems of geological formations. The sounds across material dimensions constitute dialogues between geological time and the Anthropocene. 

Traceless allows participants to experience and reflect on the agency of nature within a restricted dialogic framework. The asymmetric interaction exposes the boundaries of human intervention and guides participants to realize, through attempted 'collaboration' with the installation, that symbiosis resides not in technological domination, but in what Donna Haraway calls 'becoming with', an ongoing process of co-creation rather than control. By immersing participants in the multisensory experience, Traceless encourages them to think about the active role of nonhuman entities and to imagine a de-anthropocentric future.",nime2025_52.pdf,,,52.jpg,,,
69,music,Installation,,"installations-1, installations-2",,N/A*,In person,Point Line Piano VR,Jaroslaw Kapuscinski; Marc Downie; Paul Kaiser,"Point Line Piano is a VR project that reimagines the composition, performance, and reception of piano music by fusing its modes of creating, playing, and listening. As you interact with it, your ears, eyes, and hands act in concert. You start by drawing lines freely in the space around you, sparking musical notes that are notched as points on the lines as you draw them. These notes quickly accumulate, forming distinct melodic phrases and rhythms, while the computer generates an intricate audiovisual dance all around you. The work enables a spatial and full-body experience of abstraction not found in any other medium. In a live concert setting it can also be used as an audiovisual instrument.",nime2025_69.pdf,,,69.jpg,,,
145,music,Installation,,"installations-1, installations-2",,N/A*,In person,Glass and Fans: Spatial Infra-Instrument Framework,Melanie Buntine; Anthony Lyons; Heather Gaunt; Eugene Ughetti,"This installation explores the entangled relationships between technology, creativity, accessibility, and participation through Glass and Fans, two interactive spatial sound installations developed using the Spatial Infra-Instrument Framework (SIF). 

Rooted in Percy Grainger’s vision to “blur the distinctions between performance, composition, and listening”[7], SIF transforms everyday materials—such as glass and fans—into versatile instruments, enabling participatory engagement for composers, performers, and the public, regardless of prior musical or technical experience.

As described by Hugill, Grainger “foresaw that bricolage, often using fairly cheap and readily available technologies, would become central to ... a ‘democratic’ approach to music-making” [7] in comparison to observations from Roser et al. that “emerging technologies are often expensive and, therefore, initially limited to society’s richest. A key part of technological progress is making... these innovations affordable for everyone”[9].
SIF embodies the principles of:
• Flexibility; to bend and mold to individual artistic practices by utilising high performing, open-source toolkits such as openFrameworks to interpret real-time spatial data to endless amounts of actions
• Adaptability; we didn’t want to limit ourselves to the body as the input (e.g. hand-tracking, skeleton-tracking) so we decided on depth tracking which allows for multiple performers and/or objects as input
• Affordability; we selected the Kinect v1(USD 10 in 2025[6]) over cameras such as Intel Realsense (USD 272-499 in 2025[8]) for cost-efficiency to allow for multi-camera installations to run concurrently. In addition, the cost affords a freedom for ""hardware hacking""[5] should the urge arise.
• Accessibility; to both composer, regardless of technical ability and performer/s, regardless of musical knowledge
SIF is an effort to support the multilayered creative and technical contexts involved with building new instruments by aligning with an infra-instrument tenet of the building with the “deployment of few sensors”[4]. However, the framework is not solely focused on the technology itself, but fostering artistic and educational opportunities and communities for creating New Interfaces for Musical Expression (NIMEs). Its dual performance modes — Machine Mode, in which the instruments autonomously perform preprogrammed compositions, and Human Mode, where audience interaction shapes the sonic output—reflect the evolving and reciprocal interactions between users, instruments, and environments.

Glass reimagines hand-blown glass percussion objects, drawing on Grainger’s experiments with “musical glasses”(Fig. 3, left). The sampling of a unique and historically important glass instrument collection and incorporation within the Spatial Instrument Framework enables new forms of engagement. For composers/performers or lay audience alike, movements trigger sonic explorations of the unique tonal and textural qualities of these fragile instruments, overcoming their inherent fragility through innovative sampling and interaction.

Fans repurposes computer fans into a matrix of sonic possibilities. Audience engagement with the fans generates and modulates electromagnetic frequencies, creating an immersive interplay of acoustic, electro-acoustic, and virtual sound layers.
Together, these installations highlight the entangled dynamics of interaction, materiality, and sound, embodying a creative dialogue that spans technology, heritage, and experimentation. By providing accessible frameworks for participatory engagement, SIF bridges the gap between professional and public music-making, fostering deeper connections between musicians, audiences, and the evolving possibilities of new technology.",nime2025_145.pdf,,,145.jpg,,,Lecture Theatre 2
158,music,Installation,,"installations-1, installations-2",,N/A*,In person,Bending Nature: An Architectural Sound Installation Exploring the Resonance of Architectural Texture and Natural Sound,Han Xu; Zehao Wang,"Bending Nature is an architectural sound installation situated in a confined, irregular space. By repurposing part of the raw speakers as microphones and surface transducers placed directly on the walls activate the architectural texture, the work merges natural sounds with feedback loops and creates a resonant dialogue between the building and its environment. Through a visually minimalist presentation, viewers are immersed in a dynamic, ever-changing soundscape that challenges spatial perception and offers a novel sensory experience of sound and structure.",nime2025_158.pdf,,,158.jpg,,,
159,music,Installation,,"installations-1, installations-2",,N/A*,In person,Belly of the Beast,Jacob Hedges; Daniel Garrett; Jaxon Sharp,"Belly of the Beast is an interactive music VR experience that allows users to dynamically manipulate a spatial musical composition in real time by moving sounds in 3D space, rearranging the structure of the composition, and playing virtual instruments along with the music. Leveraging the unique interactions of hand tracking and head tracking with the Meta Quest 3, This work harnesses XR technologies to reimagine the way we interact with music, allowing users to act as both producer, performer, and audience at once. Built using Unity and Wwise, and drawing inspiration from adaptive game music, the project empowers participants to shape the composition in real time through their movements and gestures. The composition presented is a 5-10 minute, semi-linear experience, where the user’s interactions progress the unfolding of the composition. It is a one headset per user experience, designed to be playable in a 4m x 4m square. The intended installation does not require extensive setup, as it operates with the Meta Quest 3 tethered to a Windows computer running the experience software.",nime2025_159.pdf,,,159.jpg,,,SoM Café Space
166,music,Installation,,"installations-1, installations-2",,N/A*,In person,AirPens: Musical Doodling,Ciaran Frame; Steph O'Hara; Sam Trolland; Alon Ilsar,"AirPens is an installation/activation that invites participants to make music while mark-making. NIME attendees are invited to reflect on this year’s conference by doodling, scribbling or sketching onto a whiteboard with one of four AirPens — a gestural DMI that utilises IMU sensors to convert movement into sound. Part of the larger AirSticks project which includes PCB design, 3D printing and fabrication developed at Monash University’s SensiLab, the AirPen takes the physical affordance and potential of a whiteboard marker, and extends the marker’s utility from simply converting movement with ‘2D mark-making,’ to ‘3D music-making.’ AirPens is designed as an ice-breaker experience, inviting attendees to share their thoughts on the state of NIME while improvising gestural music in small groups. While writing, participants may change the way they write to accommodate a different sonic output, or focus more on the ‘sonification’ of their natural writing style. But participants are not restricted to just writing with the AirPen — participants can explore the full scope of the AirPen as a gestural DMI away from the whiteboard, going off into free musical improvisations that may inspire discussions about the technology and project more generally. Each of the four AirPens available will have their own unique mapping that is in harmony with the other AirPens, inviting participants to freely go between writing ideas and improvising gestural music together.",nime2025_166.pdf,,,166.jpg,,,"Kambri Foyer, Registration Area"
168,music,Installation,,"installations-1, installations-2",,N/A*,In person,Breathing with the Forest,Toby Gifford,"As I travelled and camped down the East coast of Australia this summer, a standout feature of the experience was the sheer volume of cicada stridulations. The spatio-temporal pulsations of this sonic mass seemed musical in its phrasing, and felt to me like the sound of the forest breathing. In a literal sense our breath is entangled with the forest as we inhale oxygen that the forest has exhaled. This interactive sound installation is inspired by the notion of breathing with the forest, and aims to emulate that feeling through a breath controlled synthetic sound mass reminiscent of a forest full of cicadas.",nime2025_168.pdf,,,168.jpg,,,SoM Café Space
190,music,Installation,,"installations-1, installations-2",,N/A*,In person,Orbis,Yukihiro Sugawara; Kotaro Watanabe; Shinnosuke Hirose; Moe Miyake; Kenshiro Taira; Sakura Takada; Ryoho Kobayashi; Yuta Uozumi; Kei Fujiwara; Shinya Fujii,"When cells divide, they initiate cell division through ripple-like patterns called “Min waves”. In the field of artificial cell engineering, researchers have successfully generated these waves within artificially constructed cells. Although cell division itself has not yet been achieved, the Min waves observed in artificial cells exhibit diverse behaviors under various conditions. “Orbis” expresses the cycles and spatial movement of Min waves in artificial cells with sound and light, creating a space where the rhythm and energy flow of the artificial cells can be experienced intuitively through hearing and vision. We developed a system to detect the movements of Min waves generated in artificial cells through image analysis and utilized their unique behaviors as an oscillator. For image analysis, Blob Tracking TOP, an OpenCV-based detection model in TouchDesigner, is used to track the positional coordinates of Min waves in the observation footage. The Min wave coordinate data from TouchDesigner is sent to Max via OSC (Open Sound Control) and is reflected in the output of several speakers and light bulbs in a circular arrangement. “Orbis” is an interface that enables the unique behavior of this quasi-life form to be converted into sound and light. By expressing the ‘vitality’ that gradually emerges from matter through auditory and visual information, it encourages the audience to question “What is life?”",nime2025_190.pdf,,,190.jpg,,,SoM Lecture Theatre 1
199,music,Installation,,"installations-1, installations-2",,N/A*,In person,Fountains of Data: Evocation Zones,Scott Smallwood; Marilene Oliver; Daniel Evans,"FlowState is a virtual reality (VR) installation inspired by one of several dozen ancient Celtic baths in the Brittany region of France, built by Bretons in the 3-5th centuries. These baths were believed to be places of healing, often of very specific ailments, and were part of a larger culture of water as a healing agent. In our piece, the visitor initially encounters a field of particles, a point cloud outlining the bath structure in an empty black space (see fig. 1). As the visitor explores the area, moving arms in swim-like strokes causes interaction with the soundscape, including the addition of water sounds, and eventually leading to the particles resolving into a more realistic conception of the space, via lidar scans, 360-degree video, and sound recorded on the actual site of the fountain.",nime2025_199.pdf,,,199.jpg,,,
270,music,Installation,,"installations-1, installations-2",,N/A*,In person,Dinosaur Choir: Adult Corythosaurus,Courtney Brown; Cezary Gajewski,"Dinosaur vocal calls have been silent since they became extinct following the large asteroid impact event 66 million years ago. Our project, Dinosaur Choir, realizes musical instruments that bring these vocalizations back to life using CT (Computational Tomography) scans, 3D fabrication, and physically-based modeling synthesis. Musicians and gallery visitors give voice to these dinosaur instruments by blowing into a mouthpiece, exciting a computational voice box, and resonating the sound through the recreated dinosaur’s fossilized nasal cavities and skull. When the participant blows into the microphone, their breath becomes the dinosaur’s breath. However, the dinosaur does not have flesh, only bone. They are interacting with and seeing the process of millions of years of decay and change in the instrument. While science is one way of knowing dinosaurs, this work explores how a musical instrument can also generate knowledge. Dinosaur Choir also delves deeper into science, filling the unknowns with informed speculation and imagination.

Our work explores the Corythosaurus, a duck-billed dinosaur with a large, hollow crest housing complicated nasal passages that scientists hypothesize were used for vocal call resonation. Dinosaur Choir begins with this hypothesis but expands its exploration into the unknown vocal boxes, sensorimotor systems, and behaviors that allow vocalization to occur. The project represents a deep collaboration between music, computation, paleontology, and the imagination to explore the intersection of what we know and we may never discover. We have collaborated and consulted with paleontologists to produce this work as well as researching specimens ourselves in university and museum collections. We also explore the dinosaur skull as a wind instrument, iterating on the musical interaction to improve the intimacy and responsiveness of the experience. We focus on breath to drive air pressure into the computational model and we capture the mouth shape of the musician via optical motion capture to determine the stretch and muscle pressure of the vocal folds. This musical interface allows the dinosaur to come alive in the interaction between the musician/gallery visitor and the skull.",nime2025_270.pdf,,,270.jpg,,,SoM Café Space
217,workshop,,,workshops-0,,,,Workshop on Musically Embodied Machine Learning,Chris Kiefer; Andrea Martelloni,"This workshop invites participants to investigate Musically  Embedded Machine Learning (MEML), the use of machine learning to build musical instruments with tuneable models, which musicians can train using the instrument as the interface. It is an occasion to reflect on how we present machine learning models to end users, and  experiment with new musical ways to interact with creative machine learning.",nime2025_217.pdf,,,217.jpg,,,Skaidrite Darius N101
244,workshop,,,workshops-1,,,,Entangled Bowing: Integrating the MetaBow into Spatialized Composition and Real-Time DSP,Roberto Alonso Trillo; Davor Vincze; Peter AC Nelson,"This workshop explores the integration of the MetaBow (Alonso Trillo et al., 2024), a sensor-embedded violin bow, into compositional practice, sound spatialization, and real-time DSP processing. The MetaBow, developed to maintain the ergonomics of a traditional violin bow while providing high resolution motion data, opens new possibilities for mapping gestural input to spatial audio processing and digital synthesis techniques.

Participants will engage in hands-on experimentation with different mapping strategies, leveraging accelerometer, gyroscope, magnetometer, and sensor fusion data to drive spatial trajectories, control DSP effects, and shape real-time sonic transformations. 

The workshop will cover:

Introduction to the MetaBow: design, sensor technology, and integration.
Mapping Strategies: from direct parameter mapping to machine learning-based gesture classification.
Spatialization Techniques: using bowing data to drive multi speaker diffusion, binaural processing, and ambisonics.
Real-Time DSP Control: applying sensor data to modulate synthesis, spectral processing, and live electronics.
Collaborative Improvisation: participants will explore interactive performance scenarios, integrating bow gestures with sound diffusion in a networked environment.

This workshop is designed for musicians, composers, and digital instrument designers interested in expanding their performative and compositional language through motion tracking and spatial sound. Participants will leave with a set of tools and techniques for embedding bow performance data into dynamic, responsive musical systems.

No prior experience with MetaBow is required, though familiarity with Max/MSP, SuperCollider, or other real-time processing environments will be beneficial. A limited number of MetaBow units will be provided for hands-on exploration.
",nime2025_244.pdf,,,244.jpg,,,Hanna Neumann 1.25
261,workshop,,,workshops-2,,,,Housework Commons: Community-Engaged Workshop,Jocelyn Ho; Margaret Schedel; Susan Goodwin; Sofy Yuditskaya,"Housework Commons, a feminist activist project under the Women’s Labor initiative, transforms domestic tools into Embedded Acoustic Instruments (EAIs) using sensor technologies. It reimagines unpaid, undervalued domestic labor—traditionally private—as a shared act of activism, addressing global gender inequality in domestic work through public engagement with gendered objects.
Housework Commons includes two custom instruments:
1.	Embedded Iron v.3, based on an early-20th-century ironing board and iron, uses machine learning and sensors to alter pitch based on the iron’s position and sound quality (timbre) depending on fabric color and texture. The board acts as a resonator with a transducer and speaker.
2.	Rheostat Rotary Rack, inspired by a rotary dryer, features rheostats, a rotary encoder, and an 8-speaker base. Hanging clothing triggers pitches based on weight, while rotating the rack by hand or wind adds select frequencies.

The workshop integrates community-engaged participant action research approaches of Story Circle Method and Photovoice Method with EAI interactive engagement, where participants engage in a co-led, interdisciplinary  workshop to dialogue about issues of gender inequality in the NIME community and write down their experiences on fabric banners. These fabric banners, together with fabric banners from a past workshop with a local mothers’ will be featured in interactive installation that sonifies housework gestures. These fabric banners that encapsulate today’s experiences of gender inequality create a “living archive” installation, a dynamic site where installation and performance unfolds.
",nime2025_261.pdf,,,261.jpg,,,Hanna Neumann 1.25
330,workshop,,,workshops-1,,,,The AirSticks Early Adopter Hackathon: Community Building Around ADMIs,Ciaran Frame; Steph O'Hara; Sam Trolland; Alon Ilsar,"The AirSticks project is now marking its 21st anniversary; from a wearable gestural hat, designed specifically for a drummer’s practice, to exploring the use of mercury tilt sensors, exoskeletons, cameras, gaming controllers and IMUs in the creation of an accessible digital musical instrument used in hundreds of workshops and performances, including kids shows, audio-visual disability-led performances and percussion concerts. The AirSticks project now grounds itself in a clear purpose: to democratise music making. We invite NIME attendees to join us in this cause through a collaborative workshop that builds on this rich history and looks to the next 20 years of AirSticks and Accessible Digital Musical Instruments (ADMIs) more generally. With this rich history of diverse inclusive music making, the AirSticks team invites attendees to our Early Adopter Hackathon. The 3-hour workshop will begin with a 30-minute introduction to the AirSticks technology, followed by a 2-hour hackathon and a 30-minute debrief. Several AirSticks will be placed around the room with an open data stream available for all participants to use. MIDI and OSC messages will be shared and can be utilised in whichever way participants wish to use them. ",nime2025_330.pdf,,,330.jpg,,,Hanna Neumann 1.37
334,workshop,,,workshops-2,,,,"Global Entanglements, Music and the Commodification of Cultural Labour in the Age of AI",Tom Willma; Oliver Bown,"Motivated by global discussions of socio-technical change and recent Australian legislative lobbying, this short-form, 3-hour workshop targets the commodification of cultural labour and current entanglements between social, political, corporate and technological actors in the music industry. The objective of this workshop is to hold a critically minded discussion between guest panellists from the Australian music sector and NIME colleagues, grappling with these sociotechnical imperatives both generally, across the industry, and particularly, within the context of live performance. Key takeaways of this workshop include:

• An empirical overview that survey’s sociotechnical change and AI use across global music industries
• A critical discussion that scopes key political-economic concerns in the music sector
• Proposed frames for NIME research community to engage with industry and develop key strategies for ethical AI adoption
• Directions for future discourses, policy considerations and further dialogues of sociotechnical transformation in the music industry

Discussions between panellists and attendees will be guided by three critical questions:
1) What political-economic changes have been taking place in global music cultures, including but not limited to those involving AI and live virtualisation technologies?
2) How does the NIME research community interface with music cultures beyond academia in an age of AI, particularly in the development of live music technologies?
3) What blind sports are being neglected by the current trajectory of political and industrial discourses? How is our work being affected, what voices are being heard above others and what could/should be done?

Please note that the content of this workshop will be recorded and may be developed and referenced in future research and publications by the organisers, as may a record of workshop minutes be published.",nime2025_334.pdf,,,334.jpg,,,Hanna Neumann 1.37
335,workshop,,,workshops-2,,,,"Somatic and somaesthetic design practices in NIME: Sharing experiences, methods, tools, and concepts",Juan Martinez Avila; Doga Cavdir; Courtney Reed; Mary Mainsbridge; Kelsey Cotton; Tove Grimstad Bang; Lucia Montesinos Garcia,"Somatic and somaesthetic perspectives have increasingly become a part of the NIME community in recent years. In particular, many practitioners within NIME (and beyond) have adopted techniques, concepts, and tools from soma design—a methodology that centers the focus of design on the soma, i.e., the body, the lived experience, and first-person perspectives on embodied phenomena. In this workshop, we aim to gather with artists, designers, researchers, musicians, and creative practitioners at NIME to provide them with a platform to: (1) learn about soma design in NIME and musicking through our two keynotes, and group of expert panelists, (2) share their experiences of employing somatic/somaesthetic techniques to guide their creative choices, and how they are producing knowledge in the form of methods, tools, and concepts in this space, and (3) network with other like-minded researchers and practitioners who share a common body-centric perspective on design, interaction, and performance.",nime2025_335.pdf,,,335.jpg,,,Skaidrite Darius N108
336,workshop,,,workshops-2,,,,Gesture and Generative AI in NIME Design,Jason Smith; Bryan Pardo,"Generative AI models have been used to great effect in creating large amounts of music and audio for creative applications. There are challenges in using generative AI in live performances, such as high latency and lack of agency in terms of expressive input for a performer to provide a generative model. We aim to build AI-based NIMEs that support musicians creatively, rather than replacing their efforts during a performance.

We propose a workshop on generative AI systems for live music, focusing on enabling a user's musical expression. This workshop will discuss recognizing human motions and gestures through a camera and real-time audio generation. We will also discuss the use of adaptive machine learning to enable a system to evolve alongside its user. This demonstration will include a live tutorial on the development of a camera-based gesture recognition application using popular frameworks and libraries.",nime2025_336.pdf,,,336.jpg,,,Hanna Neumann 3.41
337,workshop,,,workshops-1,,,,Designing Meaningful Input Devices for Movement Artists,Linnea Kirby; Christiana Rose,"Designing Meaningful Input Devices for Movement Artists is a theory-based virtual workshop where small groups will be guided through the brainstorming process of an input device to be used by movement and other non-musician performing artists. The workshop will be led by Linnea Kirby and Christiana Rose, the founders of CirqueIT, a company focused on designing interfaces, performances, and experiences with circus arts integrated with music technology, light, and interactivity. The workshop will cover four main topics including choosing a performance discipline, selecting sensors, safety considerations and solutions, and considering the justification behind each choice. This two hour workshop will be a combination of a presentation with examples from previous work and small breakout group discussions. We encourage participants of all backgrounds and levels of experience to join us.",nime2025_337.pdf,,,337.jpg,,,Skaidrite Darius N108
338,workshop,,,workshops-1,,,,MoNoDeC: The Mobile Node Controller Platform,Nick Hwang; Anthony Marasco,"MoNoDeC is a multichannel audio system that uses audience mobile phones and IoT-hardware-driven speakers as point sources for configurable and dynamic immersive audio speakers and audience performance interfaces. Audience participants register their current location within a customizable audience space diagram on their mobile phones. Their phones then become a point source speaker within the immersive experience. A person designated as the performance/installation ‘controller’ sends audio, control, and interface data to participants throughout the experience. In addition to diffusing audio to mobile devices, MoNoDeC can include embedded computer instruments known as ‘autonomous hubs’: IoT-based speakers that receive playback/diffusion data. These autonomous hubs are meant to be used in larger sound diffusion performance settings as well as installation/fixed point source scenarios in conjunction with audience-provided devices. 

This workshop will lead participants on setting up the MoNoDeC system, designing registration and location-choosing presets for audience members to engage with, and methods for panning audio around the networked mobile device speaker array during a live composition. Participants will also learn the basics of Collab-Hub, our framework for sharing control data between networked telematic performers that serves as the backbone of MoNoDeC. After learning the core features of both systems, participants will form groups and design short immersive compositions using the framework. participants will set up the MoNoDec system, design registration and location-choosing presets for audience members to engage with, and explore methods for panning audio around the networked mobile device speaker array during a live composition. At the end of the session, participants will form groups and design short immersive compositions using the system. ",nime2025_338.pdf,,,338.jpg,,,Hanna Neumann 3.41
339,workshop,,,workshops-1,,,,Entangled Listening: Exploring Relational and Diverse Listening Practices for DMI Design,June Kuhn; Brittney Allen; Nicole Robson; Andrew McPherson,"Listening is fundamental to music practices and provides technical and cultural context to the design of musical instruments. Through various entanglement theories of human-technology relations, we can understand listening as a Baradian apparatus that motivates, propels, and evaluates the design of new musical instruments. Further, we can use listening practices as a method to de-center the human. We propose a workshop primed by emerging theories in sound studies to critically examine how listening appears, how it functions, and how it performs. Through guided mediations, hands-on exercises, and prompted discussion, we aim to integrate a plurality of listening experiences and suggest tuning our listening toward more entangled design practices.",nime2025_339.pdf,,,339.jpg,,,Extra Classroom
